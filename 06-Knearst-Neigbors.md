Welcome back everybody. This is ML experts machine learning crash course. In this video, we're going to go over K nearest neighbors. The basic idea is to figure out what an unlabeled example is according to its neighbors. So for here, K is equal to two, which means we're going to use two of its nearest neighbors to try to determine what value this example should take on. Here K is equal to three and here K is equal to seven. So based on these seven neighbors, what value should this example be? Well, we can take a vote. So here we have six saying that it's green because six of the nearest neighbors are green. And one of the nearest neighbors that we're evaluating are red. So six over seven gives us an 85% majority vote that this element should be green. Let's say we're working at Symantec, the cybersecurity company and we wanted to attack intrusive processes which for our purposes, we can just call programs, okay? Every once in a while these programs or processes will make calls to the operating system called system calls in order to get them to open a file or write to the console. Things like that. Our goal is to predict which of these processes are intrusive and which ones are normal. And we're not only going to predict processes that are on a single machine, we're actually going to be predicting whether processes are normal or malicious slash intrusive across the entire network. All right, so the first thing we're gonna need is features. So we'll have a process ID which we can assume to be unique across the entire network. And this process ID will map to an ordered list of system calls that that process made to the operating system. The label is going to be either zero if the program was never found to be intrusive, or one, if the program was eventually found to be intrusive. So what we can do is take our features and these are just words, right? These are just the system calls themselves and we're going to perform TF-IDF on them. Okay, so now once we perform TF-IDF on our features since we're kind of treating them just like words, we now can plot them. Now I know that the features actually had more dimensions than just two, but let's just assume we're using the first two dimensions for these examples. Then we can plot these and just visualize where they would exist in this two dimensional space. So x one would be here, and x two would be here and we just plotted them. So all of these processes are labeled green because they were never found to be intrusive. These processes, however, were later found to be intrusive after the fact, we've labeled those red. As you can see there's some distance or separation between these two clusters. Let's bring in an example that we haven't seen yet. So we're gonna color it white, just because we don't know if it should be red or green, and we're going to run the K nearest neighbors algorithm on it to figure out how close it is to all the other neighbors and then vote amongst its closest neighbors about which category this unlabeled example should be in, either intrusive or not intrusive. By the way, here distance between one and nine we're gonna define as Euclidean distance. It's just a basic formula. A one is a one, a two is a two, b one, b one, b two, b two, and it can generalize out to multiple dimensions. So every single element would just be subtracted from the corresponding element. And then we would square those differences and then finally take the square root of the result and that would give us the distance. Here's 3.2, the distance between nine and four is 2.6. After we go through all the examples, the final example, the distance between nine and eight is 0.08. Now let's assume here that K is equal to four. So we'll look at the four closest neighbors, we can either sort them or keep certain data structures around to let us know which ones are the four closest values. If we vote what class this should be in based on the four nearest neighbors, we'll see that this should be declared as an intrusive program. Now, if we used eight neighbors, though the vote would turn green. K is an important number. In my opinion, this should probably be classified as an intrusive program, but based on the value of K, the classifications will fall differently. By the way, since we only have two classes here you wanna pick an odd number of neighbors to avoid ties. Let's say though that the intrusive programs all take a different angle in terms of being different than the normal programs. So for instance, we don't see two distinct clusters here. We actually just see a cluster of normal programs and then the odd intrusive programs along the edges. One thing that we could do is bring in an unseen example. Let's say it lands over here and take the K nearest neighbors which are normal. K equals to three and we only count the neighbors that are normal. Then what we can do is average the distances, here's 3.8. And if that distance of 3.8 is greater than some threshold that we can tune through cross validation, then we would assign that program as intrusive. Let's say that we were continuing along with those features. So we had our TF-IDF features, and then all of a sudden we got introduced this priority feature. Priority means what priority the operating system will execute the process at. Priority is an ordinal feature which means it's categorical, but there is order to it. So you can have a low priority, a medium priority and high priority as a particular process. So since we have a mix of features now, we're gonna use something called the Gower distance. High is going to be assigned to be zero, medium one, low two, then what you do is you divide this number by the maximum numbers. So zero divided by two, one divided by two, two divided by two, and that'll give us 0.5 and one and that will actually go in as our priority. If this pid one had medium priority, we would assign the feature to be 0.5. This is just a way to normalize the features to make sure they're between zero and one and still represent what they should which is distance between each other. Let's say that we have two new features now. One of the features is going to be, did the program run in pseudo mode, which is a super user. The other feature is going to be, did the program itself terminate early or exit out early before it was finished computing? So both of them are going to be what's called asymmetric binary. Let's say a vast majority of the programs don't run in pseudo. And it's actually more informative to us if we understand that a program ran in pseudo, same with early exits. If all the ton of programs finish without any problem, it's more informative that it didn't finish. We call that asymmetric binary. And with that we have to use something called the Jaccard distance where we have two examples here, pid one and some other example. We count the number of mismatches between them because these are binary. We count the number of mismatches and divide by the number of total comparisons except where they're both zero, because we've deemed that not exiting early or not being pseudo is not as important so we don't really care about that. So we count the number of mismatches and the number of total elements or comparisons, and we exclude the zeros. Okay, so we have the priority. We have the pseudo, we have the early exit, and now we have another binary feature that we think will be helpful, did the user operated their program on a Mac or on a PC in our network? And for us, it's not really more informative that they used a Mac or a PC. So this is actually called a symmetric binary. All we have to do here is just a simple matching distance. So if we have another example, count the number of mismatches, divide by the total number of comparisons. And this time we do include the zeros. So the Gower distance aims to normalize all your features between zero and one. So we'll use the Euclidean distance for these variables. We'll use the ordinal distance from this, the Jaccard distance for the asymmetric binary, and the simple matching distance for these variables. And we can just mix and match and use all of these to make up our total distance between two processes. Again, these Euclidean distances will be min-max scaled. You can also use Manhattan distance here. Manhattan distance is simply this value, but it's the difference between the elements and just the absolute value of them. Now let's move on to our next example. Imagine that we work for a marketing agency contracted by universities to drive alumni donations. We collect 10% of the total donations during the calendar year as compensation. Since calling everyone is prohibitively expensive and annoying to some alumni, we could use K nearest neighbors to predict how much someone may donate before deciding to call them. This way we can optimize donations without engaging every alumni. Okay, so the universities provide us with alumni information, including their college such as engineering, social sciences, college of math. Graduation year, which only goes to 10 years ago. So we've changed it into a feature between zero and 10, indicating how many years ago they've graduated. And finally, we have a feature which represents the join date of the university's Facebook group. The reason why this is important to us is because once that person has joined the university Facebook group, they immediately get asked to provide donations to the university. And we don't wanna double ask those people. So what we'll do is we'll just exclude anybody who has joined the Facebook group within the last year. We won't use the feature for anything else. Now for labels, what we get is the amount donated to the university by an individual within last year's campaign. So first we need to start vectorizing these inputs for the K nearest neighbors model. So for the college, it's gonna be categorical. Graduation year will be ordinal since it can be ordered and it's categorical. And the university Facebook group, we're not going to use them. That's just a do not call list. So for categorical, we're going to use Hamming distance. For ordinal, we're going to incorporate the element of the Gower distance. Hamming distance is just going to be the number of differences in category. For instance, if the major for one alumni was engineering and the other major had to do with business for the second alumni, then the distance between them is going to be one. However, if alumni one did engineering and alumni two did engineering, then the distance is zero. So it's simply a matching notation for categories. Now, imagining that the examples could be plotted in two dimensions. Obviously they exist in higher dimensions but for now it just helps for visualization purposes. We can see here that the labels of the alumni are actually numbers which we expected, but we haven't really gone over how to perform K nearest neighbors on them. So let's go ahead and deal with these continuous labels here. Let's say that we get an unseen example. So we don't know how much this alumni will donate but we have their features. And we'd like to form a prediction based on their features, how much this alumni will potentially donate. So if we assume K is equal to three, we're gonna sample the three nearest neighbors to this alumni. And we're going to take the average of the nearest neighbors, that's it. Now for practical considerations, it performs poorly when the features are in high dimension. So if you have tons and tons of features, the distance metrics break down especially Euclidean distance. So let's say that we have just a single dimension here and A lies at point one and B lies at point five, so the distance here is five. Now to get the same distance of five as separation. If we go to two dimensions though, in order to measure five units of distance, we need four units of distance in one dimension and three units of distance in the other dimension for a total of seven units of distance just to represent the original five that we got in one dimension. Now, if we continue this on to three dimensions. Imagine this goes back into the screen. We now all of a sudden need three and a half units of distance in this dimension, two and a half units in that dimension and 2.8 units in that dimension just to still represent the steady five units of difference according to the Euclidean distance formula. So as you grow in dimensionality, you actually see this result come out to mean that you need more separation between points to represent the same distance which can hurt the nearest neighbors model. So what are some actual ways to reduce the dimensions of this data? Well, later in the course, we'll actually talk about pretty generic ways to reduce dimensionality. But for now, one that you can try with K nearest neighbors is to simply pick one feature and perform cross validation on it and then pick another feature, perform cross validation on it and measure the accuracy. And if that one beats all the others, let's say you tried all of them. And this one was the best. You lock that in and pick that as a solidified feature, that feature is locked in and then you try all the other features again. And let's say this one was the best. So you have two features now and you keep doing this until you can get a good representation or a good score comparatively than rather using a lot more dimensions. So you're greedily selecting which features are best based on their incremental performance. So another thing to consider is that any model that's based upon distance, especially K nearest neighbors is going to be sensitive to scaling. Okay, for instance if we have height, waistline, and distance run weekly, and they're all in meters, the distance run weekly is going to be a lot larger than someone's waistline. In this case, you probably don't want to scale because these differences are meaningful and they all are using the same units. So in this case you probably are better going with Manhattan distance. However, if you had something like this where you wanted to predict heart disease wit a caloric intake per day. This is a very different unit than the others. So I would recommend here using Euclidean distance and doing a min-max scale to put all of these values between zero and one. Another thing we can do to improve performance is to weigh the votes. So take this example where K is equal to four and these are the three nearer neighbors. Then this fourth nearest neighbor. We can make the vote of node seven mean more than the vote of node six. So for instance, the vote of node seven is going to be the distance of eight, plus distance of seven, plus distance of two, plus distance of six, all divided by the distance of seven. The vote of node six is gonna have the same numerator and now its denominator is just going to be different. So as the closer a point is, the d seven is going to be smaller than d six. So since you're dividing by a smaller number, the overall vote will be larger. D six here is larger than d seven. So since that's in the numerator you're going to be dividing by a larger number, which will make the vote of node six be diminished. One thing to consider is that it is computationally expensive. here it's O to n, d, k, which is the number of nodes, the dimension of each node, and K for the K nearest neighbors. To get the computational complexity down, we can use the data structures such as a KD tree which allows us to locate relevant points faster than iterating through all of the points. We won't go over the data structure in detail but a KD tree is effectively a binary tree which splits the end dimensional space here too into sections by alternating each access per layer. This will help us get our time complexity at predicting a single example down to an average of K log n assuming that our dimension is fixed. Keep in mind, we have to build this KD tree and that's not free. Be aware that this data structure will suffer from dimensionality growth as well. This tree structure will degrade to O of n, k, d as the dimensions grow large. Okay, so now we've gone over what a K nearest neighbors model intuitively is, how to use it with various types of features, how to measure the distance between two examples to determine which are the K nearest neighbors. We've also covered the limitations of K nearest neighbors and also some practical considerations when using it. We also applied K nearest neighbors in a practical sense. So for the cyber security case, we got an F one score of about 83% that was offline. And within the first month we got two alarms raised by the algorithm. One of which was a false alarm, and two of which we shut down immediately because they were found to actually be intrusive. For the university that contracted us out to drive alumni donations, we accomplish two things. From the business perspective, it was great because we got about a 20% ROI. We had to hire people to make the calls out to these graduates. And we would only keep 10% of the revenue. The university got 90% of that. As well, the university reached out to us and said that they have never gotten such a positive response from being called to donate. So I think our algorithm did a good job in isolating the people who didn't wanna be called and we didn't call them. So I think it did well for two purposes there. That's it for this video, please join us next video as we continue our machine learning journey.