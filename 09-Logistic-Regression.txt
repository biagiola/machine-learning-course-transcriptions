WEBVTT <v Instructor>Welcome back to ML Experts,</v> Machine Learning crash course. In the previous video, we talked about linear regression and we used it to predict the power demand for the California ISO. In this video we'll be going over logistic regression, which is what we'll use for classification. Let's say that our manager at California ISO asked us to classify whether or not there will be a power outage the following day. This happens when the power supply to the region falls below the power demand. If we can predict which regions will fall into a power outage, we can ramp up even more power sources and power routing infrastructure. We can ensure that that region is protected. For features, we can continue to use the temperature provided by the national weather service for us per region. The labels will be binary. It will be either zero if there were no power outages that day for that temperature for that region. And it will be one otherwise. Now that we have our features and labels, we can go ahead and plot them. Right now we only have temperature representing our features and that is just one dimension of data. The labels however, we'll color code this data. So for green, we will have no power outages. And red means for that particular day, region and temperature, there were power outages. The idea is that this line separates the examples by class. So on the left we will have no power outages for those data. And on the right we will have only power outages for that data. You can see here that it's not doing its job completely because there are different examples on the other side of the line. But the idea is to find the best line that can separate this data out. All right, if we want something a little bit more visually similar to what we encountered in the linear regression case. We can add a dimension of megawatt power demand for that particular temperature. And once we plot that out, we get a similar curve as we got in linear regression, but now we have them color coded by label of whether there was a power outage or there were not a power outage for that particular region. Logistic regression is based on the same idea as linear regression, in the way that we still use a line to designate our model. The only difference is that we now want Y or the output of our model to be a probability. This probability will represent the chance of a power outage occurring the following day. So we have our linear regression line model. For logistic regression all we're going to do is take this linear regression model and put it in the exponent in this formula. This will do a couple things for us. One, it'll ensure that we get an output between zero and one. This function here is called the sigmoid function or sometimes called the logistic function which is where logistic regression will get its name. The idea is to be able to plug in some temperature here and have some coefficients solved for, let's assume that we can solve for the coefficients and to output some probability. In the linear regression example, we had the opportunity to use formulas called a closed form solution to solve for the coefficients and the bias itself. Unfortunately, that doesn't work for logistic regression. There is no closed form solution. We have to use an iterative process to zero in on what these values should be. One way we could do it is just by randomly plugging in numbers. We could just plug in three and four and then just check what does our probability say? And what is that difference compared to the actual value that we have? For instance, if we output 91.1%, and for this particular temperature input there was no power outage that particular day. Well, then the probability should actually be zero. So what we could do is just plug in random numbers for these coefficients and just see how much we progress toward getting the actual value based on these coefficients. It'd be extremely inefficient though. As an alternative, we can use a better iterative approach which will tell us how to update the coefficient and the bias. Let's see how we can do that. First we need to designate something called a loss. A loss is going to be the difference in Y hat, what our model predicted and Y, what the actual label for that example was. The loss function for logistic regression is going to be the log loss. So here we have the Y values, which are the actual labels of the examples. That's either going to be zero or one depending on if there was a resulting power outage from that example. Or zero if a resulting power outage didn't happen. The Y hats will represent the predictions that our model have output. So for our case, the prediction was 91.1%. And the actual value that Y took on was 0%. If we plug in our numbers, we'll see that this side of the equation zeros out because zero times anything is zero. And we actually only need to focus on this for when the label is zero. If the label were one, we wouldn't use this side of the equation. We would only use this side. So after we plug in our numbers, we get a zero over here. This turns into a one here. And our prediction is one minus 91.1%. We take the log of that. And the result is negative 1.05. The loss represents how far off the model's prediction was from the true label of that example. Now, if we had guessed or predicted with our model 21.1%, then we would still use this side because the true label is zero. But we would see that our loss actually gets closer to zero. All right, now what if our model had predicted zero and the true label was zero? Well, this half of the equation gets zeroed out. We use this side of the equation and interestingly one minus zero, so the log of one is zero in itself. So our loss is zero. So when we make the correct prediction, we incur no loss. Now, what you may have noticed is that as the predictions got closer to the actual value or the label of the example, the loss function got closer and closer to zero but all of the values were still in the negative domain. So really what we want is the negative of this. And then we can speak about minimizing the loss function. Here once we add this negative sign we actually get the cross-entropy loss. It's sometimes called the negative log loss. So how can we use the cross-entropy loss as a way to minimize the difference between what we actually predicted with our coefficients and what the true labels of the examples were? Well, first we have to understand how well our model is doing in general. We can do that by taking the average loss over every single example that we have in our training set. So if we had 10 examples, the summation would expand out to 10 terms and we would simply run for every single example, find the loss of it and average the losses together. So how does the average loss across all of our training examples relate to our coefficients that we have for our logistic regression model. And even better, can we use this loss in a way to cleverly update our model coefficients in order to produce better predictions that are closer to the true label. By the way, you may hear me call these parameters, coefficients, and later we'll speak about them in terms of weights. So let's see how these relate. Over here in the bottom right we have our logistic regression model. Plotted on this axis we have our B one or beta one coefficient. And on this axis over here, we have our average loss which is the same equation that we saw earlier, which is the loss averaged across all training examples. Okay, what if we plotted this? So what this is saying is that based on the value of B, whether it's low or high, we get a particular average loss across all of our training examples. So for instance, let's say this year represents that we're using all training examples from one to N, N being how many training examples we have. Let's say these trainings examples produced a loss of this when our beta one took on this value. So intuitively to me, we can take our beta one, and if we just increase the value from here to say here, we will be closer to the minimum loss. The question is, how do we do this mathematically? Well, we can use something called the slope. So the slope at any given point on a curve will tell us what change in this variable is needed to affect a particular change in this variable. So a slope in the negative direction tells us that we need to go to the right. So if we want to bring this loss function in the negative direction, we have to go to the right. Let's say that our randomly initialized beta one parameter or coefficient made our average loss across all of our training examples be this value. So we started with this beta one and that landed us with this loss value here. If we take the slope, the slope is now positive. What that means is that we have to work left, such that we update beta one to be smaller than it currently is, such that we can minimize the loss. The question now is how much are we supposed to move to the left or to the right? Well, when you take the slope of a curve, you actually get a value out that indicates for every unit B that I moved to the right, I increased the loss by three. So if I go one here to the right, then the loss will increase by three. That is the slope of this curve at this point. So using this information, how do we update B? Well, we can just update it by the negative slope. So here, if the slope is three, we can simply subtract three from beta. Mathematically it's guaranteed to move us closer to the minimum. Over here, we had a negative slope, and the value of this slope was negative two. What that means is for every value that we go to the right, so for every unit one increase that we apply to this coefficient, the loss will decrease by two. So in this instance, we would increment beta one by two in order to get closer to the minimum value of the loss. So let's increment beta one by two. Now we've landed here. And now the slope is negative 1.5. The hill, as you could call it is a little bit less steep. So now we're only going to move in this direction by 1.5. Now, as we can tenure this process, we could approach the minimum. And once we're at the minimum, the slope of this minimum is zero. So our update will be zero in that direction and zero in that direction. And at this point we say that our loss function has converged. So we have selected a value of beta one such that the loss function is minimized and thus it has converged. So what's missing? Well, we haven't actually gone over how to find the slope of this loss function at a particular point of beta one. We can find the slope of this line by using something called the derivatives. So if we take the derivative of the loss function, this will give us the derivative of the loss with respect to our parameter that we're interested in, which is beta one here. This is doing the exact same thing that we did earlier where we just found the slope of this curve at a particular value of beta one. Where now, instead of graphically, we are doing it mathematically. Now the thing is we have more than just one beta. We had the bias term as well so we had beta zero and we have beta one. So the derivative will only get us so far as to get us beta one. However, let's plot the same exact thing that we plotted earlier, but this time we're going to plot in reference to beta zero. So beta zero can have a very different dynamic in terms of the average loss created from its value than beta one. For instance, this is beta zero's minimum, which is very different from the other minimum for beta one. So here we can apply the derivative to this loss function with respect to our beta zero parameter. And once we take that derivative, this is the equation that we get. So now what do we have? Well, we've taken the derivative of the loss function in order to figure out what direction we should update the parameters or the coefficients. And when we took the derivative of the loss function with respect to beta one, the derivative was this. And when we took the derivative of the loss function with respect to beta zero, we got this as a result. When we combine these into a vector or a list basically, what we get is the gradient. So what does this get us besides updating these symbols and getting us this little upside down triangle with respect to the betas? Well, it just gets us a concise way to represent which way we should adjust each parameter or each coefficient in the model according to the loss function. So how do we actually use our gradients or the derivatives at each parameter to update our model weights? Well, it's a simple subtraction. So we take whatever the value of the weight is now say beta one, and we subtract the gradient at beta one. Let's assume that the gradient at beta zero was four. And let's assume that the value which was randomly initialized for beta zero was 0.92. Let's go ahead and do a couple of iterations of gradient descent. So we'll expand out this equation. Now, we're instead of representing Is, we're representing zero and one, which is beta zero and beta one here. What we do to get beta zero at the next time step or an updated value of beta zero, we simply subtract the value of the gradient from the current value of beta zero. We do the same thing with beta one. So for instance, here we'd have negative 3.08 and 3.04 for our next values of beta zero and beta one. We can plug those in here just for reference. Now, we can run a gradient descent again on this equation. Now, our new updated gradients are 0.98 and 0.04. Let's plug those into our equations. So beta one at the current timestamp is 3.04, the gradient is 0.04. So we subtract that from the current value to get the updated value of beta one. We do the same thing for beta zero. We can plug those results in, and then we can run gradient descent again. You'll notice here that the gradient of beta one is zero, which means that beta one has converged. Beta zero hasn't yet converged so we should continue. After one more iteration, we see that four is the final value for beta zero, and three is the final value for beta one. So any further iteration wouldn't change our parameters. This is called convergence. Well, that means that we've hit this point here on our loss function graph such that both of the parameters have converged to the minimum. So that means we don't need to continue iterating. All right, so typically our update functions will look like this, but there will also be an r in front which indicates a learning rate. Now this learning rate will typically lie between 10 to the negative six and 0.1. The idea of a learning rate is to multiply it times the gradient update such that we try not to overshoot the minimum. So let's say that we were here, we took the gradient to get our update for the parameter B of zero. And all of a sudden our learning rate was so high that we ended up over here. We updated our perimeter too much. And then let's say learning rate was high again and the update was high. So we would just possibly back and forth like this. So the idea is to choose a reasonable learning rate, such that you converge quickly and effectively, but not so fast that you bypass the minimum potentially indefinitely. Later, we're gonna talk more about these learning rates. So don't worry about it too much right now but that is the general idea. Now, we've trained our model because we've approached a gradient step of zero for each of these parameters. So let's plug that in. We got three for beta one and negative four for beta zero. What does this do for us? Well, now we can plug in values. So we can say the probability of one which is here, we'll call one a power outage will occur. So here we're looking for the probability that a power outage will occur given that our data is 30 degrees. So now we can plug in 30 here and just crunch the numbers and we get 99.9% chance that a power outage will occur given an input temperature of 30 degrees. So remember earlier we mentioned a threshold. This was a similar concept to what we did in naive Bayes, such that if the probability was greater than 50% we would declare that it was a positive case. For instance for spam, we declared the input as spam if the probability from the naive Bayes model was greater than or equal to 50%. Here, we can apply the same principle. And if this value is greater than 50%, we can declare that an outage will happen the following day. You wanna follow up with cross validation here to make sure that you're picking the optimal threshold. So now that we have a trained model, what we can do is visualize the decision boundary of the logistic regression model. So how do we figure out the value for this line? Well, we simply take the ratio of beta zero over beta one and take the negative of it. If we are in two dimensions and we want this line here, which is our decision boundary for X one being the temperature and X two being the megawatt power demand, then we simply use this equation. Earlier we mentioned the decision threshold of 50% and how cross validation can be used to pick the optimal one. So here, if we change the decision threshold to 75, our decision boundary actually moves up. Likewise, if we bring our decision threshold down to 25, then we've actually included now more negative examples to the right of the decision boundary. So now that we've gone over these multiple features, let's see how we would actually represent this in our model. So this model had a single feature of temperature and a bias term. Let's say that we wanted to incorporate another term. This separate term could be another independent variable or it could be a feature interaction term, which we talked about in the linear regression video. The only thing that would change here is that our gradient would include these additional terms. So we'd have one for beta zero, one for beta one, and now a term for beta two and which we need to find the loss for. For linear regression we were able to see the effects that the coefficients would have on the output. This time it's a bit more complicated. We'll have to take e to the coefficient to get something called the odds ratio. And then if we take one minus the odds ratio, we get the percent change in the odds. So let's say that the coefficient for temperature is 0.28. So we would take 0.28 and plug it into this formula. And what that would mean is that for every degree increase in temperature, we're 32% more likely we'll have higher odds to experience a power outage. Now it's important to realize that this doesn't mean that there's 32% more and total probability. It's just whatever probability that we were at currently, adding that one degree temperature resulted in us having a 32% higher likelihood of having an outage. If our coefficient beta one was negative, the same thing would hold except there would be lower odds. Note, that if the coefficient is zero, then that means there is going to be no change with respect to that variable in terms of the output. So just as we talked about in the linear regression video, if the confidence interval for your coefficient contains zero, that means that the coefficient itself is not statistically significant and it shouldn't be considered. If you need to brush up on what we went over in linear regression, feel free to watch that video again. What if we wanted to predict more than just will a power outage occur or not? What if we wanted to predict whether the outage would happen in the morning, afternoon, or evening. Generally power outages will happen when people get home from work and turn on all their devices and electronics. Let's just say that we did want to predict when during the day the power outage will occur. So we'll have to use something called multinomial regression, which is what we need if we wanna predict between more than two classes. Instead of a sigmoid, we're actually going to use something called a softmax function. A softmax function is a generalized sigmoid such that it produces the probability amongst K classes. So here it would produce a probability for morning, a probability for afternoon, and a probability for evening. The softmax function ensures that all of these probabilities across the classes sum to 100%. Just like our sigmoid function did for two classes. The predicted class would be the maximum value that the softmax function produces for a particular class. So now our updated model will look like this to incorporate the softmax function. So here, if we wanted to know the probability of a particular class here two, given some features, we would now have to have individual parameters or coefficients for each class. So if we wanted to predict the class equaling two, we would use the weights assigned for that particular class. And then in the denominator, we would just iterate through every single class that we have using their parameters. So our loss function will also have to be updated. So here we can see that we're iterating through each class now. Since Yi will only take on a value of a particular class at any one time, that means that this value in here will only be evaluated for a single class. So this summation, even though we're summing through. In this case, two out of three terms will equate to zero. Finally, with this loss function we'll also obtain a new gradient. So this is the updated gradient. If you notice this is extremely similar to the previous gradient, we're now just having the incorporation here of what particular class it is. When we discuss training, I mentioned that we calculate the loss function with respect to every single training example. And then we just average those results together. We then take the results of that loss and we calculate the gradients and then we update our parameters based on that gradient obtained from every single example that we have. This is called batch gradient descent, because we're taking the loss with respect to all of the training examples, finding the gradient, and then updating the parameters based on the gradient obtained from every single training example. It's called gradient descent because we're taking the gradient and we're descending down that curve to the minimum. There's another way that we can do this, it's called stochastic gradient descent. Where you effectively pick out a random example from your training examples, you find it's gradient with respect to the loss function, then you update the parameters according to that single example's gradient. This type of gradient descent can be used as an online training method, but it's often too slow to converge in practice. There's a happy medium between the two where we take a mini-batch of size m where m is typically a lot smaller than n, and we find the gradient with respect to this mini-batch. And then we have that same exact update process with the gradient. So what this does, it allows for less noisy updates to the parameters as opposed to stochastic gradient descent, since there's typically averaging introduced by the mini-batch. But as well, you don't have to pull all of your data into memory to get a gradient update as you would with batch gradient descent. As well these mini-batches can be parallelized and their updates to the parameters be aggregated. Now, imagine we're doing mini-batch gradient descent with our blackout examples. So we'll have a majority of the time we're going to not have a blackout or a power outage, and sprinkled throughout our examples we're sometimes going to find an example that is of the label blackout or an outage did occur. We select a mini-batches at random without replacing the examples, until every single element in the data has been trained on, and we call that one epoch. Since these blackout days compared to the non-blackout days are so rare. It could be that we get an entire mini-batch full of data that doesn't contain a single negative example. This isn't very good because that means our model will have a tough time learning from only positive examples. This implies that it could take a long time for a loss function to converge. A common approach is to down sample the majority class, so here we'll only consider maybe 10% of the no blackout days or no power outage days. And we'll hold on to all of the blackout days or power outage days. This means that we're far more likely to encounter a negative example or several negative examples per mini-batch that we select. However, we've now changed the distribution of our examples since non-blackout days seem far less likely than they actually are in reality. We now have to proportionally compensate for this discrepancy by upweighting the non-blackout days. By down sampling and then upweighting, we can effectively train with roughly a 10th of the data which in turn can allow our loss function to converge a lot faster. As well, we don't have to use as much compute or memory resources. So how do we actually perform this upweighting? So here's our loss function, and how we apply upweighting is we apply a weight to every single example. So here Wi would equal to 10 for every single non-black out day example. And Wi would be equal to one for every example in which a blackout did occur. That is how we can upweight the majority class after down sampling it. By the way it's usually best to perform down sampling and upweighting and then compare your results to not doing down sampling and upweighting to see how your performance is overall affected. So now we can effectively minimize our loss function, which means we can fit our model parameters here these coefficients, which minimizes the difference between the predictions and the actual values for our labels. Now, we can encounter the same problem that we had when we talked about trees which is overfitting. Overfitting will happen when a weight parameter for a feature becomes too large, which means the model is placing a lot of importance on a single feature. This can indicate that the model is too closely matching the training examples and is overfitting. So we need to be able to control the size of these parameters or coefficients, and one tool we can use to do that is called regularization. Regularization involves adding a term to the loss function which just sums up the absolute value of our weights for every single weight. So if we have model parameters like these which fit the training examples quite well, then the loss function will end up choosing a model configuration that has smaller weights even though it may sacrifice the original loss function that we talked about. So this right here is called L one, or lasso or Laplace regression, where we're just taking the absolute value of each weight and summing them through. Now there's also L two or ridge or Gaussian regularization, which squares the weight terms instead of just taking the absolute value. Now, L one regularization typically results in more zero valued coefficients, which means fewer features being used. While L two regularization usually result in small weights for many of the features that would have been zeroed out by L one regularization. The L one L two regularization terms usually have a coefficient out front, which allows us to control the degree of regularization. Too high of this value can result in under fitting, and too low of the value can result in overfitting. This parameter is best tuned through cross validation. A final technique that we can use to reduce overfitting is called early stopping. Simply put, if we're here and we're navigating our way down this curve, we can stop here instead of at the absolute minimum to avoid overfitting the training examples. Since we're now regularizing based on the value of our parameters. One thing to note here is that features on really small scales usually have larger weights to make the overall term relevant to the other features which are on larger scales. For instance, if we had hours of work per week on feature one which is between 20 and 120, and then we had a weight attached to feature number two, which was steps taken per day which ranged between 1000 and 10,000, this parameter here would force to be larger so that it could have some influence on the overall output of your model. This means that this term would be far larger than this, and this term would be penalized more in terms of loss. In order to mitigate this, we can scale our input features by using this formula. This is called min-max scaling, and it places all of your features between zero and one. As well when you scale your input features, there's a high chance that the loss function will converge faster. One of the core differences between analyzing logistic regression models compared to linear regression models is that we can't use the same R squared value that we did in linear regression. A metric typically used is called McFadden's pseudo R squared, which lies between zero and one just like R squared, but it's usually smaller than R squared itself. In McFadden's own words, a pseudo R squared between 0.2 and 0.4 usually indicates an excellent fitting model. As well, we can use the same tactics we used on the features in the linear regression such as feature interactions and using functions with the features to fit parabolas or different polynomials. Keep in mind that adding these terms does increase the chance of over-fitting the data however. As a side note, in case you get asked, recall from the naive Bayes model how we wanted to solve for this. But we didn't directly calculate this, instead we kinda flipped the question around and then multiplied it by the priors. Here in our logistic regression example, we're directly solving for this probability. This is called a generative model, and this is called a discriminative model. If you recall, during the naive Bayes optimization video, we talked about how the naive Bayes is actually a poor estimator but a great classifier. Since this logistic regression model directly solves this, it's generally a better estimator. So these probabilities can generally be trusted more than the probabilities generated by a naive Bayes model. All right, well that wraps it up for this video. Thanks for joining. Join us on the next video, where we continue our machine learning journey.