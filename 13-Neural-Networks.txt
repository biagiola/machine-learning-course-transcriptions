WEBVTT <v Instructor>Welcome back to MLExpert's</v> Machine Learning Crash Course. In this session, we're going to be talking about neural networks. We've gone over several supervised learning models, and one of them was logistic regression. In that video, we went over how to use logistic regression to separate non-linear data. As well, we also went over SPMs, which through kernels, could also classify non-linear data. Those models will work for a lot of cases in machine learning. However, there are other methods available when working with non-linear data. One such method is called a neural network. First, we're going to talk about a neuron. It takes inputs, multiplies the input by weight. So here X1 would be multiplied by W1, X2 would be multiplied by W2, and, finally, X3 would be multiplied by W3. All of these terms are then added up into this summation. That result of the summation is then passed through a non-linear function. Here, we'll call it sigma to represent the sigmoid function that we talked about in the logistic regression video. Now, this entire result is equal to our output. Now, in the supervised learning section, we talked about how that can be represented with this. So this is W transpose X. Now, this should sound very familiar, because this is effectively what logistic regression does. It has some weight, which is multiplied by some feature, and then that is put into a sigmoid function. Now, here we're missing a bias, so let's go ahead and add that. Here, this input will always be one, and then the weight or the bias here that we'll learn is the term that we would learn in our bias for logistic regression. So, this neuron and the logistic regression model that we went over are equivalent, in this case. In the logistic regression video, we also went over a loss. So here we can define our loss as the difference between y hat, our output, and y, the actual label of the example. That loss that we use here is the exact same loss that we use for logistic regression. So here we have the negative log loss summed and averaged across all the examples. Now, with that loss function, in order to update our weights appropriately, we have to take the gradient. So here we take the gradient in the same way that we did in the logistic regression video. So here we have the partial derivative of the loss with respect to the bias. We would also have the derivative of the loss with respect to weight one, which would be in here somewhere. Finally, we'd have the partial derivative of the loss with respect to weight three. Now that we have the gradient, we can go ahead and update our weights. So whatever our weights are at a particular time, we'd move in the opposite direction of our gradient and adjust by some factor called the learning rate. So one question could be, why do we need this bias here? Let's say that we wanted to train our neuron to be able to output a one, or that was the true label, so that's what we want our prediction to be. Well, since the inputs are all zero, all of these multiplications would be zero, which would mean the sum would be zero, and the value of a sigmoid function at zero is 0.5. So it's not possible to train this neuron to do what we want. Now, if we had a bias here, so this input would always be one, and we would just train a weight of how much we want to multiply this one by, if we included that in our summation, all the sudden, we would now be able to train B, such that we could output a one, 'cause now B could be trained to be something quite large maybe 20, such that our sigmoid function could actually output a one now So now that we have our trained model, this is effectively logistic regression, and we can see that we'd be able to separate these two clusters here with a linear decision boundary just as we did with logistic regression. For logistic regression and SPM, we had approaches to handle data that was not linearly separable. However, perceptrons take on another perspective. While we could use feature transforms like X squared here to separate data like this, I couldn't find a proper feature transformation that would be able to separate this data. There are other tools like k-nearest neighbors, which looks at the nearest neighbors in order to classify something. And since we have these gaps here between the spirals, this would actually work quite well. We could also use boosted trees here, but since boosted trees have to create decision boundaries that are parallel to either access, it may not be the greatest algorithm for us. So how do neurons handle this case? Well, they employ a pretty unique strategy. Let's first take a copy of this node, which is just a summation and a sigmoid, and put one over here. You know, it doesn't look like we did very much, but what we can do now is add another node here and connect all of the inputs to both this node and this node. So, effectively, what we've done is all of our inputs like logistic regression, go to this node, this node, this node, and then that is summed through a sigmoid. Additionally, though, we also have all of the inputs going to this second logistic regression function. And then we have a third logistic regression function, which takes as inputs the previous two outputs from the other neurons. This node would also have a bias, so keep that in mind. So, let's look at what happened to our parameters. We have a couple of biases here. We have now double the weight over here, and we have two added weights over here to feed into this neuron. So, let's figure out what's going on here and why we added two nodes. So what if we had data like this? Well, it's not linearly separable in the sense of here and here. We could use feature transforms here if you just multiply X1 times X2. All of the sudden this will become linearly separable, but, instead, let's say that we didn't want to use feature transformations here. In that case, we're back to a single line, right? And we can't separate this data either way we slice it. Really, the only idea is that we have two lines to separate our data. This way, everything within the lines is one class, and everything outside of the lines is another class. So how do we actually do this? Well, this neuron here, since we know it can learn only a single line could learn this line. This neuron could learn this line. And then this neuron here, since it's just a weighted sum of both of those lines, could learn to use both of those lines as tools to complete the classification. This is exactly how a neural network learns to classify non-linear data. All right, so what if we have something like this? It seems a little bit more challenging. Well, what we can do, instead of just having two neurons here, let's make four. This neuron can learn this line. This neuron can learn that line, that one, this one, and that one, that one. Finally, this neuron can learn a linear combination of all of these lines in order to classify all examples within this box as one class and all of the examples outside of the box as another. Well, what about this data? What we could do is just add a bunch more neurons and have this neuron learn this line, this neuron learn this line, and so on for maybe 1,000 neurons. And then this neuron can learn the linear combination of thousands of lines, perhaps, such that everything in this line is one category, and everything on the right side of this line is another. That is one way you could do it. And technically you only need one hidden layer, it's called, to represent any arbitrary function. However, that's not typically what's done. What's usually done is that we just add another layer of, we'll say, K neurons here. We'll call this N neurons. And K is typically a little bit smaller than N, and you generally work your way down in the case of binary classification to a single output of which class it is. Why would you do something like this? Well, what this layer can do is start to put together different lines, such that this neuron can learn a corner, this neuron can learn maybe a little zigzag, and this neuron can learn this corner over here. And then what we can do, if we have even more layers over here, this neuron, say, can connect corners and zigzags together to make a little swirl, and this neuron can learn this little swirl in here. An generally all of these neurons will learn more and more refined characteristics about the data. And then, finally, if we add enough, then this neuron can learn a linear combination of all the swirls, such that we can finally classify the data correctly. This is the preferred way to use a neural network. And as you progress your layers onward to learn more and more refined features about your data, you're increasing the depth or how deep your neural network is. This is where deep and deep learning comes from. Now, another reason that we actually want to go deeper, in most cases, than wider is because generally less parameters, these lines here, because remember each line here is a weight that we have to learn, typically it means that there's less overall parameters than if we would've went with one hidden layer to learn everything. So speaking of learning, how does this huge deep neural network with, we'll say, h hidden layers actually learn? Well, let's go through an example. Obviously, this network is a lot smaller than the one we just saw, and that's because I'm not a computer. I'll probably make a mistake if I do anything larger than this I think the still showcases exactly what's going on when we're talking about training a neural network or how a neural network learns. So let's write an equation for this network. For reference, this is going to be called h1in So h1in is going to be the hidden layer neuron one input. This is going to be hidden layer second neuron input. For every input into the neuron, we're going to have an output. So this is h1out. This is h2out. And, finally, this is yin, and, respectively, this is yout, which is going to be equal to our prediction. Keep in mind, too, even though I'm not showing them, these will have biases attached to every single neuron that we have. Okay, so let's write our first couple of equations. So h1in is going to be equal to X1 here times W1 plus X2 times W3 this year is called fully connected. So every single input connects to every single neuron in the next layer. h2 input is going to be X1 times W2 plus X2 times W4. hout is just going to be the sigmoid of hin. Same with h2. yin is going to be h1out here times W5, the weight plus, since we're summing, h2out here times W6. Finally, yout is just going to be the sigmoid of yin. So, now that we have our equation for what yout is, we need to compare that prediction or yout compared to what y should actually be or its label. So here we already went over is our loss function, which is the negative log loss or the cross entropy. These summations here and divided by N just means we're averaging across every single example that we have. But what we'll do now is just look at a single example. So we'll drop that averaging, and we'll say that our predicted y hat here this is equivalent to y hat out is 0.33, and our label for the particular example is one. So since our label is one, we're actually going to look at this side of the equation, because the side of the equation will be zeroed out. And then we'll just take the log of y hat. Now, this is equal to the log of y hat out, because that's what we defined previously. So what do we know about yout? Well, we defined yout earlier as the sigmoid of yin, and we defined yin as the sigmoid of h1out times W5 plus h2out times W6. Now, h1out was just the sigmoid of h1in, h2out, which was just a sigmoid of h2in, and then we can reach our final expansion, because h1in was X1 times W1 plus X2 times W3, and h2in was just this. So now we have our loss defined, and we should now take the gradient of the loss, so that we know how to update our parameters. Well, we can do that by taking the gradient of this function at weight one, weight two, weight three, all the biases all the way up to weight six. So, how do we actually find the derivative of these functions? Well, there's two ways. One, we can use what's called the numerical gradient. All that means is that we would take whatever value weight one is now, and we would add a small increment amount and see what happened to the output of the loss function, and then we would decrease weight one by a little bit and we would see how that affects the loss function. We would repeat this process for all the weights and the biases and update each one according to which one made the loss go down. The problem with this is it's extremely inefficient and it takes a long time. The other approach is called the analytic gradient. The analytic gradient is similar to what we've been doing before where we literally just take the derivative of the loss function with respect to some parameter and get the mathematical gradient itself. The problem is we're going to need some tools to help us accomplish this. Earlier when we were just taking the derivative of this, which is pretty simple, or the derivative of this times that, we can handle those things, but this is quite a complex function to take the derivative of. As well, we were only looking at a neural network with only one hidden layer. So one mathematical tool that we can use is called the chain rule. The chain rule states that if we want to know the partial derivative of the loss function with respect to W6, instead what we can do is multiply all of these together and they will be equivalent. So, what is this term? What this is asking for is the derivative of the loss function with respect to yout. Well, what was the loss function again? This was the loss, and we can take the derivative of this pretty easily. What about this term? It's asking for the partial derivative of yout with respect to yin. What was yout again? Yout was just the sigmoid of yin. In our last term, we have the derivative of yin with respect to W6 which is what we were looking for. What is yin again? yin was just h1out times W5 plus h2out times W6. We can take the derivative of this pretty easily too. So now that we know how to do this, I'll just condense it. Next up, I wanna figure out how we should adjust W1 in regards to the loss function. Well, let's use the chain rule on this equation. So here, the partial derivative of the loss with respect to weight one is just going to be all of the terms that trace down until we get to W1. So we start with the loss, then get to yout. Then from yout, we go to yin, yin, h1out, h1out, h1in, and then, finally, we can get to W1. Now, those of you who are Algo Experts may have noticed that these terms here and these terms here are the same. So what we can do is use dynamic programming in addition to the chain rule, such that we start with W6 first and work our way in towards W1. We can reuse this computation in this step. So to find the analytic gradient by using the chain rule and dynamic programming, this gives us something called backpropagation. Backpropagation is the standard way to train neural networks. So for training, we need a forward pass through the neural network to figure out how far our prediction is away from the actual value. And then once we have the loss, we can backpropagate those gradients to update all of the weights in our neural network. Speaking of, how do we update those weights? Well, it's very simple. It's the same equation we've been using all along. We have the current weight value. We're going to go in the opposite direction of the gradient with respect to the loss, and we're going to multiply by a factor of the learning rate. So if we plot the average loss obtained from all of the training examples against a particular value of, we'll just say, W1, it could be any W here, if we plot that, we'll end up with some function like this. The difference between this function and the function that we had with logistic regression is that here we have local optima. So before in logistic regression, we were guaranteed to have one minimum, and that would have been the global minimum. Now, if we used a single neuron, this wouldn't be the case, 'cause that would have been equivalent to logistic regression, but since we've stacked these neurons on top of each other and we've also added layers, we've opened up ourselves to local optima. Local optima, which we talked about in the k-means video, brings us an opportunity for our convergence to stop before it gets to the global optima. So our convergence could end here, here, or here. So there are some mitigation techniques that we can use that increase the chances that we won't get stuck in these local optima. One technique that we talked about earlier, stochastic gradient descent, can help with not getting stuck in a local optima. This is actually just a by-product of the fact that we're taking random examples and updating the weights with just that single example. This randomness in the weight updates can increase the chances that we don't get stuck in a local optima. The problem, however, with stochastic gradient descent, as we mentioned earlier, is that it's slow to converge. So we instead need a better solution. One of the solutions is incorporating momentum into your gradient descent. So here was our weight update. Now, the idea of momentum is to remember or to keep track of the previous updates and have this update be influenced by previous updates. So if the previous gradient before this time step's gradient was three, and let's assume the learning rates are one, so if this value was three in the previous time step, and this time step's gradient is three, then our weight update would actually be six. Now, there's a parameter we can tune, which says how much the past should influence what we're seeing now. Momentum carries this concept further by incorporating the history of every single gradient update that we've had. This is just a recursive function. So we have a weight update here. We'll now assign this to be v of t, and v of t is just going to be equal to v of t minus one minus whatever the current loss is right now. So these two equations is the same as this just written in a compact form. This overall strategy is called momentum. So let's see it in action. So here our negative gradient will be two. And here our negative gradient will be three. What we'll do is we'll add that two to the three. So our weight update would actually be five. And then we'll take this five and we'll add it to this one, because this is less of a slope here. So now our update here will be six. We physically have built momentum as we've gone down this hill, and that's why it's called momentum. Now, remember there is technically a parameter here. It's usually 0.9, so that previous gradients don't matter as much as the current gradient. So one of the problems with momentum is that even though it builds momentum to get you out of these local optima, it could build so much momentum that you actually go right past the global optima. Now there's another method called Adagrad, which adjusts the learning rate per parameter. So to see this, let's de generalize this. So let's look at a single gradient and a single weight. Now we can speak about having a learning rate for a particular parameter. This learning rate will also be different as time goes on. So, why would you wanna do something like this? Why would you want to have a different learning rate per parameter? Well, let's look at the formula and figure out why. So here, the learning rate of perimeter one at some time, t, is going to be equal to some r general, which is typically 0.01. We're then going to divide that by these square root of the squared sum of all of the previous gradients that we've seen. So what is this actually doing? So let's say that W1 is updated quite a bit, and the gradient always comes back to be pretty high. In that case, this algorithm will lower the learning rate, such that it has smoother weight updates per gradient step. On the other hand, let's say W1 only has a gradient that's very small every time. Then this value in the numerator will be small. And since we're dividing by a smaller number, the learning rate will be higher. This means that we'll be able to take more effective steps for parameters which are being updated less. Now in general, you'll see a little constant here. What this does is it stops us from dividing by zero. Now, one final note, Adagrad really helps in the case of sparse features, because if we have sparse features, that means that weights associated with those features will be updated less, and, therefore, the learning rate will be higher. Now, let's go over another method. Adam, our last method we're gonna cover, combines momentum, which we went over, and adaptive learning rates, which is what Adagrad does. So Adam's update equation looks like this. Here we have the r general. v of t is a recursive function we talked about. m here is going to be another recursively defined term. An these betas, beta one and beta two are going to be hyper parameters. Notice that the only difference between mt and vt is this squared term here. Finally, you may have noticed that these have hats on top of them. That's because we technically need to adjust these mt and vts. The reason is because these are technically moments of the function. And in order to get an unbiased moment on these functions, we have to adjust by these parameters. Now I've heard Adam be described as a ball rolling down a hill with momentum, but the ball also has friction. So take that how you will. The idea is that the friction aspect helps the parameters settle in a global optima, while the momentum helps the parameters escape the local minimum. So all of those are called optimizers. Though, you can use Adagrad, there's a couple more that we didn't go over, Adadelta, RMSprop, and we did go over Adam. if you don't know which one to start with, I would just go ahead and pick Adam. So once we have the gradients from whatever optimizer we use, multiplying these gradients together can result in a problem. So let's say that we have this network. Now, if we want to find the gradient of the loss function with respect to weight one here, by the chain rule, we need to multiply all of these terms together. Well, one major problem is that the maximum value of the sigmoid is actually 0.25. So all of these multiplications, which involve the derivative of the sigmoid, will have at maximum a value of 0.25. And if we multiply a lot of 0.25s together, this total value can brace towards zero, and we can result in an underflow. For instance, here, if we plug in all these values, we could get something very, very close to zero. Now, on the other hand, some of these term's derivative include the value of the weight itself. So if weights are extremely large, so let's say a weight over here is 100, this weight is 75, another weight in this network has the value of 68, as you're multiplying these terms together, you're going to get an extremely large value. This is called an exploding gradient, and it's the opposite of a vanishing ingredient. Both of them are problems though. So there's a few methods to mitigate this. One of them is initializing the weights of the neural network in a particular way. Some pretty bad ways to initialize the weight is just to use a uniform distribution between zero and one. Another bad way is to just initialize these parameters with a normal distribution in which the mean is zero and the standard deviation is one. What we can do instead is initialize these weights from a normal distribution in which the mean is zero and the standard deviation is this. So what is fi and of? fi is fan in, and fo is fan out. Fan in is the number of inputs to a particular layer. Fan out is the number of outputs for that layer. So here to initialize the weights of layer one, we're going to sample a normal distribution, which has a mean of zero and a standard deviation of this. In order to initialize the weights here, we have a fan in of eight and a fan out of two. So we would sample from a normal distribution according to this formula. This is called Xavier or a Glorot initialization. The reason why this helps is because we're shrinking the standard deviation by how many ever times we will be multiplying these variables together per layer. Not doing this makes the variances of each layer multiply together, and that causes the variants to grow exponentially. So if we can shrink the standard deviation down early, then these other multiplications, hopefully, won't result in exponential growth or shrinkage of our gradient. Now this works best when we're using something called a symmetric activation function. So here, these sigmoids are technically activation functions. They're symmetric, because they have proportional differences on either the y or the x-axis. Now, what if we didn't want to use a symmetric activation function? What if we wanted to use something like this? This activation function, which is called the rectified linear unit, is not symmetric. Technically it's not even differentiable, but as we talked about in previous videos, you can use subgradient methods to still perform gradient descent. What this function is is that's all negative values, their gradient will be zero, and all positive values, their gradient will be one. So why would we use this rectified linear unit or ReLU in place of a sigmoid? Well, one, it's more computationally efficient. All negative values take on the value of zero, and all positive values take on the value of itself. Likewise, when you're taking the derivative, the derivative of zero is zero, and the derivative of any value is just one. Two, empirically, it just tends to perform better than the sigmoid function. Three, one reason that it could perform better is that it has an element of sparsity to it, so it can reduce overfitting. This comes from the fact that all values that are negative on the input, the rectified linear unit will output a zero. So not all neurons will output a value, because the neurons that have a negative valued input will just output zero. So what are some downsides to the ReLU? Well, it has an uncapped activation. With sigmoid. we would have something called saturation where the value of the output of the neuron could be no larger than the value of one. However, the ReLU can output any value, and that means that we could be susceptible to exploding gradients more often. As well, we can even now be susceptible to exploding forward passes where by simply doing multiplications in the forward pass all the way through the neural network. we can also get unreasonably large numbers that overflow. So another problem called the dying ReLU problem comes from the fact that when the neuron takes on a value of zero, it will be zero forever. So let's visualize this. Let's say this ReLU got past to it a value of negative three. Then on the backpropagation when we went to go do a gradient update, the gradient will be zero. So this weight will always have the same value. That means that this neuron is completely dead. It will never output another value except zero. Even though those problems exist, rectified linear units, or ReLUs, typically perform well in practice. So for this activation function, which is asymmetric, instead of using the Xavier initialization technique, we're going to be using Kaiming initialization. Kaiming initialization is very similar to Xavier, but, in this case, it only takes in fan in as a variable. Now you want to use Kaiming initialization for any of the asymmetric activation functions that you have. For instance, we talked about the ReLU, but there's actually a lot more activation functions. One is called the leaky ReLU, which tries to get around the dead neuron problem by adding a slight angle to this or slope to this line. So far to avoid some of the degeneracies that can happen with neural networks, we found that initialization is helpful, and through that, Xavier, Kaiming, and also you'll want to scale your features in order to improve the rate at which you can converge. As for activation functions, we have gone over the sigmoid already. We've gone over ReLU and one of its variants called the leaky ReLU. There's a final activation function that is popular. We haven't gone over it yet. It's called the hyperbolic tangent or Tanh. It's extremely similar to the sigmoid function, but instead of being in the range of zero to one, TanH lies in the range of negative one to one. The idea is to cross validate between all of these activation functions to see which works best for your data. In general, though, I recommend you start with ReLU. By the way, you can use many different types of activation functions in your neural network. So here this one's using a sigmoid, and this one's using a ReLU. Finally, the last neuron will dictate what your output looks like. So for us, we use the sigmoid for binary classification. However, you can also use the softmax function in which all of these will be softmax functions, and you'll have how many ever output nodes that you have classes. By the way, the softmax function will ensure that the sum of all these outputs will equal to 100%, as we talked about earlier. And then the maximum value of the softmax output will be your prediction. So what if you wanted to use this for regression? Well, all that you have to use is a linear activation function. So what that will do is just pass the value of the summation through to the output. Now that we've gone over these activation functions for different use cases, let's go over different loss functions. For regression, we're going to use mean squared error. which is sometimes called the L2 loss. That's just going to be the difference between your predicted output and the actual output squared. Here we're showing for the entire dataset. but usually it would be for a mini batch of size N. You can use something called mean absolute error, which is sometimes called L1 loss where you just take the absolute value of the differences. For classification, you can use cross entropy, sometimes called log loss, which we've been over before with binary logistic regression. As well, you can generalize this across K classes. So now that we have our activations and we have our loss functions settled, we need to make sure we're not going to overfit. How we can do that is L1 or L2 regularization, which we've already talked about in terms of adding that to the loss. So here, this would be L1 regularization attached to the multi-class log loss. And here is the L2 regularization. One interesting thing that neural networks can do for regularization is called dropout. So dropout is when you have per layer of a neural network, a particular neuron in that layer will have some probability of sticking around. The others will be dropped out for this particular training iteration. So here, if we wanted to dropout these two nodes, our neural network would look like this. For the next training iteration, these nodes could very well come back, and these could be gone. All it depends on is this probability of a particular node being dropped out. As well, this layer can also include dropout. Now, the problem with dropout is that during training, this layer will only have 50% of the nodes present on average, this layer only 60% of the nodes present on average. When forming predictions outside of the training environment, these nodes will not be dropped out anymore. So all the sudden, these summations will be a lot higher, because we're going to have double the total amount of summations coming through, because we're no longer dropping out. That can really mess up subsequent layers during test time or prediction time. So we can use something called inverted dropout. What inverted dropout does is during training, after every mini batch, say, they'll take the output of these layers and divide by the keep alive rate or the dropout rate, depending on how you look at it. They'll do this for each layer that has dropout. This ensures that the total sum coming into here, this final node, will match on average the total sum coming into this node during prediction time or test time. So, so far for the configuration of our neural network, we've gone over feature scaling, we've gone over the activation functions we can use, the different types of loss functions, the optimizers, the regularization techniques or terms we can do, and the dropout specifications that we need per layer. So, I think the last remaining question is, how do I know what my neural network should look like? How many hidden layers should it have? How many neurons per hidden layer should it have, things like that? If your data is linearly separable, you don't need any hidden layers at all. Beyond that, I think it's safe to start with a single hidden layer, and then the number of neurons in that single hidden layer should be the average of your input and output. So for instance, if you have 11 features on the input and you're doing binary classification, which means you'll have one output node, then, generally, your hidden layer should have no more than six neurons. Another alternative is to start with more layers or units than you need, and then go examine the weights of your connections. The weights that are close to zero should allow you to prune the surrounding neuron. For instance, if this weight right here was really tiny based on this connection, then I would try to prune this neuron right here and rerun my cross validation to see how much my performance was affected. If my performance isn't affected at all, then I would finalize the removal of this neuron. So to wrap up, we learned how to train and use a fully connected neural network. We've gone over everything from feature scaling, all the way to the architecture of the neural network itself. The remainder of the videos in this section will cover different types of neural networks and their uses, but for this video, that's it. Thanks so much for joining. And join us next video as we continue our machine learning journey.