WEBVTT <v Instructor>Welcome back</v> to ML experts, machine learning crash course. In this session, we're going to go over our first unsupervised learning algorithm, K-means clustering. Let's say we know nothing about our data except for the data values themselves. If we want to learn something about our data that helps us make decisions, a starting point could be to use K-means. K-means does exactly what it sounds like. It finds K-means or averages in our data. Let's say that we were hired by team blind who owns the anonymous workplace social media app. In general, the app allows users to sign up under their company name, users can then anonymously post information for all the other users to see. Based on that content, users can view it, like it, share it and they can even direct message, the creator of that content or the person who posted it. Our team is tasked with growing the user base that's currently on the app. We know nothing about their current user base or how they got to be where they are. The only data that we have per user is how they interact with the app. So the features we have, which were obtained by some click stream, which is basically a log of clicks and interactions that users took for a particular interface, we get the likes given and received per day, the average session time, the times posted content per month, the average feed scrolling time per session and the direct messages sent or received per day. This is all we get per user. And based on these features, we want to grow the user base. Let's say our data was only in two dimensions. We have X two and X one here, and we have our data uploaded. The goal of K-means is to organize our data into K distinct groups. So for instance, if we chose K equal to two here, then what we would want K-means to accomplish is to separate these data into two groups. The first step, is to pick K random points here too and assign them to be different labels. These points are called centroids. Next what we do, is we find the distance between this centroid and every other data point there is. We repeat this process for the other centroid as well. Whichever centroid is closest to a given point, that point will take on that centroids label. So in this case, all of these points are closest to this centroid and all of these points are closest to this centroid. Then what we do is update the centroid to be the average of all the points in the given cluster. So here, the average of this cluster was right there. So now the centroid is here, the average of this cluster was right here, so that's where we'll put the centroid. We then, just repeat this process over and over again. So here we find the distances, we then update all the labels of the points based on their distance from their near centroid. Here, we can see that this cluster has actually expanded. We then update the location of the centroids to be the average of the clusters. So, if we repeat this process again, we'll notice that the centroid locations don't change. This means that our K-means has converged. So how do we figure out how well K-means did, when it comes to clustering out on labeled data? Here, we can take the distance, and by that I mean the euclidean distance between the centroid and every other data point in its cluster. We would do that for both clusters. We then, can sum up all of these distances to get something called the inertia. What this inertia tells us is how far each point in the given cluster is away from its cluster's average. Generally, we want to minimize the inertia in order to form the best clusters. Now, is this the best inertia possible? Probably not. We could verify this by finding every single cluster possible for K equals to two. But for instance, this single point could be in its own cluster and all these other points could be in a cluster and we could measure the inertia. And then we could put this point inside the clusters at this point and leave every other point out. And we could continue this process measuring the inertia every single time. If we follow this pattern for every single possible cluster, then the time complexity would be N to the KD plus one. So, how come we didn't take N to the KD plus one? Well, we use something called Lloyd's algorithm. With Lloyd's algorithm, we can get the time complexity down to NKD. However, this comes with its disadvantages. When we use Lloyd's algorithm, what we're actually doing is approximating the best cluster configuration. Let's say on the X axis here, if we could represent it we've arranged a particular cluster configuration or K equals to two. So, we've organized the points such that some subset of the points belong to one and the other subset of points belong to another group. If we plot the inertia for whatever organization that we have on the X axis here, we will get a minimum at some point. The difference between this function and all the other functions that we've wanted to get a minimum for in the past, is that this function is no longer convex. So what that means is there's more than one minimum and potentially, we could get stuck in a sub optimal point. For instance here, is a local minimum and here is the global minimum. So, let's look at how we can get stuck in these local minimum. Here, K is equal to three and we see that there are three clusters. Well, this could have happened by chance. However, let's say that randomly, we initialized two centroids in this cluster and one centroid right here. Well, we'd very likely end up with a cluster that looks like this. Why is that? Well, by definition, we said that K-means converges when the averages don't update from iteration to iteration. Here, technically, the averages would not move because every point is closest to its own centroid. This is what it would look like if we got stuck in one of these local minima. Now, there are some mitigation strategies we can use to avoid getting stuck in these local minima. One of them is called K means plus, plus but no method can guarantee we find the optimal solution except calculating the exact K-means which typically isn't a good idea in most cases. However, there are other methods. One of them is called bisected K-means. All that we do is run K-means, for K equal to two no matter what K we think we should have. Then, we measure the inertia of the two clusters. Here we got 10,000 and here we got 20,000. Whatever cluster has the most inertia, we then perform K-means on that cluster. So now we have K-means here and the inertias here are equal, so we stop. And we've only used K equals to two each time because we've bisected, but we end up with three clusters. This can be an effective way to avoid local minima. Now we need to know what K is optimal. For bisected even though we're choosing K equals to two at each iteration, we still have to select what K is best to end up with. Here, we chose K is equal to three but still we chose K equal to four, this cluster might have been split on next. So, how do we figure out what the ideal value of the final K is? There's a couple of ways, the first one is called the elbow method. All this does is we measure inertia against all of the different Ks that we can pick. We run K-means with one cluster, two clusters, three clusters all up to six and we take the inertia and graph that. The elbow method says that we should pick the elbow of this function as the best K, for K-means. Another option is that we could use the silhouette method. First, we have to calculate A of I. All A of I is the distance between a particular point and another point in the same cluster for every single point, with an average all of the results together. We then calculate BI which is the distance between a point in a cluster and every other point in its nearest neighboring cluster, and we average those values as well, for every single point in every single cluster. We then apply this formula, BI minus AI, over the max of AI, BI. If we plot the average SI for every single cluster number that we've considered, so let's say we considered K equals to one, K equals to 10. If we plot the average SI per cluster configuration, then, we'll end up with a particular peak. This peak is called the silhouette coefficient. And for us, that's K is equal to three. So, after running bisected K-means and assigning K equal to three, obtained from using the silhouette method, this is how our data, for the blind users was organized. So, we can see three distinct clusters, but how do we use this to our advantage to make any decisions? Well, what we can do is look at the features of these users in this cluster here. And we can repeat that for every other cluster and see if their features vary in any way. After doing this, what we found was that this cluster consumes a lot of material. Their average scroll session is higher, the amount of likes that they give to post is higher, things like that. This cluster, which is quite smaller is organized of content creators. Their scroll time is often a little bit less, they don't like as many other posts but they do spend a lot of time writing their own posts. This cluster right here was a combination of the two. Some of them were first-time writers, some of them have only written a couple of times but they still kept up with their feed and liked a lot of other's posts. Like I said, we didn't know how their user base got to where it is. Fortunately, they had the same data from years ago. And what we did is we ran K-means or the same exact user base just in the past. What we saw, was that obviously there were less number of users because the app has grown and user base but the clusters themselves still aligned in this particular way. Then, a year or two later, we saw it grow a little bit more and then finally, today, this is what it looks like. So since we were tasked with growing the user base, what this tells us is how we need to grow to maintain the same type of ecosystem that they have now in terms of users. So, what are our levers that we can pull here that will help control the way that we can grow the user base? At a high level, we have a few points to control growth. Here, we can control the transitions that users make. So basically, full-time readers to a little bit of both to full-time content creators. We can also grow this cluster by maybe referral programs or offering premium benefits for free. Even generally, we can just increase advertising. However, we don't wanna do that so much that there becomes too many readers or content consumers and not enough content creators. What would happen here is content would become sparse and readers would get bored and move on and maybe even delete the app. So, what we need to control are these flows here of how to get readers into this category, and then from this category which they're doing both reading and writing content, we need to then transition those into full-time writers. This way, we can put a lot of investment over here in terms of growing the reader base. And as long as we can control the flow into writers, then we don't have a problem with growth. We can transition readers into first-time writers slash sometimes readers, sometimes writers by encouraging these users to write something. This can be done by emailing these users, mentioning how people want to hear about events that took place at their company such as layoffs, acquisitions or product releases. So after we get some users into this middle category here, we want to now get them to the writer category. We can do a couple of things here. One, is we can push their first-time content to specific readers who we think would enjoy the content. And if the response is poor at first, because no one's good at writing the first time they do it, so even if these people don't like their content we can have chat bots, reach out to them encouraging them to keep writing. We can also inflate their likes of their content to motivate them to continue writing and refining the craft. Once they've become a decent writer, we can then begin pushing out their content to readers such that they would enjoy reading. At that point, we hypothesized that they will convert to full-time writers if the response is big enough. Finally, if we want to increase the size of the writers inorganically, we can simply hire writers to get onto the platform. The idea is that all of these strategies together can responsibly grow the platform. So, one interesting thing we can do with these clusters as well, is let's say we get a new customer and we don't yet know what stage they're in in terms of their usage, what we can do is use a centroid classifier and it works very similarly to K nearest neighbors such that this point would be a purple point. Now there's different variations we can use in terms of K-means. We can use K-medians, which is yes, the median instead of the mean. How this works in practice, is you use Manhattan distance instead of Euclidean distance. K-medoids requires that the cluster average be approximated by the closest point. So for K-means, we actually did use the cluster average. K- medoids would require us to find the cluster average and then find the closest real data point to that average and that would be assigned as the medoid. Finally K-modes is useful if we don't have numerical data instead it gauges distance on the dissimilarities of the examples and it uses the modes or the most recurring elements in those dissimilarities to organize these clusters. So, what if our data looks like this? Well, if we use K-means then this would be what we would get. Literally, this is not what we wanted. We wanted the outer ring to be one category and the inner circle to be another category. Well, is there a clustering algorithm out there that can help us cluster our data like this? The answer is yes, all the algorithm does which is called agglomerative clustering takes the two nearest points and combines them into a cluster. So for instance, these were the closest points out of all the other points and it would combine them into a cluster. Then, it would find the next two closest points and it would combine them. Now, this would carry on until there were two or maybe even just one cluster but then you would use silhouette method to figure out that two clusters is actually best here. And that's how we can cluster this circular ring data. So now that we have clusters, say this cluster and this cluster, we now have to figure out a way to gauge the distance between a point and a cluster. It's very easy to tell the distance between two points. We would just use Euclidean distance, but if we have a cluster next to a point and we need to know if we should merge those or if we should merge a point and another point, because their distance is smaller, we can use something called single linkage clustering. Single linkage clustering takes the closest points in each cluster and uses those to measure the distance between the two clusters. Complete linkage clustering takes the two furthest points in the cluster and uses those to measure the distance between the clusters. So I'd wrap this up, a great library it's called the scikit-learn. For K-means, it actually uses K-means plus, plus under the hood and it also supports agglomerative clustering. If you're using spark, they actually offer bisected K-means. Now, remember that this algorithm is distance based so we should keep scaling in mind. And generally, we should standardize our features which means we would take our features, say if we were just looking at all the X ones we would find the average of the X one and the standard deviation of the X one. And then we would take every feature XI, subtract the average from it divided by the X one standard deviation and that would be our new standardized feature. Just as in K nearest neighbors, the KD tree can help performance and as well it's vulnerable to high dimensional data. That's it for this video. Thanks for joining. Join us in the next video to continue our machine learning journey.