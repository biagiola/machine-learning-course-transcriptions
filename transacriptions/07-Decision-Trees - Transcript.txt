WEBVTT

1
00:00:01.020 --> 00:00:02.490
<v Instructor>Welcome back to MLExperts,</v>

2
00:00:02.490 --> 00:00:04.120
Machine Learning Crash Course.

3
00:00:04.120 --> 00:00:06.970
In this episode, we're gonna talk about decision trees.

4
00:00:06.970 --> 00:00:09.830
For instance, let's say we want to determine out of a group

5
00:00:09.830 --> 00:00:13.530
who will like a particular unreleased movie.

6
00:00:13.530 --> 00:00:15.920
We can take a portion of the group of people

7
00:00:15.920 --> 00:00:18.090
and show them a preview of the movie,

8
00:00:18.090 --> 00:00:20.610
and then ask if they would go see that movie

9
00:00:20.610 --> 00:00:21.920
if it were released.

10
00:00:21.920 --> 00:00:24.130
We can also ask them to fill out a survey.

11
00:00:24.130 --> 00:00:25.660
The survey could include questions

12
00:00:25.660 --> 00:00:28.360
about what movies they've previously enjoyed.

13
00:00:28.360 --> 00:00:30.850
Now, if we send the same survey to the rest of the group

14
00:00:30.850 --> 00:00:32.270
who didn't preview the movie,

15
00:00:32.270 --> 00:00:34.910
we could train a decision tree to predict those in the group

16
00:00:34.910 --> 00:00:37.590
who will go see the movie once it's released.

17
00:00:37.590 --> 00:00:39.230
If the survey only had two questions,

18
00:00:39.230 --> 00:00:40.870
one about liking "Finding Nemo"

19
00:00:40.870 --> 00:00:42.670
and the other about liking "The Dark Knight,"

20
00:00:42.670 --> 00:00:44.620
the decision tree would look like this.

21
00:00:45.750 --> 00:00:47.890
If they answer yes, we put them on the left node.

22
00:00:47.890 --> 00:00:50.180
If they answer no, we put them on the right node.

23
00:00:50.180 --> 00:00:52.130
This particular person answered yes.

24
00:00:52.130 --> 00:00:53.430
They did like "Finding Nemo."

25
00:00:53.430 --> 00:00:55.890
Then we asked them, "Did you like "The Dark Knight"?"

26
00:00:55.890 --> 00:00:57.240
The same process follows.

27
00:00:57.240 --> 00:00:58.850
Put them on the left if they liked "Dark Knight,"

28
00:00:58.850 --> 00:01:00.570
and we put them on the right if they don't.

29
00:01:00.570 --> 00:01:03.450
In this case, the person didn't like "The Dark Knight."

30
00:01:03.450 --> 00:01:05.220
So now that we've exhausted our questions,

31
00:01:05.220 --> 00:01:08.520
we simply change this leaf node to positive,

32
00:01:08.520 --> 00:01:11.560
because this person indicated that they would in fact go

33
00:01:11.560 --> 00:01:15.760
and see the unreleased movie if it were to be released.

34
00:01:15.760 --> 00:01:18.323
We then repeat this process for every other person.

35
00:01:19.890 --> 00:01:22.350
So what we have now is three leaf nodes.

36
00:01:22.350 --> 00:01:25.470
One person who landed over here who said they would,

37
00:01:25.470 --> 00:01:27.110
another person landed over here

38
00:01:27.110 --> 00:01:29.110
said they would also go see the unreleased movie,

39
00:01:29.110 --> 00:01:30.517
and this person who answered,

40
00:01:30.517 --> 00:01:31.747
"No, I don't like "Finding Nemo",

41
00:01:31.747 --> 00:01:33.430
"but yes, I like "Dark Knight."

42
00:01:33.430 --> 00:01:35.720
they answered they would not go and see the movie

43
00:01:35.720 --> 00:01:37.400
based on the preview they were shown.

44
00:01:37.400 --> 00:01:40.670
So, let's say that we wanted to predict within these people

45
00:01:40.670 --> 00:01:42.950
who didn't get to go see the preview

46
00:01:42.950 --> 00:01:44.220
of the unreleased movie,

47
00:01:44.220 --> 00:01:46.430
but they did fill out the same survey

48
00:01:46.430 --> 00:01:48.600
as the people who previewed the movie.

49
00:01:48.600 --> 00:01:49.990
Let's say we wanted to predict

50
00:01:49.990 --> 00:01:51.810
whether they would go and see it.

51
00:01:51.810 --> 00:01:53.340
So we hand them the same survey,

52
00:01:53.340 --> 00:01:56.410
and based on their responses, we traverse down this tree.

53
00:01:56.410 --> 00:01:57.810
And we can see that this first person,

54
00:01:57.810 --> 00:02:01.160
they ended on a positive node, which means we would predict

55
00:02:01.160 --> 00:02:03.200
that they would in fact go see the movie

56
00:02:03.200 --> 00:02:04.660
if it were to be released.

57
00:02:04.660 --> 00:02:06.030
We can just repeat this process

58
00:02:06.030 --> 00:02:07.520
for every person in the sample.

59
00:02:07.520 --> 00:02:09.760
Four people are predicted to would have gone

60
00:02:09.760 --> 00:02:10.980
to see it if it were released,

61
00:02:10.980 --> 00:02:13.210
seven people were predicted to not have gone seen it,

62
00:02:13.210 --> 00:02:15.900
and four people landed in this node over here,

63
00:02:15.900 --> 00:02:17.340
which means we don't know.

64
00:02:17.340 --> 00:02:18.650
We could have sampled more people

65
00:02:18.650 --> 00:02:19.940
to maybe fill in that node,

66
00:02:19.940 --> 00:02:21.740
but our sample size was too small.

67
00:02:21.740 --> 00:02:24.060
Okay, so there's several methods that we can use

68
00:02:24.060 --> 00:02:26.340
to build this tree for us, but we're gonna concentrate

69
00:02:26.340 --> 00:02:27.370
on something called CART,

70
00:02:27.370 --> 00:02:30.040
or Classification and Regression Trees.

71
00:02:30.040 --> 00:02:32.050
Let's dive into a real example.

72
00:02:32.050 --> 00:02:34.590
We were recently hired by Precision Drilling Corporation,

73
00:02:34.590 --> 00:02:37.460
which is among the top oil drilling companies in the U.S.

74
00:02:37.460 --> 00:02:39.560
Drilling for oil is an expensive project.

75
00:02:39.560 --> 00:02:41.950
As well, when you commit to drilling for oil

76
00:02:41.950 --> 00:02:44.680
at a particular site, you have no way of knowing

77
00:02:44.680 --> 00:02:47.160
if you'll actually hit the amount of oil output

78
00:02:47.160 --> 00:02:48.260
that you'd like to hit.

79
00:02:48.260 --> 00:02:49.680
Precision Drilling would like us

80
00:02:49.680 --> 00:02:52.740
to report which potential drill sites they should commit to

81
00:02:52.740 --> 00:02:54.650
in order to recover the most oil,

82
00:02:54.650 --> 00:02:57.050
and therefore afford them the highest profits.

83
00:02:57.050 --> 00:02:58.820
We have access to drill site measurements

84
00:02:58.820 --> 00:03:01.210
for the past successful and unsuccessful sites

85
00:03:01.210 --> 00:03:03.150
to use this data to train our model.

86
00:03:03.150 --> 00:03:05.570
The features we have are how porous the rock

87
00:03:05.570 --> 00:03:08.520
we're drilling into is, how much natural radiation

88
00:03:08.520 --> 00:03:12.100
is emitted from the rock, the density of the rock,

89
00:03:12.100 --> 00:03:14.190
and the time required for a sound wave

90
00:03:14.190 --> 00:03:15.510
to travel through the rock.

91
00:03:15.510 --> 00:03:17.320
Of these features, there are different units,

92
00:03:17.320 --> 00:03:19.140
and they exist on different scales.

93
00:03:19.140 --> 00:03:22.600
For instance, porosity will sit between zero and 0.5,

94
00:03:22.600 --> 00:03:26.000
and sonic can be in the thousandths of milliseconds.

95
00:03:26.000 --> 00:03:29.120
Fortunately for us, CARTs are invariant to scale,

96
00:03:29.120 --> 00:03:30.600
and are pretty robust to outliers.

97
00:03:30.600 --> 00:03:32.410
So we can leave out feature scaling.

98
00:03:32.410 --> 00:03:35.140
The features we have are reported for different depths.

99
00:03:35.140 --> 00:03:38.880
So for each depth, 1,000, 2,000, 3,000, 4,000 meters,

100
00:03:38.880 --> 00:03:41.180
we have all of these measurements.

101
00:03:41.180 --> 00:03:43.350
Taking these measurements is expensive.

102
00:03:43.350 --> 00:03:46.520
Not as expensive as drilling for oil, but definitely costly.

103
00:03:46.520 --> 00:03:47.930
As such, some of the measurements

104
00:03:47.930 --> 00:03:50.350
are actually skipped to save cost.

105
00:03:50.350 --> 00:03:52.970
Fortunately for us, CARTs can handle missing data

106
00:03:52.970 --> 00:03:54.930
in the training set, as well as for examples

107
00:03:54.930 --> 00:03:56.690
we want to make predictions for.

108
00:03:56.690 --> 00:03:59.210
The labels we're provided with per feature set

109
00:03:59.210 --> 00:04:01.500
are binary indicators which represent

110
00:04:01.500 --> 00:04:04.250
whether the project was profitable or not profitable

111
00:04:04.250 --> 00:04:06.200
after committing to that drilling site.

112
00:04:07.360 --> 00:04:08.540
So let's start with a few examples

113
00:04:08.540 --> 00:04:10.760
to see how we build our CART.

114
00:04:10.760 --> 00:04:13.800
Let's start by figuring out how to split up our examples

115
00:04:13.800 --> 00:04:15.160
by their labels.

116
00:04:15.160 --> 00:04:17.100
So we only have numerical features.

117
00:04:17.100 --> 00:04:18.520
So according to the CART algorithm,

118
00:04:18.520 --> 00:04:22.010
we need to decide which feature we should split on,

119
00:04:22.010 --> 00:04:24.340
and also if we do find a feature to split on,

120
00:04:24.340 --> 00:04:28.130
what value of that feature is best to make the split for.

121
00:04:28.130 --> 00:04:30.040
The goal is to find the best feature

122
00:04:30.040 --> 00:04:34.960
and value which separates our examples by labels the most.

123
00:04:34.960 --> 00:04:38.320
So for each feature, we need to figure out the best value

124
00:04:38.320 --> 00:04:40.020
to split on within that feature.

125
00:04:40.020 --> 00:04:42.180
Let's say that we were going to look at porosity

126
00:04:42.180 --> 00:04:45.470
at 1,000 meters, which is this first column here.

127
00:04:45.470 --> 00:04:46.410
The first thing we should do

128
00:04:46.410 --> 00:04:49.090
is order our examples by that column.

129
00:04:49.090 --> 00:04:49.923
So we did that here.

130
00:04:49.923 --> 00:04:54.670
We have 0.02, 0.07, 0.13 and 0.21.

131
00:04:54.670 --> 00:04:58.293
Then we just find the average between each adjacent data.

132
00:04:59.140 --> 00:05:04.140
So for instance, 0.045 is the average between these two,

133
00:05:04.810 --> 00:05:06.830
this is the average between these two,

134
00:05:06.830 --> 00:05:09.450
and this is the average between these two.

135
00:05:09.450 --> 00:05:13.440
Each of these averages is a split point that we'll evaluate.

136
00:05:13.440 --> 00:05:17.330
So we have the feature here, the porosity at 1,000 meters,

137
00:05:17.330 --> 00:05:19.080
and we also have each split point

138
00:05:19.080 --> 00:05:21.650
we want to evaluate for this porosity.

139
00:05:21.650 --> 00:05:24.520
We take all of our examples and we put them in a node,

140
00:05:24.520 --> 00:05:28.480
and if the value is less than a split point, you go left.

141
00:05:28.480 --> 00:05:31.720
If the value of the split point is greater in your example,

142
00:05:31.720 --> 00:05:32.790
then you go right.

143
00:05:32.790 --> 00:05:37.320
So for instance, 0.02 is less than 0.045, so it went left,

144
00:05:37.320 --> 00:05:38.670
all the other ones went right.

145
00:05:38.670 --> 00:05:43.000
And we evaluate every single adjacent split point like this.

146
00:05:43.000 --> 00:05:44.950
So how do we evaluate the effectiveness

147
00:05:44.950 --> 00:05:46.100
of each of these splits?

148
00:05:46.100 --> 00:05:47.010
Well, we can use something

149
00:05:47.010 --> 00:05:49.430
called Gini Impurity to help us out.

150
00:05:49.430 --> 00:05:52.340
The first step is we find the probability

151
00:05:52.340 --> 00:05:54.720
of getting class 1, square it,

152
00:05:54.720 --> 00:05:57.300
and add that to the probability of getting class 0,

153
00:05:57.300 --> 00:05:59.720
and square it, in a particular node.

154
00:05:59.720 --> 00:06:03.387
For reference X1 and X2 were successful drilling sites.

155
00:06:03.387 --> 00:06:07.570
X3 and X4 were not successful and are class 0.

156
00:06:07.570 --> 00:06:10.460
So here the Gini Impurity of this node

157
00:06:10.460 --> 00:06:12.620
is going to be 1 minus the probability

158
00:06:12.620 --> 00:06:16.300
of getting class 1 squared, which is 1 squared,

159
00:06:16.300 --> 00:06:18.970
plus the probability of getting class 0,

160
00:06:18.970 --> 00:06:21.950
which is 0 since there's no class 0's here.

161
00:06:21.950 --> 00:06:23.810
Three and four are not in this node,

162
00:06:23.810 --> 00:06:25.690
and you simply add those up.

163
00:06:25.690 --> 00:06:29.190
So 1 minus 1 is 0.

164
00:06:29.190 --> 00:06:32.620
Now for this node, the probability of getting an example

165
00:06:32.620 --> 00:06:37.620
with class 1 is 33%, because there's only one example here,

166
00:06:37.660 --> 00:06:39.670
X2, which is class 1.

167
00:06:39.670 --> 00:06:42.050
And the other examples, the other two examples,

168
00:06:42.050 --> 00:06:46.340
are class 0, so that probability is 0.66.

169
00:06:46.340 --> 00:06:48.540
Crunching those numbers, that gives us 0.45.

170
00:06:49.530 --> 00:06:52.570
Now the next step is to weight each node.

171
00:06:52.570 --> 00:06:55.680
So here we only contain one out of the four

172
00:06:55.680 --> 00:06:57.840
total examples among the split,

173
00:06:57.840 --> 00:07:01.170
so we times this result by 0.25.

174
00:07:01.170 --> 00:07:05.750
And here we contain 3/4 or 75% of the examples,

175
00:07:05.750 --> 00:07:08.230
so we multiply it by 0.75.

176
00:07:08.230 --> 00:07:12.810
That will give us 0.3375, which is the Gini Impurity

177
00:07:12.810 --> 00:07:15.860
produced by a split like that.

178
00:07:15.860 --> 00:07:17.550
Then we just continue and do the same thing

179
00:07:17.550 --> 00:07:19.240
for all the other splits.

180
00:07:19.240 --> 00:07:21.760
Notice that the lowest Gini Impurity produced

181
00:07:21.760 --> 00:07:23.700
is from this split right here.

182
00:07:23.700 --> 00:07:26.230
What we have is a very clean separation,

183
00:07:26.230 --> 00:07:28.700
a perfect separation, if you will,

184
00:07:28.700 --> 00:07:30.740
because the Impurity is 0,

185
00:07:30.740 --> 00:07:33.410
where X1 and X2 belong to the same class,

186
00:07:33.410 --> 00:07:35.110
and they're the only ones in that node.

187
00:07:35.110 --> 00:07:37.450
And X3 and X4 belong to the same class,

188
00:07:37.450 --> 00:07:38.980
and they're the only ones in that node.

189
00:07:38.980 --> 00:07:41.260
So in our example, for this case,

190
00:07:41.260 --> 00:07:43.970
the best split for porosity,

191
00:07:43.970 --> 00:07:47.080
and since you can't beat Gini Impurity of 0,

192
00:07:47.080 --> 00:07:50.530
our first split will be porosity at 1,000 meters

193
00:07:50.530 --> 00:07:51.733
with a split of 0.1.

194
00:07:53.560 --> 00:07:56.610
Now, let's say that we didn't achieve a Gini index of 0.

195
00:07:56.610 --> 00:07:58.530
What we would do is just go through

196
00:07:58.530 --> 00:08:01.680
and do the same thing with every other feature.

197
00:08:01.680 --> 00:08:02.870
But fortunately for us, like I said

198
00:08:02.870 --> 00:08:05.570
we can't beat a Gini index of zero.

199
00:08:05.570 --> 00:08:07.560
So we will pick porosity

200
00:08:07.560 --> 00:08:10.860
at 1000 meters with a split point of 0.1.

201
00:08:10.860 --> 00:08:13.920
So once we do have this first split point settled,

202
00:08:13.920 --> 00:08:17.390
we just repeat this process recursively for each node.

203
00:08:17.390 --> 00:08:20.580
So unfortunately we can't split these nodes anymore

204
00:08:20.580 --> 00:08:22.440
but if we could, we would just recursively

205
00:08:22.440 --> 00:08:25.490
repeat the same exact process for every subsequent node.

206
00:08:25.490 --> 00:08:28.960
And yes, you can use the same features again.

207
00:08:28.960 --> 00:08:30.350
That's completely fine.

208
00:08:30.350 --> 00:08:31.780
So this tree is an example

209
00:08:31.780 --> 00:08:34.290
of what could come from other data.

210
00:08:34.290 --> 00:08:37.660
So here we split on porosity first at 0.1.

211
00:08:37.660 --> 00:08:39.880
Down here, we split on Sonic.

212
00:08:39.880 --> 00:08:41.740
Then we split on Sonic again.

213
00:08:41.740 --> 00:08:42.990
And then basically we ended up

214
00:08:42.990 --> 00:08:46.010
with leaf nodes that contain all of the data.

215
00:08:46.010 --> 00:08:48.270
And each of these data will have their own labels.

216
00:08:48.270 --> 00:08:50.650
When do we stop splitting?

217
00:08:50.650 --> 00:08:52.900
Well, we can assign a max depth to a tree.

218
00:08:52.900 --> 00:08:56.170
We can assign a minimum number of examples per node.

219
00:08:56.170 --> 00:08:57.400
Let's say, we said 10.

220
00:08:57.400 --> 00:09:02.400
Though if a node has 10 examples, then we can't split.

221
00:09:02.690 --> 00:09:04.720
Or we could go until the nodes are pure

222
00:09:04.720 --> 00:09:07.150
which could introduce quite a bit of overfitting

223
00:09:07.150 --> 00:09:08.340
with our model.

224
00:09:08.340 --> 00:09:09.173
All right.

225
00:09:09.173 --> 00:09:11.220
So now that we have our decision tree built,

226
00:09:11.220 --> 00:09:12.160
how do we use it?

227
00:09:12.160 --> 00:09:15.110
So let's say that we have Xi in here.

228
00:09:15.110 --> 00:09:16.820
It's an unseen example that we haven't seen

229
00:09:16.820 --> 00:09:17.730
in our training data.

230
00:09:17.730 --> 00:09:19.890
We would simply navigate down the tree like we did

231
00:09:19.890 --> 00:09:22.800
on the toy example at the beginning of the video.

232
00:09:22.800 --> 00:09:25.720
Then once we got here, we would look at this node.

233
00:09:25.720 --> 00:09:30.620
Let's say that four of them were profitable drill sites,

234
00:09:30.620 --> 00:09:31.550
as a result.

235
00:09:31.550 --> 00:09:34.030
Two of them were negative drill sites.

236
00:09:34.030 --> 00:09:37.530
So this unlabeled example that we've seen, we would label it

237
00:09:37.530 --> 00:09:42.110
as a vote of all of the examples present on that node.

238
00:09:42.110 --> 00:09:43.990
So we would assign this unseen example

239
00:09:43.990 --> 00:09:45.840
to be a profitable drill site.

240
00:09:45.840 --> 00:09:47.860
That would be our prediction.

241
00:09:47.860 --> 00:09:50.030
Okay. So how do we handle missing data?

242
00:09:50.030 --> 00:09:52.000
Well, it's actually pretty easy.

243
00:09:52.000 --> 00:09:53.160
Remember when we were going through

244
00:09:53.160 --> 00:09:54.780
and we were establishing split points

245
00:09:54.780 --> 00:09:57.750
for features and then the split point values themselves.

246
00:09:57.750 --> 00:10:02.750
Well, all we do is we keep track of the N best split points.

247
00:10:02.890 --> 00:10:05.380
So let's say that the first split point we found porosity

248
00:10:05.380 --> 00:10:08.510
at 1000 meters at 0.1, let's say that feature

249
00:10:08.510 --> 00:10:13.510
did produce a Gini Impurity split of zero.

250
00:10:13.760 --> 00:10:15.030
Well, then what we do is we would go

251
00:10:15.030 --> 00:10:17.130
to the next best feature split.

252
00:10:17.130 --> 00:10:21.640
So let's say Sonic at 2000 meters at 0.4 value

253
00:10:21.640 --> 00:10:25.860
that produced the next best Gini Impurity for the split.

254
00:10:25.860 --> 00:10:27.010
And then we just continue down

255
00:10:27.010 --> 00:10:29.080
for effectively all the features.

256
00:10:29.080 --> 00:10:30.950
These are called surrogate splits.

257
00:10:30.950 --> 00:10:32.690
And what they do is let's say we get

258
00:10:32.690 --> 00:10:36.030
in an unseen example that is missing some data.

259
00:10:36.030 --> 00:10:38.360
This example, they happen to not take the measurement

260
00:10:38.360 --> 00:10:42.550
of porosity at 1000 meters, which was our most predictive

261
00:10:42.550 --> 00:10:44.550
and our best feature to split on.

262
00:10:44.550 --> 00:10:45.630
So what then we would do,

263
00:10:45.630 --> 00:10:47.980
is we'd go to the next best feature to split on.

264
00:10:47.980 --> 00:10:49.900
And let's say that somewhere in here.

265
00:10:49.900 --> 00:10:51.730
And we would split on that instead.

266
00:10:51.730 --> 00:10:53.620
And we would just continue all the way down the tree

267
00:10:53.620 --> 00:10:54.453
in the same way.

268
00:10:54.453 --> 00:10:56.240
So that's how we'd handle missing data.

269
00:10:56.240 --> 00:10:58.120
What about multi-class labels?

270
00:10:58.120 --> 00:10:59.530
Now let's say that instead

271
00:10:59.530 --> 00:11:02.070
of deeming a drill site profitable

272
00:11:02.070 --> 00:11:04.770
or not profitable, let's say now that they want

273
00:11:04.770 --> 00:11:06.710
to know what drilling technique they should use.

274
00:11:06.710 --> 00:11:09.000
So there's horizontal, there's vertical,

275
00:11:09.000 --> 00:11:11.070
and hydraulic drilling techniques.

276
00:11:11.070 --> 00:11:13.850
Well, now we just have to add another element

277
00:11:13.850 --> 00:11:14.960
to our Gini Impurity.

278
00:11:14.960 --> 00:11:18.320
Let's say that the label for example two was 1.

279
00:11:18.320 --> 00:11:20.910
The labels for example three and four are 0.

280
00:11:20.910 --> 00:11:24.260
And the labels for example one is 2.

281
00:11:24.260 --> 00:11:27.210
We'll call 0 to be vertical, 1 to be horizontal

282
00:11:27.210 --> 00:11:30.090
and 2 to be hydraulic.

283
00:11:30.090 --> 00:11:32.780
So the only difference is we just calculate Gini Impurity

284
00:11:32.780 --> 00:11:34.140
with this added term

285
00:11:34.140 --> 00:11:36.730
but it's exactly the same way that we did it

286
00:11:36.730 --> 00:11:38.260
with just two labels.

287
00:11:38.260 --> 00:11:40.280
So now how do we handle multi-class when it comes

288
00:11:40.280 --> 00:11:41.220
to predictions?

289
00:11:41.220 --> 00:11:43.660
Well, we still do the same thing, going all the way down.

290
00:11:43.660 --> 00:11:45.240
And let's say, we end up this node.

291
00:11:45.240 --> 00:11:47.970
Now this node will be containing examples

292
00:11:47.970 --> 00:11:49.300
through three different classes:

293
00:11:49.300 --> 00:11:52.500
hydraulic, vertical or horizontal drilling methods.

294
00:11:52.500 --> 00:11:56.780
And now just take the relative majority or the mode.

295
00:11:56.780 --> 00:11:58.900
And here that looks like this class.

296
00:11:58.900 --> 00:12:02.410
So now this example would be labeled as this class

297
00:12:02.410 --> 00:12:05.550
which in this case, green was hydraulic drilling.

298
00:12:05.550 --> 00:12:07.210
Okay. So we know how to handle missing data.

299
00:12:07.210 --> 00:12:10.070
We know how to handle multiple classes.

300
00:12:10.070 --> 00:12:11.290
What about regression?

301
00:12:11.290 --> 00:12:14.120
So what if instead of predicting profitability

302
00:12:14.120 --> 00:12:17.630
they actually want us to predict barrels of oil a day.

303
00:12:17.630 --> 00:12:19.520
So it's gonna range from a hundred

304
00:12:19.520 --> 00:12:21.350
to a thousand barrels per day.

305
00:12:21.350 --> 00:12:22.183
So in this case,

306
00:12:22.183 --> 00:12:24.720
when we're making these split points on the features,

307
00:12:24.720 --> 00:12:26.110
we're still gonna make the split points

308
00:12:26.110 --> 00:12:27.610
on the features in the same way,

309
00:12:27.610 --> 00:12:29.580
ordering them and taking the averages

310
00:12:29.580 --> 00:12:30.770
of the adjacent features.

311
00:12:30.770 --> 00:12:33.620
However, we're no longer gonna use the Gini Impurity.

312
00:12:33.620 --> 00:12:36.180
We're instead gonna use the Mean Squared Error.

313
00:12:36.180 --> 00:12:39.440
All this means is that we are summing the difference

314
00:12:39.440 --> 00:12:41.127
between all the values in the node

315
00:12:41.127 --> 00:12:43.520
and the average of that node.

316
00:12:43.520 --> 00:12:47.710
So here we only have one example, X1 is equal to 320.

317
00:12:47.710 --> 00:12:49.810
So that's 320 is the average.

318
00:12:49.810 --> 00:12:51.070
It's the only example.

319
00:12:51.070 --> 00:12:53.260
And we only have one example to sum through,

320
00:12:53.260 --> 00:12:54.920
so that's also 320.

321
00:12:54.920 --> 00:12:57.010
We square that term divided by one

322
00:12:57.010 --> 00:12:59.840
because there's one example and we get zero.

323
00:12:59.840 --> 00:13:02.190
Basically what this is saying is the difference

324
00:13:02.190 --> 00:13:07.190
between this example and the average of this node is zero

325
00:13:07.560 --> 00:13:09.570
which is the case because there's only one example.

326
00:13:09.570 --> 00:13:11.210
So what about this node?

327
00:13:11.210 --> 00:13:15.710
Well, the average of this node is going to be 421

328
00:13:15.710 --> 00:13:18.030
and let's expand this summation out.

329
00:13:18.030 --> 00:13:22.897
So we have the leaf of X2 is going to be 140.

330
00:13:24.020 --> 00:13:25.330
So we'll plug that in.

331
00:13:25.330 --> 00:13:27.390
And this is actually, this is X4.

332
00:13:27.390 --> 00:13:31.050
So X4 is going to be 413.

333
00:13:31.050 --> 00:13:33.090
There's three nodes, so our n is three.

334
00:13:33.090 --> 00:13:37.600
So we actually get 54,182 as the Mean Squared Error.

335
00:13:37.600 --> 00:13:40.760
Now that's the Mean Squared Error of each node?

336
00:13:40.760 --> 00:13:43.650
However, we actually want the Mean Squared Error

337
00:13:43.650 --> 00:13:45.560
of the entire split.

338
00:13:45.560 --> 00:13:48.750
Now to do that, we'll actually just take all the terms

339
00:13:48.750 --> 00:13:51.180
that we've had in all of the numerators before.

340
00:13:51.180 --> 00:13:55.150
So here, this is X1 minus the average of X1

341
00:13:55.150 --> 00:13:59.630
and this is X4 minus the average of this node

342
00:13:59.630 --> 00:14:03.510
and we're going to divide by the total number of examples.

343
00:14:03.510 --> 00:14:05.640
This will give us the Mean Squared Error

344
00:14:05.640 --> 00:14:07.540
of the entire split.

345
00:14:07.540 --> 00:14:11.650
Here, that's 40636.5.

346
00:14:11.650 --> 00:14:14.300
Now we want to compare the split Mean Squared Error

347
00:14:14.300 --> 00:14:15.930
with the other splits.

348
00:14:15.930 --> 00:14:18.490
So next we'll do this split here

349
00:14:18.490 --> 00:14:21.350
and we can find each nodes' Mean Squared Error.

350
00:14:21.350 --> 00:14:25.710
So here that'd be 8,100, and that would be 22052,

351
00:14:25.710 --> 00:14:27.670
just in the same way that we calculated each

352
00:14:27.670 --> 00:14:29.650
of these nodes' Mean Squared Error.

353
00:14:29.650 --> 00:14:32.190
Now to find the Mean Squared Error of the entire split,

354
00:14:32.190 --> 00:14:34.590
as we mentioned before, we have to take all

355
00:14:34.590 --> 00:14:37.630
of the squared errors in the numerator

356
00:14:37.630 --> 00:14:41.000
and divide by the total number of examples

357
00:14:41.000 --> 00:14:42.220
across the split.

358
00:14:42.220 --> 00:14:43.860
Here, that's four as well.

359
00:14:43.860 --> 00:14:48.710
So the Mean Squared Error of this split is 15,076.

360
00:14:48.710 --> 00:14:52.200
And we'll follow the same process for this split over here.

361
00:14:52.200 --> 00:14:55.570
That would give us 14,150.

362
00:14:55.570 --> 00:14:57.910
Now the split that we will select,

363
00:14:57.910 --> 00:15:00.180
out of all three of these, will be the split

364
00:15:00.180 --> 00:15:03.630
that gives us the smallest Mean Squared Error

365
00:15:03.630 --> 00:15:04.730
for the entire split.

366
00:15:04.730 --> 00:15:07.530
So here that would be this split over here.

367
00:15:07.530 --> 00:15:10.130
So, how do we form predictions with unseen examples?

368
00:15:10.130 --> 00:15:12.790
Well, we have Xi and have to traverse down to here.

369
00:15:12.790 --> 00:15:14.740
Instead of binary examples,

370
00:15:14.740 --> 00:15:16.470
because we're no longer have binary examples,

371
00:15:16.470 --> 00:15:17.870
all of these examples are actually

372
00:15:17.870 --> 00:15:19.580
just going to have values.

373
00:15:19.580 --> 00:15:21.370
We actually just take these values

374
00:15:21.370 --> 00:15:23.610
and average them together.

375
00:15:23.610 --> 00:15:24.930
The average of the values

376
00:15:24.930 --> 00:15:27.753
in that node is going to be assigned to Xi.

377
00:15:28.603 --> 00:15:30.130
Alright, so we've handled missing data.

378
00:15:30.130 --> 00:15:31.800
We've handled multi-class labels.

379
00:15:31.800 --> 00:15:33.640
We've even handled regression now.

380
00:15:33.640 --> 00:15:35.950
Fortunately for us, our model has been doing quite well

381
00:15:35.950 --> 00:15:38.170
in the U.S. and our company has been reached out to

382
00:15:38.170 --> 00:15:41.280
by Angola, and they would like to use our model.

383
00:15:41.280 --> 00:15:42.930
They have their own drilling site records

384
00:15:42.930 --> 00:15:44.020
that they've shared with us.

385
00:15:44.020 --> 00:15:46.030
The big difference is that Angola is

386
00:15:46.030 --> 00:15:47.420
in the Southern hemisphere

387
00:15:47.420 --> 00:15:49.860
and that could mean geological differences in the rocks.

388
00:15:49.860 --> 00:15:51.830
So we'll actually add a binary feature

389
00:15:51.830 --> 00:15:55.320
to our model signaling whether the example is

390
00:15:55.320 --> 00:15:57.530
in the Northern or the Southern hemisphere.

391
00:15:57.530 --> 00:15:59.260
So how do we split on the binary feature?

392
00:15:59.260 --> 00:16:00.410
Well, it's extremely simple.

393
00:16:00.410 --> 00:16:01.540
If you're on the Northern you'd go left,

394
00:16:01.540 --> 00:16:02.400
Southern, you'd go right.

395
00:16:02.400 --> 00:16:05.950
If you're doing classification, you use Gini Impurity.

396
00:16:05.950 --> 00:16:07.300
If you're looking at Regression,

397
00:16:07.300 --> 00:16:08.610
you'd still use Mean Squared Error.

398
00:16:08.610 --> 00:16:11.260
So we go continue using this model for some time.

399
00:16:11.260 --> 00:16:13.820
Finally, other members of OPEC hear about the success

400
00:16:13.820 --> 00:16:17.150
with the model and Angola now want to use it too.

401
00:16:17.150 --> 00:16:19.900
So no longer will this binary feature suffice.

402
00:16:19.900 --> 00:16:22.140
So we'll actually need to be representing

403
00:16:22.140 --> 00:16:23.510
each country, categorically.

404
00:16:23.510 --> 00:16:25.500
So we'll be working with the U.S., Saudi Arabia,

405
00:16:25.500 --> 00:16:28.560
Iraq, Iran, UAE, and Angola.

406
00:16:28.560 --> 00:16:30.880
So to evaluate the categorical split points

407
00:16:30.880 --> 00:16:32.720
we actually have to evaluate them

408
00:16:32.720 --> 00:16:35.410
as all the subsets of the categories.

409
00:16:35.410 --> 00:16:37.830
For instance, one split point could be

410
00:16:37.830 --> 00:16:39.670
all the U.S. examples go to the left

411
00:16:39.670 --> 00:16:41.320
and all the other countries go to the right.

412
00:16:41.320 --> 00:16:43.520
And then we could say all the U.S.

413
00:16:43.520 --> 00:16:45.530
and the Saudi Arabia examples go left

414
00:16:45.530 --> 00:16:46.710
and all the others go right.

415
00:16:46.710 --> 00:16:49.710
And we would continue on for every single subset.

416
00:16:49.710 --> 00:16:52.120
Now there's 2 to the n, minus 1 subsets.

417
00:16:52.120 --> 00:16:54.560
Just be careful that as your categories grow,

418
00:16:54.560 --> 00:16:55.990
the number of subsets that you need

419
00:16:55.990 --> 00:16:58.260
to evaluate, the split points, also grow.

420
00:16:58.260 --> 00:17:00.100
Again, same exact principle here.

421
00:17:00.100 --> 00:17:02.200
Classification, you'd use a Gini Impurity.

422
00:17:02.200 --> 00:17:03.033
If you're doing Regression,

423
00:17:03.033 --> 00:17:04.840
you would use Mean Squared Error.

424
00:17:04.840 --> 00:17:05.673
All right.

425
00:17:05.673 --> 00:17:06.930
So what's the downsides of this?

426
00:17:06.930 --> 00:17:07.870
It sounds pretty great.

427
00:17:07.870 --> 00:17:09.380
You know, you don't need to scale the data.

428
00:17:09.380 --> 00:17:10.810
You can handle missing data.

429
00:17:10.810 --> 00:17:13.650
Tons of different types of data, but what's the problem?

430
00:17:13.650 --> 00:17:16.380
Well, it actually over fits really easily.

431
00:17:16.380 --> 00:17:18.400
Remember we were talking about the depth.

432
00:17:18.400 --> 00:17:21.220
As the depth increases, the ability for the model

433
00:17:21.220 --> 00:17:23.350
to overfit the data increases as well.

434
00:17:23.350 --> 00:17:26.830
So, typically what we can do is limit the depth,

435
00:17:26.830 --> 00:17:28.370
typically between 3 and 5,

436
00:17:28.370 --> 00:17:30.370
and then use a really cool technique

437
00:17:30.370 --> 00:17:31.930
that I like, called Boosting.

438
00:17:31.930 --> 00:17:32.763
What is Boosting?

439
00:17:32.763 --> 00:17:35.270
Well, imagine we were doing a regression example.

440
00:17:35.270 --> 00:17:39.350
The ith example we've predicted the label to be 230.

441
00:17:39.350 --> 00:17:42.730
Well, the true label for that example was 270.

442
00:17:42.730 --> 00:17:46.070
So we had our tree, which we'll call tree 1,

443
00:17:46.070 --> 00:17:49.110
predicted 230, and the real answer was 270.

444
00:17:49.110 --> 00:17:51.720
So we actually generated an error here.

445
00:17:51.720 --> 00:17:56.680
Now, the true value is actually tree 1 plus the error.

446
00:17:56.680 --> 00:17:59.310
So what if instead we could take this error

447
00:17:59.310 --> 00:18:01.320
and train another tree on it?

448
00:18:01.320 --> 00:18:04.130
So it sounds like we're cheating and we kind of are, right?

449
00:18:04.130 --> 00:18:08.560
So tree 1 would have the label of 270 and predict 230.

450
00:18:08.560 --> 00:18:12.220
Tree 2 would have the label of the error, which is 40.

451
00:18:12.220 --> 00:18:14.370
And that would be the prediction.

452
00:18:14.370 --> 00:18:17.870
So we can continue this process indefinitely.

453
00:18:17.870 --> 00:18:21.800
So tree 1 would be the label of the example.

454
00:18:21.800 --> 00:18:25.520
Tree 2, we would train on the error of tree 1.

455
00:18:25.520 --> 00:18:28.320
Tree 3 we would train on the error tree 2.

456
00:18:28.320 --> 00:18:29.870
And we could just go on forever.

457
00:18:29.870 --> 00:18:31.670
So what's wrong with boosted trees?

458
00:18:31.670 --> 00:18:35.560
Well, they even overfit more than regular trees typically.

459
00:18:35.560 --> 00:18:38.020
So we need to perform a cross-validation

460
00:18:38.020 --> 00:18:40.900
to figure out how many trees and for how long we want

461
00:18:40.900 --> 00:18:43.240
to iterate this process of training

462
00:18:43.240 --> 00:18:45.860
on the previous tree's error.

463
00:18:45.860 --> 00:18:48.130
And we also need to figure out what depth

464
00:18:48.130 --> 00:18:50.040
do we want each tree to be?

465
00:18:50.040 --> 00:18:52.470
Remember, the idea was to take a CART,

466
00:18:52.470 --> 00:18:55.390
trim it down to maybe depth 3 or 5,

467
00:18:55.390 --> 00:18:58.150
to make what's called a weak learner, technically.

468
00:18:58.150 --> 00:19:00.540
And then use a whole bunch of these weak learners

469
00:19:00.540 --> 00:19:04.070
and an ensemble to create a better prediction.

470
00:19:04.070 --> 00:19:06.410
Another way we can improve the performance

471
00:19:06.410 --> 00:19:08.490
of boosted trees is by using Bagging.

472
00:19:08.490 --> 00:19:10.600
Bagging is a sampling technique

473
00:19:10.600 --> 00:19:13.230
and it stands for Bootstrap Aggregation.

474
00:19:13.230 --> 00:19:15.700
So all it is is, again, this is just the prediction model

475
00:19:15.700 --> 00:19:16.533
that we're showing here.

476
00:19:16.533 --> 00:19:17.930
Now we're showing the actual trees.

477
00:19:17.930 --> 00:19:19.840
All of these have a depth 2.

478
00:19:19.840 --> 00:19:22.200
And these are our examples.

479
00:19:22.200 --> 00:19:24.070
So basically, to train tree 1,

480
00:19:24.070 --> 00:19:26.500
we would take a subset of all the examples.

481
00:19:26.500 --> 00:19:28.670
Tree 2, we would take a different subset.

482
00:19:28.670 --> 00:19:30.992
We're basically sampling with replacement here.

483
00:19:30.992 --> 00:19:33.960
Okay. So, two trees can get the same example

484
00:19:33.960 --> 00:19:34.793
and the third tree, we're sampling

485
00:19:34.793 --> 00:19:38.150
with replacement a subset of the data.

486
00:19:38.150 --> 00:19:41.210
As well as sampling the examples,

487
00:19:41.210 --> 00:19:43.210
we can also bootstrap the features.

488
00:19:43.210 --> 00:19:46.500
We'll only look at this feature and this feature

489
00:19:46.500 --> 00:19:49.100
and we'll pretend that the other features don't exist.

490
00:19:49.100 --> 00:19:50.720
Likewise, sampling the features

491
00:19:50.720 --> 00:19:52.670
and the examples all the same.

492
00:19:52.670 --> 00:19:54.520
So this whole process is called Bagging,

493
00:19:54.520 --> 00:19:55.760
Bootstrap Aggregation.

494
00:19:55.760 --> 00:19:59.600
Out of pure math, it works out that roughly 36.7,

495
00:19:59.600 --> 00:20:02.110
which is 1 over e, of the examples

496
00:20:02.110 --> 00:20:04.460
actually won't be trained on.

497
00:20:04.460 --> 00:20:08.350
We automatically get a out-of-bag sample

498
00:20:08.350 --> 00:20:10.340
which we can use as our validation set.

499
00:20:10.340 --> 00:20:13.060
So Boosting is an ensemble of weak learners.

500
00:20:13.060 --> 00:20:16.480
Bagging reduces the variance in the predictions

501
00:20:16.480 --> 00:20:19.340
by making use of sampling techniques.

502
00:20:19.340 --> 00:20:21.120
For boosted trees, what we saw

503
00:20:21.120 --> 00:20:23.610
was tree 1 would make a prediction

504
00:20:23.610 --> 00:20:26.900
and we would use that error and that prediction

505
00:20:26.900 --> 00:20:29.670
and train the second tree on the error of prediction one.

506
00:20:29.670 --> 00:20:31.570
So we would have this sequence.

507
00:20:31.570 --> 00:20:35.690
Well, what Random Forests do is they just average the result

508
00:20:35.690 --> 00:20:36.800
of all of the trees.

509
00:20:36.800 --> 00:20:38.970
So they train a whole bunch of weak learners.

510
00:20:38.970 --> 00:20:40.660
They could use Bagging as well.

511
00:20:40.660 --> 00:20:43.487
And instead they just say, "Hey, what's your prediction?"

512
00:20:43.487 --> 00:20:44.327
"Hey, what's your prediction?"

513
00:20:44.327 --> 00:20:45.460
"Hey, what's your prediction?"

514
00:20:45.460 --> 00:20:47.570
And then just average those results together.

515
00:20:47.570 --> 00:20:50.960
You might also hear of a tree called C4.5.

516
00:20:50.960 --> 00:20:53.690
It was originally designed for classification only.

517
00:20:53.690 --> 00:20:56.300
Instead of binary splits, it can do n-ary splits

518
00:20:56.300 --> 00:20:58.170
and instead of Gini index,

519
00:20:58.170 --> 00:21:01.830
they use a type of information gain related to entropy.

520
00:21:01.830 --> 00:21:04.070
So far, we've gone over CARTs,

521
00:21:04.070 --> 00:21:07.970
Boosted trees, Bagging samples, and Bagging features.

522
00:21:07.970 --> 00:21:11.050
Some libraries that you can use to get started on your own.

523
00:21:11.050 --> 00:21:12.840
A great one I use is actually Boost,

524
00:21:12.840 --> 00:21:14.990
LightGBM I think is good too.

525
00:21:14.990 --> 00:21:16.400
Cat Boost is nice.

526
00:21:16.400 --> 00:21:17.233
Just letting you know,

527
00:21:17.233 --> 00:21:20.460
basically you'll rarely see CARTs used alone in practice.

528
00:21:20.460 --> 00:21:22.240
You'll generally see some form

529
00:21:22.240 --> 00:21:25.310
of a Boosting, Bagging or ensemble method.

530
00:21:25.310 --> 00:21:26.680
All right, that's it for this video.

531
00:21:26.680 --> 00:21:27.580
Join us next time

532
00:21:27.580 --> 00:21:29.780
as we continue our machine learning journey.

