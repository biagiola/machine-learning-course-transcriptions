WEBVTT

1
00:00:00.870 --> 00:00:01.703
<v Instructor>Welcome back</v>

2
00:00:01.703 --> 00:00:04.680
to ML experts, machine learning crash course.

3
00:00:04.680 --> 00:00:06.480
In this session, we're going to go over

4
00:00:06.480 --> 00:00:09.650
our first unsupervised learning algorithm,

5
00:00:09.650 --> 00:00:11.540
K-means clustering.

6
00:00:11.540 --> 00:00:13.700
Let's say we know nothing about our data

7
00:00:13.700 --> 00:00:16.020
except for the data values themselves.

8
00:00:16.020 --> 00:00:17.990
If we want to learn something about our data

9
00:00:17.990 --> 00:00:19.920
that helps us make decisions,

10
00:00:19.920 --> 00:00:22.963
a starting point could be to use K-means.

11
00:00:23.800 --> 00:00:26.330
K-means does exactly what it sounds like.

12
00:00:26.330 --> 00:00:29.940
It finds K-means or averages in our data.

13
00:00:29.940 --> 00:00:31.960
Let's say that we were hired by team blind

14
00:00:31.960 --> 00:00:34.870
who owns the anonymous workplace social media app.

15
00:00:34.870 --> 00:00:37.320
In general, the app allows users to sign up

16
00:00:37.320 --> 00:00:38.830
under their company name,

17
00:00:38.830 --> 00:00:41.370
users can then anonymously post information

18
00:00:41.370 --> 00:00:43.180
for all the other users to see.

19
00:00:43.180 --> 00:00:46.580
Based on that content, users can view it, like it, share it

20
00:00:46.580 --> 00:00:48.670
and they can even direct message,

21
00:00:48.670 --> 00:00:51.820
the creator of that content or the person who posted it.

22
00:00:51.820 --> 00:00:54.740
Our team is tasked with growing the user base

23
00:00:54.740 --> 00:00:56.270
that's currently on the app.

24
00:00:56.270 --> 00:00:58.560
We know nothing about their current user base

25
00:00:58.560 --> 00:01:00.750
or how they got to be where they are.

26
00:01:00.750 --> 00:01:03.160
The only data that we have per user

27
00:01:03.160 --> 00:01:04.970
is how they interact with the app.

28
00:01:04.970 --> 00:01:06.810
So the features we have, which were obtained

29
00:01:06.810 --> 00:01:10.240
by some click stream, which is basically a log of clicks

30
00:01:10.240 --> 00:01:13.250
and interactions that users took for a particular interface,

31
00:01:13.250 --> 00:01:15.880
we get the likes given and received per day,

32
00:01:15.880 --> 00:01:17.620
the average session time,

33
00:01:17.620 --> 00:01:20.450
the times posted content per month,

34
00:01:20.450 --> 00:01:23.300
the average feed scrolling time per session

35
00:01:23.300 --> 00:01:26.640
and the direct messages sent or received per day.

36
00:01:26.640 --> 00:01:28.030
This is all we get per user.

37
00:01:28.030 --> 00:01:31.500
And based on these features, we want to grow the user base.

38
00:01:31.500 --> 00:01:33.650
Let's say our data was only in two dimensions.

39
00:01:33.650 --> 00:01:37.673
We have X two and X one here, and we have our data uploaded.

40
00:01:38.810 --> 00:01:41.640
The goal of K-means is to organize our data

41
00:01:41.640 --> 00:01:44.070
into K distinct groups.

42
00:01:44.070 --> 00:01:46.930
So for instance, if we chose K equal to two here,

43
00:01:46.930 --> 00:01:49.070
then what we would want K-means to accomplish

44
00:01:49.070 --> 00:01:53.160
is to separate these data into two groups.

45
00:01:53.160 --> 00:01:57.200
The first step, is to pick K random points here too

46
00:01:57.200 --> 00:01:59.610
and assign them to be different labels.

47
00:01:59.610 --> 00:02:02.070
These points are called centroids.

48
00:02:02.070 --> 00:02:04.890
Next what we do, is we find the distance

49
00:02:04.890 --> 00:02:09.150
between this centroid and every other data point there is.

50
00:02:09.150 --> 00:02:10.470
We repeat this process

51
00:02:10.470 --> 00:02:12.900
for the other centroid as well.

52
00:02:12.900 --> 00:02:16.080
Whichever centroid is closest to a given point,

53
00:02:16.080 --> 00:02:18.770
that point will take on that centroids label.

54
00:02:18.770 --> 00:02:20.760
So in this case, all of these points

55
00:02:20.760 --> 00:02:22.740
are closest to this centroid

56
00:02:22.740 --> 00:02:26.060
and all of these points are closest to this centroid.

57
00:02:26.060 --> 00:02:29.850
Then what we do is update the centroid to be the average

58
00:02:29.850 --> 00:02:33.160
of all the points in the given cluster.

59
00:02:33.160 --> 00:02:35.740
So here, the average of this cluster was right there.

60
00:02:35.740 --> 00:02:37.350
So now the centroid is here,

61
00:02:37.350 --> 00:02:39.540
the average of this cluster was right here,

62
00:02:39.540 --> 00:02:41.300
so that's where we'll put the centroid.

63
00:02:41.300 --> 00:02:44.450
We then, just repeat this process over and over again.

64
00:02:44.450 --> 00:02:49.070
So here we find the distances,

65
00:02:49.070 --> 00:02:51.580
we then update all the labels of the points

66
00:02:51.580 --> 00:02:54.350
based on their distance from their near centroid.

67
00:02:54.350 --> 00:02:57.890
Here, we can see that this cluster has actually expanded.

68
00:02:57.890 --> 00:03:00.240
We then update the location of the centroids

69
00:03:00.240 --> 00:03:02.423
to be the average of the clusters.

70
00:03:03.430 --> 00:03:05.740
So, if we repeat this process again,

71
00:03:05.740 --> 00:03:08.680
we'll notice that the centroid locations don't change.

72
00:03:08.680 --> 00:03:11.340
This means that our K-means has converged.

73
00:03:11.340 --> 00:03:14.150
So how do we figure out how well K-means did,

74
00:03:14.150 --> 00:03:16.820
when it comes to clustering out on labeled data?

75
00:03:16.820 --> 00:03:18.950
Here, we can take the distance,

76
00:03:18.950 --> 00:03:21.180
and by that I mean the euclidean distance

77
00:03:21.180 --> 00:03:22.180
between the centroid

78
00:03:22.180 --> 00:03:25.340
and every other data point in its cluster.

79
00:03:25.340 --> 00:03:27.690
We would do that for both clusters.

80
00:03:27.690 --> 00:03:30.690
We then, can sum up all of these distances

81
00:03:30.690 --> 00:03:32.553
to get something called the inertia.

82
00:03:33.410 --> 00:03:36.890
What this inertia tells us is how far each point

83
00:03:36.890 --> 00:03:41.180
in the given cluster is away from its cluster's average.

84
00:03:41.180 --> 00:03:43.820
Generally, we want to minimize the inertia

85
00:03:43.820 --> 00:03:46.620
in order to form the best clusters.

86
00:03:46.620 --> 00:03:49.480
Now, is this the best inertia possible?

87
00:03:49.480 --> 00:03:50.730
Probably not.

88
00:03:50.730 --> 00:03:51.850
We could verify this

89
00:03:51.850 --> 00:03:54.370
by finding every single cluster possible

90
00:03:54.370 --> 00:03:55.980
for K equals to two.

91
00:03:55.980 --> 00:03:57.480
But for instance, this single point

92
00:03:57.480 --> 00:03:58.790
could be in its own cluster

93
00:03:58.790 --> 00:04:01.210
and all these other points could be in a cluster

94
00:04:01.210 --> 00:04:02.970
and we could measure the inertia.

95
00:04:02.970 --> 00:04:04.800
And then we could put this point

96
00:04:04.800 --> 00:04:06.710
inside the clusters at this point

97
00:04:06.710 --> 00:04:08.490
and leave every other point out.

98
00:04:08.490 --> 00:04:10.170
And we could continue this process

99
00:04:10.170 --> 00:04:12.720
measuring the inertia every single time.

100
00:04:12.720 --> 00:04:16.860
If we follow this pattern for every single possible cluster,

101
00:04:16.860 --> 00:04:20.580
then the time complexity would be N to the KD plus one.

102
00:04:20.580 --> 00:04:24.130
So, how come we didn't take N to the KD plus one?

103
00:04:24.130 --> 00:04:26.590
Well, we use something called Lloyd's algorithm.

104
00:04:26.590 --> 00:04:27.620
With Lloyd's algorithm,

105
00:04:27.620 --> 00:04:29.770
we can get the time complexity down to NKD.

106
00:04:31.600 --> 00:04:33.883
However, this comes with its disadvantages.

107
00:04:34.770 --> 00:04:37.500
When we use Lloyd's algorithm, what we're actually doing

108
00:04:37.500 --> 00:04:41.100
is approximating the best cluster configuration.

109
00:04:41.100 --> 00:04:44.230
Let's say on the X axis here, if we could represent it

110
00:04:44.230 --> 00:04:47.150
we've arranged a particular cluster configuration

111
00:04:47.150 --> 00:04:48.670
or K equals to two.

112
00:04:48.670 --> 00:04:50.390
So, we've organized the points

113
00:04:50.390 --> 00:04:53.120
such that some subset of the points belong to one

114
00:04:53.120 --> 00:04:56.440
and the other subset of points belong to another group.

115
00:04:56.440 --> 00:04:59.770
If we plot the inertia for whatever organization

116
00:04:59.770 --> 00:05:02.260
that we have on the X axis here,

117
00:05:02.260 --> 00:05:05.990
we will get a minimum at some point.

118
00:05:05.990 --> 00:05:07.690
The difference between this function

119
00:05:07.690 --> 00:05:09.640
and all the other functions that we've wanted

120
00:05:09.640 --> 00:05:12.050
to get a minimum for in the past,

121
00:05:12.050 --> 00:05:14.670
is that this function is no longer convex.

122
00:05:14.670 --> 00:05:17.870
So what that means is there's more than one minimum

123
00:05:17.870 --> 00:05:22.080
and potentially, we could get stuck in a sub optimal point.

124
00:05:22.080 --> 00:05:25.160
For instance here, is a local minimum

125
00:05:25.160 --> 00:05:27.350
and here is the global minimum.

126
00:05:27.350 --> 00:05:29.580
So, let's look at how we can get stuck

127
00:05:29.580 --> 00:05:30.730
in these local minimum.

128
00:05:31.840 --> 00:05:33.230
Here, K is equal to three

129
00:05:33.230 --> 00:05:35.740
and we see that there are three clusters.

130
00:05:35.740 --> 00:05:37.770
Well, this could have happened by chance.

131
00:05:37.770 --> 00:05:39.720
However, let's say that randomly,

132
00:05:39.720 --> 00:05:42.500
we initialized two centroids in this cluster

133
00:05:42.500 --> 00:05:44.740
and one centroid right here.

134
00:05:44.740 --> 00:05:46.890
Well, we'd very likely end up with a cluster

135
00:05:46.890 --> 00:05:48.500
that looks like this.

136
00:05:48.500 --> 00:05:49.690
Why is that?

137
00:05:49.690 --> 00:05:53.380
Well, by definition, we said that K-means converges

138
00:05:53.380 --> 00:05:57.840
when the averages don't update from iteration to iteration.

139
00:05:57.840 --> 00:06:00.320
Here, technically, the averages would not move

140
00:06:00.320 --> 00:06:04.000
because every point is closest to its own centroid.

141
00:06:04.000 --> 00:06:06.280
This is what it would look like if we got stuck

142
00:06:06.280 --> 00:06:07.710
in one of these local minima.

143
00:06:07.710 --> 00:06:10.000
Now, there are some mitigation strategies we can use

144
00:06:10.000 --> 00:06:12.200
to avoid getting stuck in these local minima.

145
00:06:12.200 --> 00:06:14.800
One of them is called K means plus, plus

146
00:06:14.800 --> 00:06:16.180
but no method can guarantee

147
00:06:16.180 --> 00:06:17.500
we find the optimal solution

148
00:06:17.500 --> 00:06:19.770
except calculating the exact K-means

149
00:06:19.770 --> 00:06:23.280
which typically isn't a good idea in most cases.

150
00:06:23.280 --> 00:06:25.600
However, there are other methods.

151
00:06:25.600 --> 00:06:27.880
One of them is called bisected K-means.

152
00:06:27.880 --> 00:06:31.540
All that we do is run K-means, for K equal to two

153
00:06:31.540 --> 00:06:33.620
no matter what K we think we should have.

154
00:06:33.620 --> 00:06:36.800
Then, we measure the inertia of the two clusters.

155
00:06:36.800 --> 00:06:39.740
Here we got 10,000 and here we got 20,000.

156
00:06:39.740 --> 00:06:42.670
Whatever cluster has the most inertia,

157
00:06:42.670 --> 00:06:45.690
we then perform K-means on that cluster.

158
00:06:45.690 --> 00:06:47.480
So now we have K-means here

159
00:06:47.480 --> 00:06:50.720
and the inertias here are equal, so we stop.

160
00:06:50.720 --> 00:06:53.840
And we've only used K equals to two each time

161
00:06:53.840 --> 00:06:58.240
because we've bisected, but we end up with three clusters.

162
00:06:58.240 --> 00:07:01.440
This can be an effective way to avoid local minima.

163
00:07:01.440 --> 00:07:03.550
Now we need to know what K is optimal.

164
00:07:03.550 --> 00:07:06.330
For bisected even though we're choosing K equals to two

165
00:07:06.330 --> 00:07:09.340
at each iteration, we still have to select

166
00:07:09.340 --> 00:07:11.670
what K is best to end up with.

167
00:07:11.670 --> 00:07:13.560
Here, we chose K is equal to three

168
00:07:13.560 --> 00:07:15.850
but still we chose K equal to four,

169
00:07:15.850 --> 00:07:18.290
this cluster might have been split on next.

170
00:07:18.290 --> 00:07:19.770
So, how do we figure out

171
00:07:19.770 --> 00:07:23.440
what the ideal value of the final K is?

172
00:07:23.440 --> 00:07:24.540
There's a couple of ways,

173
00:07:24.540 --> 00:07:26.980
the first one is called the elbow method.

174
00:07:26.980 --> 00:07:29.060
All this does is we measure inertia

175
00:07:29.060 --> 00:07:31.830
against all of the different Ks that we can pick.

176
00:07:31.830 --> 00:07:34.700
We run K-means with one cluster, two clusters,

177
00:07:34.700 --> 00:07:36.300
three clusters all up to six

178
00:07:36.300 --> 00:07:39.840
and we take the inertia and graph that.

179
00:07:39.840 --> 00:07:42.640
The elbow method says that we should pick the elbow

180
00:07:42.640 --> 00:07:47.000
of this function as the best K, for K-means.

181
00:07:47.000 --> 00:07:50.640
Another option is that we could use the silhouette method.

182
00:07:50.640 --> 00:07:53.530
First, we have to calculate A of I.

183
00:07:53.530 --> 00:07:57.710
All A of I is the distance between a particular point

184
00:07:57.710 --> 00:08:00.340
and another point in the same cluster

185
00:08:00.340 --> 00:08:01.850
for every single point,

186
00:08:01.850 --> 00:08:04.180
with an average all of the results together.

187
00:08:04.180 --> 00:08:07.280
We then calculate BI which is the distance

188
00:08:07.280 --> 00:08:09.310
between a point in a cluster

189
00:08:09.310 --> 00:08:12.990
and every other point in its nearest neighboring cluster,

190
00:08:12.990 --> 00:08:15.200
and we average those values as well,

191
00:08:15.200 --> 00:08:18.180
for every single point in every single cluster.

192
00:08:18.180 --> 00:08:21.370
We then apply this formula, BI minus AI,

193
00:08:21.370 --> 00:08:23.410
over the max of AI, BI.

194
00:08:23.410 --> 00:08:27.940
If we plot the average SI for every single cluster number

195
00:08:27.940 --> 00:08:28.850
that we've considered,

196
00:08:28.850 --> 00:08:33.090
so let's say we considered K equals to one, K equals to 10.

197
00:08:33.090 --> 00:08:36.810
If we plot the average SI per cluster configuration,

198
00:08:36.810 --> 00:08:39.830
then, we'll end up with a particular peak.

199
00:08:39.830 --> 00:08:43.080
This peak is called the silhouette coefficient.

200
00:08:43.080 --> 00:08:45.583
And for us, that's K is equal to three.

201
00:08:46.740 --> 00:08:49.630
So, after running bisected K-means

202
00:08:49.630 --> 00:08:51.550
and assigning K equal to three,

203
00:08:51.550 --> 00:08:54.660
obtained from using the silhouette method,

204
00:08:54.660 --> 00:08:59.113
this is how our data, for the blind users was organized.

205
00:08:59.990 --> 00:09:02.180
So, we can see three distinct clusters,

206
00:09:02.180 --> 00:09:04.230
but how do we use this to our advantage

207
00:09:04.230 --> 00:09:05.720
to make any decisions?

208
00:09:05.720 --> 00:09:09.390
Well, what we can do is look at the features of these users

209
00:09:09.390 --> 00:09:10.223
in this cluster here.

210
00:09:10.223 --> 00:09:13.000
And we can repeat that for every other cluster

211
00:09:13.000 --> 00:09:16.140
and see if their features vary in any way.

212
00:09:16.140 --> 00:09:17.860
After doing this, what we found

213
00:09:17.860 --> 00:09:21.750
was that this cluster consumes a lot of material.

214
00:09:21.750 --> 00:09:23.970
Their average scroll session is higher,

215
00:09:23.970 --> 00:09:26.520
the amount of likes that they give to post is higher,

216
00:09:26.520 --> 00:09:27.650
things like that.

217
00:09:27.650 --> 00:09:30.330
This cluster, which is quite smaller

218
00:09:30.330 --> 00:09:32.980
is organized of content creators.

219
00:09:32.980 --> 00:09:35.520
Their scroll time is often a little bit less,

220
00:09:35.520 --> 00:09:38.040
they don't like as many other posts

221
00:09:38.040 --> 00:09:41.313
but they do spend a lot of time writing their own posts.

222
00:09:42.220 --> 00:09:45.180
This cluster right here was a combination of the two.

223
00:09:45.180 --> 00:09:46.860
Some of them were first-time writers,

224
00:09:46.860 --> 00:09:49.070
some of them have only written a couple of times

225
00:09:49.070 --> 00:09:51.080
but they still kept up with their feed

226
00:09:51.080 --> 00:09:53.640
and liked a lot of other's posts.

227
00:09:53.640 --> 00:09:56.660
Like I said, we didn't know how their user base

228
00:09:56.660 --> 00:09:57.910
got to where it is.

229
00:09:57.910 --> 00:10:01.010
Fortunately, they had the same data from years ago.

230
00:10:01.010 --> 00:10:03.150
And what we did is we ran K-means

231
00:10:03.150 --> 00:10:06.350
or the same exact user base just in the past.

232
00:10:06.350 --> 00:10:09.570
What we saw, was that obviously there were less number

233
00:10:09.570 --> 00:10:12.970
of users because the app has grown and user base

234
00:10:12.970 --> 00:10:16.060
but the clusters themselves still aligned

235
00:10:16.060 --> 00:10:17.660
in this particular way.

236
00:10:17.660 --> 00:10:21.190
Then, a year or two later, we saw it grow a little bit more

237
00:10:21.190 --> 00:10:24.260
and then finally, today, this is what it looks like.

238
00:10:24.260 --> 00:10:27.060
So since we were tasked with growing the user base,

239
00:10:27.060 --> 00:10:30.020
what this tells us is how we need to grow

240
00:10:30.020 --> 00:10:32.710
to maintain the same type of ecosystem

241
00:10:32.710 --> 00:10:35.510
that they have now in terms of users.

242
00:10:35.510 --> 00:10:38.070
So, what are our levers that we can pull here

243
00:10:38.070 --> 00:10:39.410
that will help control

244
00:10:39.410 --> 00:10:42.460
the way that we can grow the user base?

245
00:10:42.460 --> 00:10:45.920
At a high level, we have a few points to control growth.

246
00:10:45.920 --> 00:10:48.690
Here, we can control the transitions that users make.

247
00:10:48.690 --> 00:10:51.100
So basically, full-time readers

248
00:10:51.100 --> 00:10:54.703
to a little bit of both to full-time content creators.

249
00:10:55.690 --> 00:10:59.360
We can also grow this cluster by maybe referral programs

250
00:10:59.360 --> 00:11:01.290
or offering premium benefits for free.

251
00:11:01.290 --> 00:11:04.460
Even generally, we can just increase advertising.

252
00:11:04.460 --> 00:11:06.330
However, we don't wanna do that so much

253
00:11:06.330 --> 00:11:08.170
that there becomes too many readers

254
00:11:08.170 --> 00:11:11.720
or content consumers and not enough content creators.

255
00:11:11.720 --> 00:11:14.500
What would happen here is content would become sparse

256
00:11:14.500 --> 00:11:16.850
and readers would get bored and move on

257
00:11:16.850 --> 00:11:18.600
and maybe even delete the app.

258
00:11:18.600 --> 00:11:21.660
So, what we need to control are these flows here

259
00:11:21.660 --> 00:11:25.200
of how to get readers into this category,

260
00:11:25.200 --> 00:11:26.740
and then from this category

261
00:11:26.740 --> 00:11:30.300
which they're doing both reading and writing content,

262
00:11:30.300 --> 00:11:33.210
we need to then transition those into full-time writers.

263
00:11:33.210 --> 00:11:35.820
This way, we can put a lot of investment over here

264
00:11:35.820 --> 00:11:37.730
in terms of growing the reader base.

265
00:11:37.730 --> 00:11:40.500
And as long as we can control the flow into writers,

266
00:11:40.500 --> 00:11:42.840
then we don't have a problem with growth.

267
00:11:42.840 --> 00:11:44.270
We can transition readers

268
00:11:44.270 --> 00:11:47.110
into first-time writers slash sometimes readers,

269
00:11:47.110 --> 00:11:49.870
sometimes writers by encouraging these users

270
00:11:49.870 --> 00:11:51.370
to write something.

271
00:11:51.370 --> 00:11:53.720
This can be done by emailing these users,

272
00:11:53.720 --> 00:11:55.760
mentioning how people want to hear about events

273
00:11:55.760 --> 00:11:56.930
that took place at their company

274
00:11:56.930 --> 00:11:59.890
such as layoffs, acquisitions or product releases.

275
00:11:59.890 --> 00:12:03.830
So after we get some users into this middle category here,

276
00:12:03.830 --> 00:12:07.490
we want to now get them to the writer category.

277
00:12:07.490 --> 00:12:09.220
We can do a couple of things here.

278
00:12:09.220 --> 00:12:11.680
One, is we can push their first-time content

279
00:12:11.680 --> 00:12:15.820
to specific readers who we think would enjoy the content.

280
00:12:15.820 --> 00:12:17.550
And if the response is poor at first,

281
00:12:17.550 --> 00:12:20.456
because no one's good at writing the first time they do it,

282
00:12:20.456 --> 00:12:22.560
so even if these people don't like their content

283
00:12:22.560 --> 00:12:24.870
we can have chat bots, reach out to them

284
00:12:24.870 --> 00:12:26.830
encouraging them to keep writing.

285
00:12:26.830 --> 00:12:29.750
We can also inflate their likes of their content

286
00:12:29.750 --> 00:12:33.640
to motivate them to continue writing and refining the craft.

287
00:12:33.640 --> 00:12:35.730
Once they've become a decent writer,

288
00:12:35.730 --> 00:12:38.630
we can then begin pushing out their content to readers

289
00:12:38.630 --> 00:12:41.205
such that they would enjoy reading.

290
00:12:41.205 --> 00:12:43.200
At that point, we hypothesized

291
00:12:43.200 --> 00:12:45.240
that they will convert to full-time writers

292
00:12:45.240 --> 00:12:48.050
if the response is big enough.

293
00:12:48.050 --> 00:12:49.500
Finally, if we want to increase

294
00:12:49.500 --> 00:12:52.250
the size of the writers inorganically,

295
00:12:52.250 --> 00:12:56.440
we can simply hire writers to get onto the platform.

296
00:12:56.440 --> 00:12:58.830
The idea is that all of these strategies together

297
00:12:58.830 --> 00:13:01.430
can responsibly grow the platform.

298
00:13:01.430 --> 00:13:03.380
So, one interesting thing we can do

299
00:13:03.380 --> 00:13:04.920
with these clusters as well,

300
00:13:04.920 --> 00:13:07.070
is let's say we get a new customer

301
00:13:07.070 --> 00:13:10.460
and we don't yet know what stage they're in

302
00:13:10.460 --> 00:13:12.260
in terms of their usage,

303
00:13:12.260 --> 00:13:14.820
what we can do is use a centroid classifier

304
00:13:14.820 --> 00:13:17.920
and it works very similarly to K nearest neighbors

305
00:13:17.920 --> 00:13:21.470
such that this point would be a purple point.

306
00:13:21.470 --> 00:13:23.650
Now there's different variations we can use

307
00:13:23.650 --> 00:13:25.030
in terms of K-means.

308
00:13:25.030 --> 00:13:28.350
We can use K-medians, which is yes, the median

309
00:13:28.350 --> 00:13:29.520
instead of the mean.

310
00:13:29.520 --> 00:13:32.740
How this works in practice, is you use Manhattan distance

311
00:13:32.740 --> 00:13:34.280
instead of Euclidean distance.

312
00:13:34.280 --> 00:13:37.870
K-medoids requires that the cluster average

313
00:13:37.870 --> 00:13:40.680
be approximated by the closest point.

314
00:13:40.680 --> 00:13:44.780
So for K-means, we actually did use the cluster average.

315
00:13:44.780 --> 00:13:47.720
K- medoids would require us to find the cluster average

316
00:13:47.720 --> 00:13:51.870
and then find the closest real data point to that average

317
00:13:51.870 --> 00:13:54.380
and that would be assigned as the medoid.

318
00:13:54.380 --> 00:13:57.850
Finally K-modes is useful if we don't have numerical data

319
00:13:57.850 --> 00:14:00.700
instead it gauges distance on the dissimilarities

320
00:14:00.700 --> 00:14:03.470
of the examples and it uses the modes

321
00:14:03.470 --> 00:14:05.270
or the most recurring elements

322
00:14:05.270 --> 00:14:09.040
in those dissimilarities to organize these clusters.

323
00:14:09.040 --> 00:14:11.400
So, what if our data looks like this?

324
00:14:11.400 --> 00:14:13.220
Well, if we use K-means

325
00:14:13.220 --> 00:14:15.610
then this would be what we would get.

326
00:14:15.610 --> 00:14:17.660
Literally, this is not what we wanted.

327
00:14:17.660 --> 00:14:19.920
We wanted the outer ring to be one category

328
00:14:19.920 --> 00:14:23.080
and the inner circle to be another category.

329
00:14:23.080 --> 00:14:25.430
Well, is there a clustering algorithm out there

330
00:14:25.430 --> 00:14:28.180
that can help us cluster our data like this?

331
00:14:28.180 --> 00:14:31.200
The answer is yes, all the algorithm does

332
00:14:31.200 --> 00:14:33.450
which is called agglomerative clustering

333
00:14:33.450 --> 00:14:36.250
takes the two nearest points

334
00:14:36.250 --> 00:14:38.430
and combines them into a cluster.

335
00:14:38.430 --> 00:14:40.200
So for instance, these were the closest points

336
00:14:40.200 --> 00:14:41.460
out of all the other points

337
00:14:41.460 --> 00:14:44.800
and it would combine them into a cluster.

338
00:14:44.800 --> 00:14:47.457
Then, it would find the next two closest points

339
00:14:47.457 --> 00:14:49.100
and it would combine them.

340
00:14:49.100 --> 00:14:51.930
Now, this would carry on until there were two

341
00:14:51.930 --> 00:14:54.610
or maybe even just one cluster

342
00:14:54.610 --> 00:14:57.240
but then you would use silhouette method to figure out

343
00:14:57.240 --> 00:15:00.130
that two clusters is actually best here.

344
00:15:00.130 --> 00:15:03.650
And that's how we can cluster this circular ring data.

345
00:15:03.650 --> 00:15:05.660
So now that we have clusters, say this cluster

346
00:15:05.660 --> 00:15:07.900
and this cluster, we now have to figure out a way

347
00:15:07.900 --> 00:15:11.910
to gauge the distance between a point and a cluster.

348
00:15:11.910 --> 00:15:14.780
It's very easy to tell the distance between two points.

349
00:15:14.780 --> 00:15:16.700
We would just use Euclidean distance,

350
00:15:16.700 --> 00:15:19.000
but if we have a cluster next to a point

351
00:15:19.000 --> 00:15:21.010
and we need to know if we should merge those

352
00:15:21.010 --> 00:15:23.280
or if we should merge a point and another point,

353
00:15:23.280 --> 00:15:24.950
because their distance is smaller,

354
00:15:24.950 --> 00:15:28.300
we can use something called single linkage clustering.

355
00:15:28.300 --> 00:15:31.770
Single linkage clustering takes the closest points

356
00:15:31.770 --> 00:15:35.680
in each cluster and uses those to measure the distance

357
00:15:35.680 --> 00:15:37.470
between the two clusters.

358
00:15:37.470 --> 00:15:41.230
Complete linkage clustering takes the two furthest points

359
00:15:41.230 --> 00:15:44.300
in the cluster and uses those to measure the distance

360
00:15:44.300 --> 00:15:45.950
between the clusters.

361
00:15:45.950 --> 00:15:48.180
So I'd wrap this up, a great library

362
00:15:48.180 --> 00:15:49.770
it's called the scikit-learn.

363
00:15:49.770 --> 00:15:53.440
For K-means, it actually uses K-means plus, plus

364
00:15:53.440 --> 00:15:54.400
under the hood

365
00:15:54.400 --> 00:15:57.510
and it also supports agglomerative clustering.

366
00:15:57.510 --> 00:15:58.830
If you're using spark,

367
00:15:58.830 --> 00:16:01.300
they actually offer bisected K-means.

368
00:16:01.300 --> 00:16:04.990
Now, remember that this algorithm is distance based

369
00:16:04.990 --> 00:16:07.380
so we should keep scaling in mind.

370
00:16:07.380 --> 00:16:09.570
And generally, we should standardize our features

371
00:16:09.570 --> 00:16:11.470
which means we would take our features,

372
00:16:11.470 --> 00:16:13.440
say if we were just looking at all the X ones

373
00:16:13.440 --> 00:16:15.600
we would find the average of the X one

374
00:16:15.600 --> 00:16:18.250
and the standard deviation of the X one.

375
00:16:18.250 --> 00:16:20.840
And then we would take every feature XI,

376
00:16:20.840 --> 00:16:22.100
subtract the average from it

377
00:16:22.100 --> 00:16:25.040
divided by the X one standard deviation

378
00:16:25.040 --> 00:16:27.680
and that would be our new standardized feature.

379
00:16:27.680 --> 00:16:29.110
Just as in K nearest neighbors,

380
00:16:29.110 --> 00:16:32.420
the KD tree can help performance and as well

381
00:16:32.420 --> 00:16:35.540
it's vulnerable to high dimensional data.

382
00:16:35.540 --> 00:16:36.740
That's it for this video.

383
00:16:36.740 --> 00:16:37.860
Thanks for joining.

384
00:16:37.860 --> 00:16:39.180
Join us in the next video

385
00:16:39.180 --> 00:16:41.453
to continue our machine learning journey.

