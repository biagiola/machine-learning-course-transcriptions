WEBVTT

1
00:00:00.150 --> 00:00:01.460
<v Instructor>Welcome back everybody.</v>

2
00:00:01.460 --> 00:00:05.070
This is ML experts machine learning crash course.

3
00:00:05.070 --> 00:00:08.930
In this video, we're going to go over K nearest neighbors.

4
00:00:08.930 --> 00:00:13.400
The basic idea is to figure out what an unlabeled example is

5
00:00:13.400 --> 00:00:15.473
according to its neighbors.

6
00:00:16.490 --> 00:00:19.720
So for here, K is equal to two,

7
00:00:19.720 --> 00:00:23.490
which means we're going to use two of its nearest neighbors

8
00:00:23.490 --> 00:00:27.603
to try to determine what value this example should take on.

9
00:00:28.740 --> 00:00:32.623
Here K is equal to three and here K is equal to seven.

10
00:00:34.250 --> 00:00:36.340
So based on these seven neighbors,

11
00:00:36.340 --> 00:00:39.110
what value should this example be?

12
00:00:39.110 --> 00:00:41.150
Well, we can take a vote.

13
00:00:41.150 --> 00:00:44.080
So here we have six saying that it's green

14
00:00:44.080 --> 00:00:46.720
because six of the nearest neighbors are green.

15
00:00:46.720 --> 00:00:48.210
And one of the nearest neighbors

16
00:00:48.210 --> 00:00:51.700
that we're evaluating are red.

17
00:00:51.700 --> 00:00:56.670
So six over seven gives us an 85% majority vote

18
00:00:56.670 --> 00:00:59.850
that this element should be green.

19
00:00:59.850 --> 00:01:01.270
Let's say we're working at Symantec,

20
00:01:01.270 --> 00:01:02.690
the cybersecurity company

21
00:01:02.690 --> 00:01:05.880
and we wanted to attack intrusive processes

22
00:01:05.880 --> 00:01:10.190
which for our purposes, we can just call programs, okay?

23
00:01:10.190 --> 00:01:13.020
Every once in a while these programs or processes

24
00:01:13.020 --> 00:01:17.575
will make calls to the operating system called system calls

25
00:01:17.575 --> 00:01:21.770
in order to get them to open a file or write to the console.

26
00:01:21.770 --> 00:01:23.180
Things like that.

27
00:01:23.180 --> 00:01:25.620
Our goal is to predict which of these processes

28
00:01:25.620 --> 00:01:28.790
are intrusive and which ones are normal.

29
00:01:28.790 --> 00:01:31.100
And we're not only going to predict processes

30
00:01:31.100 --> 00:01:32.550
that are on a single machine,

31
00:01:32.550 --> 00:01:34.304
we're actually going to be predicting

32
00:01:34.304 --> 00:01:36.270
whether processes are normal

33
00:01:36.270 --> 00:01:40.370
or malicious slash intrusive across the entire network.

34
00:01:40.370 --> 00:01:42.110
All right, so the first thing we're gonna need is features.

35
00:01:42.110 --> 00:01:43.510
So we'll have a process ID

36
00:01:43.510 --> 00:01:46.380
which we can assume to be unique across the entire network.

37
00:01:46.380 --> 00:01:49.200
And this process ID will map to an ordered list

38
00:01:49.200 --> 00:01:52.180
of system calls that that process made

39
00:01:52.180 --> 00:01:54.220
to the operating system.

40
00:01:54.220 --> 00:01:57.010
The label is going to be either zero

41
00:01:57.010 --> 00:02:00.000
if the program was never found to be intrusive,

42
00:02:00.000 --> 00:02:05.000
or one, if the program was eventually found to be intrusive.

43
00:02:05.020 --> 00:02:07.210
So what we can do is take our features

44
00:02:07.210 --> 00:02:09.160
and these are just words, right?

45
00:02:09.160 --> 00:02:11.560
These are just the system calls themselves

46
00:02:11.560 --> 00:02:14.313
and we're going to perform TF-IDF on them.

47
00:02:15.180 --> 00:02:19.040
Okay, so now once we perform TF-IDF on our features

48
00:02:19.040 --> 00:02:21.700
since we're kind of treating them just like words,

49
00:02:21.700 --> 00:02:23.610
we now can plot them.

50
00:02:23.610 --> 00:02:26.910
Now I know that the features actually had more dimensions

51
00:02:26.910 --> 00:02:30.050
than just two, but let's just assume we're using

52
00:02:30.050 --> 00:02:32.820
the first two dimensions for these examples.

53
00:02:32.820 --> 00:02:35.840
Then we can plot these and just visualize where

54
00:02:35.840 --> 00:02:39.420
they would exist in this two dimensional space.

55
00:02:39.420 --> 00:02:42.490
So x one would be here,

56
00:02:42.490 --> 00:02:46.150
and x two would be here and we just plotted them.

57
00:02:46.150 --> 00:02:49.860
So all of these processes are labeled green

58
00:02:49.860 --> 00:02:52.910
because they were never found to be intrusive.

59
00:02:52.910 --> 00:02:56.910
These processes, however, were later found to be intrusive

60
00:02:56.910 --> 00:02:59.680
after the fact, we've labeled those red.

61
00:02:59.680 --> 00:03:02.130
As you can see there's some distance or separation

62
00:03:02.130 --> 00:03:04.270
between these two clusters.

63
00:03:04.270 --> 00:03:06.730
Let's bring in an example that we haven't seen yet.

64
00:03:06.730 --> 00:03:08.060
So we're gonna color it white,

65
00:03:08.060 --> 00:03:11.060
just because we don't know if it should be red or green,

66
00:03:11.060 --> 00:03:13.370
and we're going to run the K nearest neighbors algorithm

67
00:03:13.370 --> 00:03:15.420
on it to figure out how close it is

68
00:03:15.420 --> 00:03:16.330
to all the other neighbors

69
00:03:16.330 --> 00:03:18.880
and then vote amongst its closest neighbors

70
00:03:18.880 --> 00:03:21.840
about which category this unlabeled example

71
00:03:21.840 --> 00:03:24.940
should be in, either intrusive or not intrusive.

72
00:03:24.940 --> 00:03:27.570
By the way, here distance between one and nine

73
00:03:27.570 --> 00:03:29.540
we're gonna define as Euclidean distance.

74
00:03:29.540 --> 00:03:31.230
It's just a basic formula.

75
00:03:31.230 --> 00:03:33.860
A one is a one, a two is a two,

76
00:03:33.860 --> 00:03:36.404
b one, b one, b two, b two,

77
00:03:36.404 --> 00:03:40.060
and it can generalize out to multiple dimensions.

78
00:03:40.060 --> 00:03:43.380
So every single element would just be subtracted

79
00:03:43.380 --> 00:03:44.860
from the corresponding element.

80
00:03:44.860 --> 00:03:46.830
And then we would square those differences

81
00:03:46.830 --> 00:03:49.060
and then finally take the square root of the result

82
00:03:49.060 --> 00:03:50.810
and that would give us the distance.

83
00:03:50.810 --> 00:03:54.784
Here's 3.2, the distance between nine and four is 2.6.

84
00:03:54.784 --> 00:03:56.130
After we go through all the examples,

85
00:03:56.130 --> 00:03:57.340
the final example, the distance

86
00:03:57.340 --> 00:03:59.430
between nine and eight is 0.08.

87
00:03:59.430 --> 00:04:01.700
Now let's assume here that K is equal to four.

88
00:04:01.700 --> 00:04:03.480
So we'll look at the four closest neighbors,

89
00:04:03.480 --> 00:04:06.350
we can either sort them or keep certain data structures

90
00:04:06.350 --> 00:04:07.810
around to let us know which ones

91
00:04:07.810 --> 00:04:09.980
are the four closest values.

92
00:04:09.980 --> 00:04:13.010
If we vote what class this should be in

93
00:04:13.010 --> 00:04:14.390
based on the four nearest neighbors,

94
00:04:14.390 --> 00:04:16.420
we'll see that this should be declared

95
00:04:16.420 --> 00:04:18.010
as an intrusive program.

96
00:04:18.010 --> 00:04:20.040
Now, if we used eight neighbors, though

97
00:04:20.040 --> 00:04:21.560
the vote would turn green.

98
00:04:21.560 --> 00:04:22.720
K is an important number.

99
00:04:22.720 --> 00:04:24.815
In my opinion, this should probably be classified

100
00:04:24.815 --> 00:04:27.410
as an intrusive program,

101
00:04:27.410 --> 00:04:29.120
but based on the value of K,

102
00:04:29.120 --> 00:04:31.690
the classifications will fall differently.

103
00:04:31.690 --> 00:04:33.610
By the way, since we only have two classes here

104
00:04:33.610 --> 00:04:37.060
you wanna pick an odd number of neighbors to avoid ties.

105
00:04:37.060 --> 00:04:39.400
Let's say though that the intrusive programs

106
00:04:39.400 --> 00:04:43.160
all take a different angle in terms of being different

107
00:04:43.160 --> 00:04:44.850
than the normal programs.

108
00:04:44.850 --> 00:04:47.500
So for instance, we don't see two distinct clusters here.

109
00:04:47.500 --> 00:04:50.250
We actually just see a cluster of normal programs

110
00:04:50.250 --> 00:04:54.320
and then the odd intrusive programs along the edges.

111
00:04:54.320 --> 00:04:56.680
One thing that we could do is bring in an unseen example.

112
00:04:56.680 --> 00:04:58.080
Let's say it lands over here

113
00:04:58.080 --> 00:04:59.870
and take the K nearest neighbors

114
00:04:59.870 --> 00:05:01.040
which are normal.

115
00:05:01.040 --> 00:05:03.450
K equals to three and we only count the neighbors

116
00:05:03.450 --> 00:05:04.283
that are normal.

117
00:05:04.283 --> 00:05:08.290
Then what we can do is average the distances, here's 3.8.

118
00:05:08.290 --> 00:05:13.260
And if that distance of 3.8 is greater than some threshold

119
00:05:13.260 --> 00:05:15.970
that we can tune through cross validation,

120
00:05:15.970 --> 00:05:18.710
then we would assign that program as intrusive.

121
00:05:18.710 --> 00:05:21.730
Let's say that we were continuing along with those features.

122
00:05:21.730 --> 00:05:24.140
So we had our TF-IDF features,

123
00:05:24.140 --> 00:05:25.370
and then all of a sudden we got introduced

124
00:05:25.370 --> 00:05:26.990
this priority feature.

125
00:05:26.990 --> 00:05:30.630
Priority means what priority the operating system

126
00:05:30.630 --> 00:05:32.360
will execute the process at.

127
00:05:32.360 --> 00:05:36.260
Priority is an ordinal feature which means it's categorical,

128
00:05:36.260 --> 00:05:37.850
but there is order to it.

129
00:05:37.850 --> 00:05:39.300
So you can have a low priority,

130
00:05:39.300 --> 00:05:43.400
a medium priority and high priority as a particular process.

131
00:05:43.400 --> 00:05:45.860
So since we have a mix of features now,

132
00:05:45.860 --> 00:05:47.950
we're gonna use something called the Gower distance.

133
00:05:47.950 --> 00:05:50.930
High is going to be assigned to be zero,

134
00:05:50.930 --> 00:05:53.140
medium one, low two,

135
00:05:53.140 --> 00:05:55.410
then what you do is you divide this number

136
00:05:55.410 --> 00:05:56.790
by the maximum numbers.

137
00:05:56.790 --> 00:05:59.440
So zero divided by two, one divided by two,

138
00:05:59.440 --> 00:06:04.130
two divided by two, and that'll give us 0.5 and one

139
00:06:04.130 --> 00:06:07.020
and that will actually go in as our priority.

140
00:06:07.020 --> 00:06:09.350
If this pid one had medium priority,

141
00:06:09.350 --> 00:06:11.940
we would assign the feature to be 0.5.

142
00:06:11.940 --> 00:06:14.430
This is just a way to normalize the features

143
00:06:14.430 --> 00:06:15.810
to make sure they're between zero

144
00:06:15.810 --> 00:06:19.460
and one and still represent what they should

145
00:06:19.460 --> 00:06:21.960
which is distance between each other.

146
00:06:21.960 --> 00:06:24.230
Let's say that we have two new features now.

147
00:06:24.230 --> 00:06:25.530
One of the features is going to be,

148
00:06:25.530 --> 00:06:29.350
did the program run in pseudo mode, which is a super user.

149
00:06:29.350 --> 00:06:30.580
The other feature is going to be,

150
00:06:30.580 --> 00:06:33.270
did the program itself terminate early

151
00:06:33.270 --> 00:06:36.030
or exit out early before it was finished computing?

152
00:06:36.030 --> 00:06:37.830
So both of them are going to be

153
00:06:37.830 --> 00:06:39.670
what's called asymmetric binary.

154
00:06:39.670 --> 00:06:41.610
Let's say a vast majority of the programs

155
00:06:41.610 --> 00:06:42.780
don't run in pseudo.

156
00:06:42.780 --> 00:06:45.920
And it's actually more informative to us

157
00:06:45.920 --> 00:06:48.290
if we understand that a program ran in pseudo,

158
00:06:48.290 --> 00:06:49.800
same with early exits.

159
00:06:49.800 --> 00:06:52.780
If all the ton of programs finish without any problem,

160
00:06:52.780 --> 00:06:55.280
it's more informative that it didn't finish.

161
00:06:55.280 --> 00:06:57.450
We call that asymmetric binary.

162
00:06:57.450 --> 00:06:58.940
And with that we have to use something called

163
00:06:58.940 --> 00:07:00.440
the Jaccard distance

164
00:07:00.440 --> 00:07:03.030
where we have two examples here, pid one

165
00:07:03.030 --> 00:07:05.010
and some other example.

166
00:07:05.010 --> 00:07:07.800
We count the number of mismatches between them

167
00:07:07.800 --> 00:07:08.840
because these are binary.

168
00:07:08.840 --> 00:07:11.840
We count the number of mismatches and divide by the number

169
00:07:11.840 --> 00:07:15.680
of total comparisons except where they're both zero,

170
00:07:15.680 --> 00:07:18.650
because we've deemed that not exiting early

171
00:07:18.650 --> 00:07:21.300
or not being pseudo is not as important

172
00:07:21.300 --> 00:07:22.610
so we don't really care about that.

173
00:07:22.610 --> 00:07:25.020
So we count the number of mismatches and the number

174
00:07:25.020 --> 00:07:29.480
of total elements or comparisons, and we exclude the zeros.

175
00:07:29.480 --> 00:07:32.170
Okay, so we have the priority.

176
00:07:32.170 --> 00:07:34.680
We have the pseudo, we have the early exit,

177
00:07:34.680 --> 00:07:36.610
and now we have another binary feature

178
00:07:36.610 --> 00:07:37.890
that we think will be helpful,

179
00:07:37.890 --> 00:07:39.540
did the user operated their program

180
00:07:39.540 --> 00:07:42.610
on a Mac or on a PC in our network?

181
00:07:42.610 --> 00:07:46.160
And for us, it's not really more informative

182
00:07:46.160 --> 00:07:48.130
that they used a Mac or a PC.

183
00:07:48.130 --> 00:07:50.520
So this is actually called a symmetric binary.

184
00:07:50.520 --> 00:07:52.550
All we have to do here is just a simple matching distance.

185
00:07:52.550 --> 00:07:54.010
So if we have another example,

186
00:07:54.010 --> 00:07:55.560
count the number of mismatches,

187
00:07:55.560 --> 00:07:57.360
divide by the total number of comparisons.

188
00:07:57.360 --> 00:07:59.780
And this time we do include the zeros.

189
00:07:59.780 --> 00:08:02.320
So the Gower distance aims to normalize all your features

190
00:08:02.320 --> 00:08:03.630
between zero and one.

191
00:08:03.630 --> 00:08:06.740
So we'll use the Euclidean distance for these variables.

192
00:08:06.740 --> 00:08:08.760
We'll use the ordinal distance from this,

193
00:08:08.760 --> 00:08:10.900
the Jaccard distance for the asymmetric binary,

194
00:08:10.900 --> 00:08:13.280
and the simple matching distance for these variables.

195
00:08:13.280 --> 00:08:15.640
And we can just mix and match and use all of these

196
00:08:15.640 --> 00:08:19.166
to make up our total distance between two processes.

197
00:08:19.166 --> 00:08:22.159
Again, these Euclidean distances will be min-max scaled.

198
00:08:22.159 --> 00:08:24.380
You can also use Manhattan distance here.

199
00:08:24.380 --> 00:08:26.550
Manhattan distance is simply this value,

200
00:08:26.550 --> 00:08:27.983
but it's the difference between the elements

201
00:08:27.983 --> 00:08:30.240
and just the absolute value of them.

202
00:08:30.240 --> 00:08:32.150
Now let's move on to our next example.

203
00:08:32.150 --> 00:08:34.550
Imagine that we work for a marketing agency contracted

204
00:08:34.550 --> 00:08:37.000
by universities to drive alumni donations.

205
00:08:37.000 --> 00:08:39.530
We collect 10% of the total donations

206
00:08:39.530 --> 00:08:42.430
during the calendar year as compensation.

207
00:08:42.430 --> 00:08:45.740
Since calling everyone is prohibitively expensive

208
00:08:45.740 --> 00:08:48.000
and annoying to some alumni,

209
00:08:48.000 --> 00:08:49.720
we could use K nearest neighbors to predict

210
00:08:49.720 --> 00:08:51.530
how much someone may donate

211
00:08:51.530 --> 00:08:53.250
before deciding to call them.

212
00:08:53.250 --> 00:08:55.380
This way we can optimize donations

213
00:08:55.380 --> 00:08:57.750
without engaging every alumni.

214
00:08:57.750 --> 00:08:59.703
Okay, so the universities provide us

215
00:08:59.703 --> 00:09:03.040
with alumni information, including their college

216
00:09:03.040 --> 00:09:04.780
such as engineering, social sciences,

217
00:09:04.780 --> 00:09:05.880
college of math.

218
00:09:05.880 --> 00:09:08.850
Graduation year, which only goes to 10 years ago.

219
00:09:08.850 --> 00:09:12.360
So we've changed it into a feature between zero and 10,

220
00:09:12.360 --> 00:09:15.330
indicating how many years ago they've graduated.

221
00:09:15.330 --> 00:09:17.130
And finally, we have a feature

222
00:09:17.130 --> 00:09:18.810
which represents the join date

223
00:09:18.810 --> 00:09:20.420
of the university's Facebook group.

224
00:09:20.420 --> 00:09:22.830
The reason why this is important to us is because

225
00:09:22.830 --> 00:09:26.040
once that person has joined the university Facebook group,

226
00:09:26.040 --> 00:09:28.430
they immediately get asked to provide donations

227
00:09:28.430 --> 00:09:29.263
to the university.

228
00:09:29.263 --> 00:09:31.270
And we don't wanna double ask those people.

229
00:09:31.270 --> 00:09:34.030
So what we'll do is we'll just exclude anybody

230
00:09:34.030 --> 00:09:37.870
who has joined the Facebook group within the last year.

231
00:09:37.870 --> 00:09:39.980
We won't use the feature for anything else.

232
00:09:39.980 --> 00:09:43.610
Now for labels, what we get is the amount donated

233
00:09:43.610 --> 00:09:45.680
to the university by an individual

234
00:09:45.680 --> 00:09:47.860
within last year's campaign.

235
00:09:47.860 --> 00:09:49.900
So first we need to start vectorizing these inputs

236
00:09:49.900 --> 00:09:51.570
for the K nearest neighbors model.

237
00:09:51.570 --> 00:09:52.410
So for the college,

238
00:09:52.410 --> 00:09:53.730
it's gonna be categorical.

239
00:09:53.730 --> 00:09:56.920
Graduation year will be ordinal since it can be ordered

240
00:09:56.920 --> 00:09:58.240
and it's categorical.

241
00:09:58.240 --> 00:09:59.650
And the university Facebook group,

242
00:09:59.650 --> 00:10:01.220
we're not going to use them.

243
00:10:01.220 --> 00:10:02.890
That's just a do not call list.

244
00:10:02.890 --> 00:10:05.370
So for categorical, we're going to use Hamming distance.

245
00:10:05.370 --> 00:10:07.650
For ordinal, we're going to incorporate the element

246
00:10:07.650 --> 00:10:08.610
of the Gower distance.

247
00:10:08.610 --> 00:10:11.362
Hamming distance is just going to be the number

248
00:10:11.362 --> 00:10:14.060
of differences in category.

249
00:10:14.060 --> 00:10:18.870
For instance, if the major for one alumni was engineering

250
00:10:18.870 --> 00:10:20.220
and the other major had to do

251
00:10:20.220 --> 00:10:22.320
with business for the second alumni,

252
00:10:22.320 --> 00:10:25.770
then the distance between them is going to be one.

253
00:10:25.770 --> 00:10:27.720
However, if alumni one did engineering

254
00:10:27.720 --> 00:10:31.000
and alumni two did engineering, then the distance is zero.

255
00:10:31.000 --> 00:10:34.410
So it's simply a matching notation for categories.

256
00:10:34.410 --> 00:10:37.270
Now, imagining that the examples could be plotted

257
00:10:37.270 --> 00:10:38.430
in two dimensions.

258
00:10:38.430 --> 00:10:40.210
Obviously they exist in higher dimensions

259
00:10:40.210 --> 00:10:42.450
but for now it just helps for visualization purposes.

260
00:10:42.450 --> 00:10:45.370
We can see here that the labels of the alumni

261
00:10:45.370 --> 00:10:48.020
are actually numbers which we expected,

262
00:10:48.020 --> 00:10:49.450
but we haven't really gone over

263
00:10:49.450 --> 00:10:51.620
how to perform K nearest neighbors on them.

264
00:10:51.620 --> 00:10:52.540
So let's go ahead and deal

265
00:10:52.540 --> 00:10:53.960
with these continuous labels here.

266
00:10:53.960 --> 00:10:56.152
Let's say that we get an unseen example.

267
00:10:56.152 --> 00:10:59.810
So we don't know how much this alumni will donate

268
00:10:59.810 --> 00:11:01.120
but we have their features.

269
00:11:01.120 --> 00:11:03.540
And we'd like to form a prediction based on their features,

270
00:11:03.540 --> 00:11:06.210
how much this alumni will potentially donate.

271
00:11:06.210 --> 00:11:08.400
So if we assume K is equal to three,

272
00:11:08.400 --> 00:11:11.140
we're gonna sample the three nearest neighbors

273
00:11:11.140 --> 00:11:12.680
to this alumni.

274
00:11:12.680 --> 00:11:14.410
And we're going to take the average

275
00:11:14.410 --> 00:11:16.190
of the nearest neighbors, that's it.

276
00:11:16.190 --> 00:11:17.960
Now for practical considerations,

277
00:11:17.960 --> 00:11:20.500
it performs poorly when the features are in high dimension.

278
00:11:20.500 --> 00:11:23.060
So if you have tons and tons of features,

279
00:11:23.060 --> 00:11:25.170
the distance metrics break down

280
00:11:25.170 --> 00:11:26.680
especially Euclidean distance.

281
00:11:26.680 --> 00:11:29.490
So let's say that we have just a single dimension here

282
00:11:29.490 --> 00:11:33.550
and A lies at point one and B lies at point five,

283
00:11:33.550 --> 00:11:35.530
so the distance here is five.

284
00:11:35.530 --> 00:11:39.130
Now to get the same distance of five as separation.

285
00:11:39.130 --> 00:11:40.700
If we go to two dimensions though,

286
00:11:40.700 --> 00:11:44.090
in order to measure five units of distance,

287
00:11:44.090 --> 00:11:46.600
we need four units of distance in one dimension

288
00:11:46.600 --> 00:11:48.930
and three units of distance in the other dimension

289
00:11:48.930 --> 00:11:51.110
for a total of seven units of distance

290
00:11:51.110 --> 00:11:52.990
just to represent the original five

291
00:11:52.990 --> 00:11:54.610
that we got in one dimension.

292
00:11:54.610 --> 00:11:56.270
Now, if we continue this on to three dimensions.

293
00:11:56.270 --> 00:11:58.600
Imagine this goes back into the screen.

294
00:11:58.600 --> 00:11:59.990
We now all of a sudden need three

295
00:11:59.990 --> 00:12:02.040
and a half units of distance in this dimension,

296
00:12:02.040 --> 00:12:03.380
two and a half units in that dimension

297
00:12:03.380 --> 00:12:05.890
and 2.8 units in that dimension just to still

298
00:12:05.890 --> 00:12:08.520
represent the steady five units of difference

299
00:12:08.520 --> 00:12:11.090
according to the Euclidean distance formula.

300
00:12:11.090 --> 00:12:13.780
So as you grow in dimensionality,

301
00:12:13.780 --> 00:12:15.820
you actually see this result come out

302
00:12:15.820 --> 00:12:17.880
to mean that you need more separation

303
00:12:17.880 --> 00:12:20.650
between points to represent the same distance

304
00:12:20.650 --> 00:12:22.600
which can hurt the nearest neighbors model.

305
00:12:22.600 --> 00:12:24.770
So what are some actual ways to reduce the dimensions

306
00:12:24.770 --> 00:12:25.603
of this data?

307
00:12:25.603 --> 00:12:27.170
Well, later in the course, we'll actually talk

308
00:12:27.170 --> 00:12:29.640
about pretty generic ways to reduce dimensionality.

309
00:12:29.640 --> 00:12:31.060
But for now, one that you can try

310
00:12:31.060 --> 00:12:33.830
with K nearest neighbors is to simply pick one feature

311
00:12:33.830 --> 00:12:36.040
and perform cross validation on it

312
00:12:36.040 --> 00:12:37.210
and then pick another feature,

313
00:12:37.210 --> 00:12:39.890
perform cross validation on it and measure the accuracy.

314
00:12:39.890 --> 00:12:42.400
And if that one beats all the others,

315
00:12:42.400 --> 00:12:43.560
let's say you tried all of them.

316
00:12:43.560 --> 00:12:44.393
And this one was the best.

317
00:12:44.393 --> 00:12:48.060
You lock that in and pick that as a solidified feature,

318
00:12:48.060 --> 00:12:49.210
that feature is locked in

319
00:12:49.210 --> 00:12:51.480
and then you try all the other features again.

320
00:12:51.480 --> 00:12:52.960
And let's say this one was the best.

321
00:12:52.960 --> 00:12:55.810
So you have two features now and you keep doing this

322
00:12:55.810 --> 00:12:57.970
until you can get a good representation

323
00:12:57.970 --> 00:12:59.960
or a good score comparatively

324
00:12:59.960 --> 00:13:03.070
than rather using a lot more dimensions.

325
00:13:03.070 --> 00:13:06.490
So you're greedily selecting which features are best

326
00:13:06.490 --> 00:13:09.040
based on their incremental performance.

327
00:13:09.040 --> 00:13:11.920
So another thing to consider is that any model

328
00:13:11.920 --> 00:13:14.350
that's based upon distance, especially K nearest neighbors

329
00:13:14.350 --> 00:13:16.570
is going to be sensitive to scaling.

330
00:13:16.570 --> 00:13:19.209
Okay, for instance if we have height, waistline,

331
00:13:19.209 --> 00:13:22.830
and distance run weekly, and they're all in meters,

332
00:13:22.830 --> 00:13:25.840
the distance run weekly is going to be a lot larger

333
00:13:25.840 --> 00:13:27.160
than someone's waistline.

334
00:13:27.160 --> 00:13:29.450
In this case, you probably don't want to scale

335
00:13:29.450 --> 00:13:31.460
because these differences are meaningful

336
00:13:31.460 --> 00:13:33.950
and they all are using the same units.

337
00:13:33.950 --> 00:13:35.900
So in this case you probably are better going

338
00:13:35.900 --> 00:13:37.350
with Manhattan distance.

339
00:13:37.350 --> 00:13:39.020
However, if you had something like this

340
00:13:39.020 --> 00:13:40.390
where you wanted to predict heart disease

341
00:13:40.390 --> 00:13:42.120
wit a caloric intake per day.

342
00:13:42.120 --> 00:13:44.310
This is a very different unit than the others.

343
00:13:44.310 --> 00:13:47.160
So I would recommend here using Euclidean distance

344
00:13:47.160 --> 00:13:49.450
and doing a min-max scale to put all

345
00:13:49.450 --> 00:13:51.870
of these values between zero and one.

346
00:13:51.870 --> 00:13:53.738
Another thing we can do to improve performance

347
00:13:53.738 --> 00:13:55.410
is to weigh the votes.

348
00:13:55.410 --> 00:13:58.190
So take this example where K is equal to four

349
00:13:58.190 --> 00:14:01.150
and these are the three nearer neighbors.

350
00:14:01.150 --> 00:14:02.980
Then this fourth nearest neighbor.

351
00:14:02.980 --> 00:14:06.520
We can make the vote of node seven mean more

352
00:14:06.520 --> 00:14:08.190
than the vote of node six.

353
00:14:08.190 --> 00:14:10.300
So for instance, the vote of node seven

354
00:14:10.300 --> 00:14:12.650
is going to be the distance of eight,

355
00:14:12.650 --> 00:14:13.680
plus distance of seven,

356
00:14:13.680 --> 00:14:14.610
plus distance of two,

357
00:14:14.610 --> 00:14:15.700
plus distance of six,

358
00:14:15.700 --> 00:14:18.470
all divided by the distance of seven.

359
00:14:18.470 --> 00:14:21.360
The vote of node six is gonna have the same numerator

360
00:14:21.360 --> 00:14:24.150
and now its denominator is just going to be different.

361
00:14:24.150 --> 00:14:26.450
So as the closer a point is,

362
00:14:26.450 --> 00:14:28.720
the d seven is going to be smaller than d six.

363
00:14:28.720 --> 00:14:31.200
So since you're dividing by a smaller number,

364
00:14:31.200 --> 00:14:33.490
the overall vote will be larger.

365
00:14:33.490 --> 00:14:36.410
D six here is larger than d seven.

366
00:14:36.410 --> 00:14:37.700
So since that's in the numerator

367
00:14:37.700 --> 00:14:39.970
you're going to be dividing by a larger number,

368
00:14:39.970 --> 00:14:42.830
which will make the vote of node six be diminished.

369
00:14:42.830 --> 00:14:43.910
One thing to consider is that

370
00:14:43.910 --> 00:14:45.400
it is computationally expensive.

371
00:14:45.400 --> 00:14:47.260
here it's O to n, d, k,

372
00:14:47.260 --> 00:14:48.540
which is the number of nodes,

373
00:14:48.540 --> 00:14:50.420
the dimension of each node,

374
00:14:50.420 --> 00:14:52.910
and K for the K nearest neighbors.

375
00:14:52.910 --> 00:14:55.100
To get the computational complexity down,

376
00:14:55.100 --> 00:14:57.530
we can use the data structures such as a KD tree

377
00:14:57.530 --> 00:15:00.370
which allows us to locate relevant points faster

378
00:15:00.370 --> 00:15:02.010
than iterating through all of the points.

379
00:15:02.010 --> 00:15:04.620
We won't go over the data structure in detail

380
00:15:04.620 --> 00:15:07.466
but a KD tree is effectively a binary tree

381
00:15:07.466 --> 00:15:10.180
which splits the end dimensional space here too

382
00:15:10.180 --> 00:15:14.920
into sections by alternating each access per layer.

383
00:15:14.920 --> 00:15:16.440
This will help us get our time complexity

384
00:15:16.440 --> 00:15:18.800
at predicting a single example down to an average

385
00:15:18.800 --> 00:15:22.800
of K log n assuming that our dimension is fixed.

386
00:15:22.800 --> 00:15:24.500
Keep in mind, we have to build this KD tree

387
00:15:24.500 --> 00:15:25.470
and that's not free.

388
00:15:25.470 --> 00:15:28.040
Be aware that this data structure will suffer

389
00:15:28.040 --> 00:15:29.950
from dimensionality growth as well.

390
00:15:29.950 --> 00:15:31.390
This tree structure will degrade

391
00:15:31.390 --> 00:15:35.740
to O of n, k, d as the dimensions grow large.

392
00:15:35.740 --> 00:15:37.050
Okay, so now we've gone over

393
00:15:37.050 --> 00:15:39.490
what a K nearest neighbors model intuitively is,

394
00:15:39.490 --> 00:15:41.249
how to use it with various types of features,

395
00:15:41.249 --> 00:15:43.910
how to measure the distance between two examples

396
00:15:43.910 --> 00:15:47.090
to determine which are the K nearest neighbors.

397
00:15:47.090 --> 00:15:49.520
We've also covered the limitations of K nearest neighbors

398
00:15:49.520 --> 00:15:52.130
and also some practical considerations when using it.

399
00:15:52.130 --> 00:15:54.850
We also applied K nearest neighbors in a practical sense.

400
00:15:54.850 --> 00:15:56.440
So for the cyber security case,

401
00:15:56.440 --> 00:16:00.460
we got an F one score of about 83% that was offline.

402
00:16:00.460 --> 00:16:04.350
And within the first month we got two alarms raised

403
00:16:04.350 --> 00:16:05.380
by the algorithm.

404
00:16:05.380 --> 00:16:07.170
One of which was a false alarm,

405
00:16:07.170 --> 00:16:09.320
and two of which we shut down immediately

406
00:16:09.320 --> 00:16:11.670
because they were found to actually be intrusive.

407
00:16:11.670 --> 00:16:14.610
For the university that contracted us out

408
00:16:14.610 --> 00:16:17.640
to drive alumni donations, we accomplish two things.

409
00:16:17.640 --> 00:16:18.930
From the business perspective,

410
00:16:18.930 --> 00:16:22.120
it was great because we got about a 20% ROI.

411
00:16:22.120 --> 00:16:24.650
We had to hire people to make the calls out

412
00:16:24.650 --> 00:16:25.590
to these graduates.

413
00:16:25.590 --> 00:16:27.440
And we would only keep 10% of the revenue.

414
00:16:27.440 --> 00:16:29.580
The university got 90% of that.

415
00:16:29.580 --> 00:16:31.990
As well, the university reached out to us and said

416
00:16:31.990 --> 00:16:34.950
that they have never gotten such a positive response

417
00:16:34.950 --> 00:16:37.240
from being called to donate.

418
00:16:37.240 --> 00:16:39.390
So I think our algorithm did a good job

419
00:16:39.390 --> 00:16:41.890
in isolating the people who didn't wanna be called

420
00:16:41.890 --> 00:16:43.280
and we didn't call them.

421
00:16:43.280 --> 00:16:46.130
So I think it did well for two purposes there.

422
00:16:46.130 --> 00:16:48.400
That's it for this video, please join us next video

423
00:16:48.400 --> 00:16:50.600
as we continue our machine learning journey.

