WEBVTT

1
00:00:00.370 --> 00:00:01.530
<v Instructor>Welcome back everybody.</v>

2
00:00:01.530 --> 00:00:05.940
This is ML Experts Machine Learning Crash Course.

3
00:00:05.940 --> 00:00:08.980
Previously, we went over logistic regression.

4
00:00:08.980 --> 00:00:12.540
Logistic regression determined the probability of a class.

5
00:00:12.540 --> 00:00:15.820
For instance, no power outage or power outage,

6
00:00:15.820 --> 00:00:17.470
given some features.

7
00:00:17.470 --> 00:00:22.290
Now we used a tuneable decision boundary here 50%

8
00:00:22.290 --> 00:00:23.620
to determine the class

9
00:00:23.620 --> 00:00:25.890
in thus the decision boundary itself.

10
00:00:25.890 --> 00:00:27.260
What if we didn't care

11
00:00:27.260 --> 00:00:29.740
about calculating the exact probability

12
00:00:29.740 --> 00:00:31.740
of one class or another?

13
00:00:31.740 --> 00:00:33.030
What if we just cared

14
00:00:33.030 --> 00:00:35.670
that the examples were correctly classified?

15
00:00:35.670 --> 00:00:38.760
This is the idea behind Support Vector Machines.

16
00:00:38.760 --> 00:00:41.480
SVMs or Support Vector Machines,

17
00:00:41.480 --> 00:00:44.570
remove the concept of a decision threshold

18
00:00:44.570 --> 00:00:47.580
by instead selecting a decision boundary

19
00:00:47.580 --> 00:00:51.000
which maximizes the distance between itself

20
00:00:51.000 --> 00:00:54.810
and the two most difficult examples to classify.

21
00:00:54.810 --> 00:00:57.870
These two most difficult examples to classify

22
00:00:57.870 --> 00:00:59.960
are called the Support Vectors.

23
00:00:59.960 --> 00:01:04.100
In comparison, logistic regression found a decision boundary

24
00:01:04.100 --> 00:01:06.950
which minimized the negative log loss

25
00:01:06.950 --> 00:01:08.253
of the training examples.

26
00:01:09.140 --> 00:01:13.400
SVMs on the other hand only focus on the support vectors

27
00:01:13.400 --> 00:01:17.313
or the most difficult to classify points across the data.

28
00:01:18.250 --> 00:01:21.230
Now for SVMs, the decision boundary

29
00:01:21.230 --> 00:01:22.980
also called the hyperplane

30
00:01:22.980 --> 00:01:27.980
is defined by the equation zero is equal to WX minus B.

31
00:01:30.460 --> 00:01:32.120
This should be extremely familiar

32
00:01:32.120 --> 00:01:33.750
as it's the same line equation

33
00:01:33.750 --> 00:01:35.690
we've been using for your linear regression

34
00:01:35.690 --> 00:01:38.200
and also logistic regression.

35
00:01:38.200 --> 00:01:41.430
The values of W and B will be optimized

36
00:01:41.430 --> 00:01:44.350
when the distance between the decision boundary

37
00:01:44.350 --> 00:01:47.340
and the support vectors is maximized.

38
00:01:47.340 --> 00:01:49.120
You can think about support vector machines

39
00:01:49.120 --> 00:01:51.070
is actually making a three lines,

40
00:01:51.070 --> 00:01:55.320
one line for the hyperplane defined by this equation.

41
00:01:55.320 --> 00:01:57.760
Another line defined by this equation

42
00:01:57.760 --> 00:02:00.240
in which all of the positive examples,

43
00:02:00.240 --> 00:02:02.250
here a positive example is

44
00:02:02.250 --> 00:02:05.220
any example which was labeled to have a power outage.

45
00:02:05.220 --> 00:02:09.470
All of the positive examples lie to the right of this line

46
00:02:09.470 --> 00:02:11.450
and all of the negative examples

47
00:02:11.450 --> 00:02:14.170
or the examples where a power outage did not occur

48
00:02:14.170 --> 00:02:16.760
lie to the left of that line.

49
00:02:16.760 --> 00:02:18.710
Notice here that a negative example

50
00:02:18.710 --> 00:02:21.570
is defined by negative one instead of zero

51
00:02:21.570 --> 00:02:24.160
as it was in logistic regression.

52
00:02:24.160 --> 00:02:26.940
Now here, we actually have two dimensions of data

53
00:02:26.940 --> 00:02:28.530
to predict our label

54
00:02:28.530 --> 00:02:32.200
which means that this equation is actually a dot product.

55
00:02:32.200 --> 00:02:34.790
A dot product is going to be the first weight

56
00:02:34.790 --> 00:02:37.400
times the first dimension, plus the second weight

57
00:02:37.400 --> 00:02:38.680
times a second dimension.

58
00:02:38.680 --> 00:02:40.760
This is exactly how we've been representing

59
00:02:40.760 --> 00:02:43.200
our line equations for both linear regression

60
00:02:43.200 --> 00:02:44.550
and logistic regression.

61
00:02:44.550 --> 00:02:47.430
We're now just introducing a more compact form

62
00:02:47.430 --> 00:02:48.820
called the dot product.

63
00:02:48.820 --> 00:02:51.860
Now the distance between the negative example line

64
00:02:51.860 --> 00:02:55.630
and the positive example line is called the margin,

65
00:02:55.630 --> 00:03:00.630
and the margin is equal to two over the norm of W.

66
00:03:01.210 --> 00:03:04.940
Here the norm is just equal to the first weight squared

67
00:03:04.940 --> 00:03:06.810
plus all the other weight squared

68
00:03:06.810 --> 00:03:08.650
and then the square root of that,

69
00:03:08.650 --> 00:03:12.240
two over that number will give you the margin.

70
00:03:12.240 --> 00:03:14.880
We said earlier that the goal of an SVM

71
00:03:14.880 --> 00:03:18.620
is to maximize the distance between the decision boundary

72
00:03:18.620 --> 00:03:22.070
or the hyperplane and its support vectors.

73
00:03:22.070 --> 00:03:25.300
So here, this is one over the norm of W,

74
00:03:25.300 --> 00:03:28.950
this side over here is one over norm W

75
00:03:28.950 --> 00:03:31.790
and the total distance between the two

76
00:03:31.790 --> 00:03:33.690
is going to be two times that

77
00:03:33.690 --> 00:03:35.543
and that's where we get our margin.

78
00:03:36.400 --> 00:03:39.310
Now, if our goal is to maximize the distance

79
00:03:39.310 --> 00:03:41.950
between our decision boundary and our support vectors

80
00:03:41.950 --> 00:03:46.210
then technically we want to minimize the denominator here

81
00:03:46.210 --> 00:03:50.250
that will ensure that we have the largest margin possible.

82
00:03:50.250 --> 00:03:54.560
What we can't do is maximize our margin so much so

83
00:03:54.560 --> 00:03:57.000
that we actually end up going past

84
00:03:57.000 --> 00:03:59.150
the support vectors themselves.

85
00:03:59.150 --> 00:04:02.240
Remember, the idea was that this line

86
00:04:02.240 --> 00:04:05.580
will ensure that all the examples to the left of it

87
00:04:05.580 --> 00:04:07.790
are negative examples

88
00:04:07.790 --> 00:04:10.130
and this line to the right of it

89
00:04:10.130 --> 00:04:13.210
all of the examples will be positive labels.

90
00:04:13.210 --> 00:04:15.490
So if we make the margin too big

91
00:04:15.490 --> 00:04:18.650
then positive examples actually end up within the margin.

92
00:04:18.650 --> 00:04:21.240
And so do negative examples as well.

93
00:04:21.240 --> 00:04:23.520
This violates our rules.

94
00:04:23.520 --> 00:04:25.260
Mathematically, what this means

95
00:04:25.260 --> 00:04:28.080
is that we want to minimize our norm

96
00:04:28.080 --> 00:04:30.290
therefore maximizing the margin

97
00:04:30.290 --> 00:04:32.600
because that's two over the norm of W

98
00:04:33.510 --> 00:04:37.870
while also considering that our first-line equation

99
00:04:37.870 --> 00:04:40.540
which categorizes positive examples

100
00:04:40.540 --> 00:04:44.300
always produces something greater than or equal to one

101
00:04:44.300 --> 00:04:47.600
given that it is in fact, a positive example.

102
00:04:47.600 --> 00:04:49.650
We also want to enforce the fact

103
00:04:49.650 --> 00:04:51.900
that the negative example line

104
00:04:51.900 --> 00:04:54.810
will always be less than or equal to one,

105
00:04:54.810 --> 00:04:58.050
given that the example itself is negative.

106
00:04:58.050 --> 00:05:01.010
This is saying the same thing we just said mathematically

107
00:05:01.010 --> 00:05:02.560
instead of graphically.

108
00:05:02.560 --> 00:05:05.480
So here, what we want is the positive examples

109
00:05:05.480 --> 00:05:07.590
to be on the right side of the margin.

110
00:05:07.590 --> 00:05:09.190
We want the negative examples

111
00:05:09.190 --> 00:05:11.170
to be on the left side of the margin

112
00:05:11.170 --> 00:05:13.390
and how we represent that mathematically

113
00:05:13.390 --> 00:05:15.523
is with these two inequalities.

114
00:05:16.610 --> 00:05:19.610
Technically for mathematical conciseness

115
00:05:19.610 --> 00:05:23.740
we can combine them into a single inequality like this.

116
00:05:23.740 --> 00:05:26.150
All we did was now multiplied by the

117
00:05:26.150 --> 00:05:28.370
true label of the example

118
00:05:28.370 --> 00:05:32.270
and that will allow us to combine the two inequalities

119
00:05:32.270 --> 00:05:34.230
into one inequality.

120
00:05:34.230 --> 00:05:38.510
Now, this is the equation to solve for that line

121
00:05:38.510 --> 00:05:41.980
that best separates the support vectors here.

122
00:05:41.980 --> 00:05:44.660
Our goal is to maximize the margin

123
00:05:44.660 --> 00:05:47.170
or minimize the norm of W

124
00:05:47.170 --> 00:05:49.930
since that's in the denominator of the margin.

125
00:05:49.930 --> 00:05:53.220
The other goal is to make sure that all of our labels

126
00:05:53.220 --> 00:05:56.900
are on the correct side of the margin.

127
00:05:56.900 --> 00:05:59.760
Notice that this optimization is different

128
00:05:59.760 --> 00:06:03.130
from what we had to optimize for logistic regression.

129
00:06:03.130 --> 00:06:05.420
This problem has constraints

130
00:06:05.420 --> 00:06:07.840
as well as something to optimize.

131
00:06:07.840 --> 00:06:12.030
Logistic regression was minimizing the Cross-entropy loss

132
00:06:12.030 --> 00:06:15.690
here, we're minimizing the norm of the W

133
00:06:16.640 --> 00:06:19.450
but in the case of SVMs we also have

134
00:06:19.450 --> 00:06:21.990
a constraint to worry about.

135
00:06:21.990 --> 00:06:24.130
Gradient descent in its typical form

136
00:06:24.130 --> 00:06:25.920
can't perform optimization

137
00:06:25.920 --> 00:06:28.600
while also considering constraints.

138
00:06:28.600 --> 00:06:31.420
So we have to use another optimization technique

139
00:06:31.420 --> 00:06:35.090
which can handle constrained optimization problems.

140
00:06:35.090 --> 00:06:37.200
Since W is a linear term,

141
00:06:37.200 --> 00:06:39.870
we could use a technique called linear programming.

142
00:06:39.870 --> 00:06:42.400
However linear programming won't guarantee us

143
00:06:42.400 --> 00:06:43.800
a unique solution

144
00:06:43.800 --> 00:06:46.660
and it can be unstable in some cases.

145
00:06:46.660 --> 00:06:49.570
So typically people square this term

146
00:06:49.570 --> 00:06:51.680
which makes the term quadratic.

147
00:06:51.680 --> 00:06:54.740
This allows us to use quadratic programming

148
00:06:54.740 --> 00:06:57.780
which will guarantee us a unique solution.

149
00:06:57.780 --> 00:07:01.680
If we used quadratic programming to fit our model weights

150
00:07:01.680 --> 00:07:05.810
we'd have, what's called a Hard-Margin SVM.

151
00:07:05.810 --> 00:07:09.770
This could work with our data that we show earlier.

152
00:07:09.770 --> 00:07:12.250
So what if our data looks like this?

153
00:07:12.250 --> 00:07:15.110
Here we have a case of a cluster over here

154
00:07:15.110 --> 00:07:18.510
and another cluster over here of different labeled data.

155
00:07:18.510 --> 00:07:21.420
And in the middle we have what's called an outlier.

156
00:07:21.420 --> 00:07:23.570
Well with our hard margin SVM

157
00:07:23.570 --> 00:07:26.800
we'd get this line as the optimal solution

158
00:07:26.800 --> 00:07:30.350
according to our quadratic program optimization.

159
00:07:30.350 --> 00:07:33.650
To me it doesn't really look correct.

160
00:07:33.650 --> 00:07:36.020
This is because these two data points

161
00:07:36.020 --> 00:07:37.980
would be considered the support vectors

162
00:07:37.980 --> 00:07:39.570
which we talked about earlier

163
00:07:39.570 --> 00:07:43.680
which are the two most difficult points to classify.

164
00:07:43.680 --> 00:07:46.790
What we'd really like is to be able to give some slack

165
00:07:46.790 --> 00:07:48.920
to this Hard-Margin SVM,

166
00:07:48.920 --> 00:07:52.340
such that it could assign support vectors like this

167
00:07:52.340 --> 00:07:55.380
and ignore this outlier.

168
00:07:55.380 --> 00:07:57.020
To ignore this outlier,

169
00:07:57.020 --> 00:08:00.020
we're going to have to use something called slack.

170
00:08:00.020 --> 00:08:02.740
Slack allows us to relax our constraints,

171
00:08:02.740 --> 00:08:05.770
such that every single label of this type

172
00:08:05.770 --> 00:08:07.710
doesn't have to be to the left

173
00:08:07.710 --> 00:08:09.760
of our support vector on the left.

174
00:08:09.760 --> 00:08:13.700
This gives us something called a Soft Margin SVM.

175
00:08:13.700 --> 00:08:16.010
So now we'll have to update our optimization

176
00:08:16.010 --> 00:08:19.550
and our constraints to support the idea of slack.

177
00:08:19.550 --> 00:08:21.090
To update this constraint,

178
00:08:21.090 --> 00:08:23.080
We'll have to add an ei term here.

179
00:08:23.080 --> 00:08:25.760
What this is saying is that no longer

180
00:08:25.760 --> 00:08:27.360
does every single point

181
00:08:27.360 --> 00:08:31.080
have to exist on the correct side of the margin.

182
00:08:31.080 --> 00:08:34.760
Instead, we can allow some points to go within the margin.

183
00:08:34.760 --> 00:08:38.890
This ei is the distance between our new support vector

184
00:08:38.890 --> 00:08:39.910
which is here

185
00:08:39.910 --> 00:08:41.390
and the distance between

186
00:08:41.390 --> 00:08:43.140
what the old support vector would have been

187
00:08:43.140 --> 00:08:46.120
in the case of the hard margin SVM

188
00:08:46.120 --> 00:08:48.260
as well as updating the constraint,

189
00:08:48.260 --> 00:08:51.380
we also have to update what we want to minimize.

190
00:08:51.380 --> 00:08:53.560
What this term means is that we're going to

191
00:08:53.560 --> 00:08:58.190
sum all of the variables or the distance from the margin

192
00:08:58.190 --> 00:09:00.090
that these examples are.

193
00:09:00.090 --> 00:09:03.230
We're going to sum that up and we want to minimize it.

194
00:09:03.230 --> 00:09:05.920
So, yes, while we've now allowed

195
00:09:05.920 --> 00:09:08.630
certain points to exist within the margin.

196
00:09:08.630 --> 00:09:11.150
We also want to minimize the amount

197
00:09:11.150 --> 00:09:13.600
that we allow points in the margin.

198
00:09:13.600 --> 00:09:16.983
This parameter here C is for regularization.

199
00:09:17.860 --> 00:09:20.910
This C term indicates how much we want to penalize

200
00:09:20.910 --> 00:09:24.850
a particular example lying with inside the margin.

201
00:09:24.850 --> 00:09:27.620
Let's sum up what this ei will equal

202
00:09:27.620 --> 00:09:30.210
based on where an example lies

203
00:09:30.210 --> 00:09:33.010
either inside or outside of the margin.

204
00:09:33.010 --> 00:09:36.140
E of I or some ei of an example

205
00:09:36.140 --> 00:09:37.940
is going to be equal to zero

206
00:09:37.940 --> 00:09:42.400
when that example lies on the correct side of the margin.

207
00:09:42.400 --> 00:09:44.840
If an example lies within its margin

208
00:09:44.840 --> 00:09:47.690
but not over the hyperplane itself

209
00:09:47.690 --> 00:09:51.250
then it will incur an error between zero and one.

210
00:09:51.250 --> 00:09:52.760
However, if an example lies

211
00:09:52.760 --> 00:09:55.020
on the other side of the hyperplane

212
00:09:55.020 --> 00:09:59.160
and not within its own margin between it and the hyperplane

213
00:09:59.160 --> 00:10:02.060
then the penalty incurred will be higher than one.

214
00:10:02.060 --> 00:10:03.740
And it will make this term higher

215
00:10:03.740 --> 00:10:05.143
when we want to minimize it.

216
00:10:06.090 --> 00:10:09.170
So here this ei is between zero and one

217
00:10:09.170 --> 00:10:12.040
because it hasn't quite crossed to the hyperplane yet.

218
00:10:12.040 --> 00:10:14.130
If this example did cross the hyperplane

219
00:10:14.130 --> 00:10:16.670
the ei would be larger than one.

220
00:10:16.670 --> 00:10:19.370
If this point lied on the correct side of the margin

221
00:10:19.370 --> 00:10:21.330
then the ei would be zero.

222
00:10:21.330 --> 00:10:24.480
All right so now we have detailed out our balancing act.

223
00:10:24.480 --> 00:10:26.700
This constraint is allowing us to ei

224
00:10:26.700 --> 00:10:28.750
and have points with inside the margin.

225
00:10:28.750 --> 00:10:31.160
And this equation is trying to minimize

226
00:10:31.160 --> 00:10:33.230
how many errors we have incurred

227
00:10:33.230 --> 00:10:36.640
from points lying within or over the margin.

228
00:10:36.640 --> 00:10:39.630
Notice that we have matching ei here

229
00:10:39.630 --> 00:10:43.160
and idea is that we could solve for ei in this equation

230
00:10:43.160 --> 00:10:45.800
and plug it in for ei here

231
00:10:45.800 --> 00:10:48.320
such that we no longer have any constraints

232
00:10:48.320 --> 00:10:51.200
associated with our optimization problem.

233
00:10:51.200 --> 00:10:53.690
We can do that by solving for ei.

234
00:10:53.690 --> 00:10:55.920
And with the understanding that e of i

235
00:10:55.920 --> 00:10:58.710
is always going to be greater than or equal to zero

236
00:10:58.710 --> 00:11:01.830
we can actually write our inequality like this.

237
00:11:01.830 --> 00:11:04.950
Now we can substitute this n for this ei.

238
00:11:04.950 --> 00:11:08.950
We now have an optimization problem that has no constraints.

239
00:11:08.950 --> 00:11:12.310
An obvious question is, can we use gradient descent now?

240
00:11:12.310 --> 00:11:14.470
If you recall for gradient descent

241
00:11:14.470 --> 00:11:17.820
we had to take the derivative of the loss function.

242
00:11:17.820 --> 00:11:20.230
Here we would take the derivative of this function

243
00:11:20.230 --> 00:11:21.530
that we're trying to minimize

244
00:11:21.530 --> 00:11:24.130
in order to figure out how we should update the weights

245
00:11:24.130 --> 00:11:26.240
in order to optimize.

246
00:11:26.240 --> 00:11:29.020
The problem is that this function

247
00:11:29.020 --> 00:11:32.720
which is called the hinge loss is not differentiable.

248
00:11:32.720 --> 00:11:34.340
If something's not differentiable

249
00:11:34.340 --> 00:11:36.520
we can't take the derivative of it.

250
00:11:36.520 --> 00:11:39.190
Therefore, we can't find the gradient of the function.

251
00:11:39.190 --> 00:11:40.920
Let's plot this hinge loss.

252
00:11:40.920 --> 00:11:41.850
Now that we've plotted

253
00:11:41.850 --> 00:11:44.627
we can see that there's a sharp point here at one.

254
00:11:44.627 --> 00:11:47.430
And this sharp point is not differentiable.

255
00:11:47.430 --> 00:11:48.440
Now, fortunately for us,

256
00:11:48.440 --> 00:11:50.700
there is a technique called Sub-gradient Descent,

257
00:11:50.700 --> 00:11:51.760
which in particular

258
00:11:51.760 --> 00:11:55.010
we can use Pegasus for soft margin SVMs,

259
00:11:55.010 --> 00:11:58.460
which can solve or optimize this equation

260
00:11:58.460 --> 00:11:59.780
with gradient descent.

261
00:11:59.780 --> 00:12:03.240
One benefit with this is that we get a guaranteed minimum

262
00:12:03.240 --> 00:12:05.270
just like we got with logistic regression

263
00:12:05.270 --> 00:12:09.120
which guaranteed that the loss function was convex.

264
00:12:09.120 --> 00:12:11.590
Now we would update the weights for this equation

265
00:12:11.590 --> 00:12:14.760
in the same exact way we would for gradient descent.

266
00:12:14.760 --> 00:12:16.760
In logistic regression,

267
00:12:16.760 --> 00:12:17.960
we would find the gradient

268
00:12:17.960 --> 00:12:21.010
or the sub gradient here through the Pegasus algorithm.

269
00:12:21.010 --> 00:12:23.280
And then we would update the weights W

270
00:12:23.280 --> 00:12:26.350
in the opposite directions of the gradient.

271
00:12:26.350 --> 00:12:28.970
So in total, we've learned that soft margin SVMs

272
00:12:28.970 --> 00:12:30.450
allow us to fit data,

273
00:12:30.450 --> 00:12:33.720
that is either one, not completely separable

274
00:12:33.720 --> 00:12:35.510
in which a hard margin SVM

275
00:12:35.510 --> 00:12:38.010
wouldn't be able to find a solution.

276
00:12:38.010 --> 00:12:39.860
And two it allows us to fit data

277
00:12:39.860 --> 00:12:42.340
that would have been fragile for a hard margin SVM

278
00:12:42.340 --> 00:12:45.330
such as data points with outliers on them.

279
00:12:45.330 --> 00:12:48.060
Finally, we can use the C term here to tune

280
00:12:48.060 --> 00:12:50.910
how much we want to penalize points lying

281
00:12:50.910 --> 00:12:53.370
either inside of the margins or

282
00:12:53.370 --> 00:12:55.320
complete mis-classifications.

283
00:12:55.320 --> 00:12:56.920
I recommend you use cross validation

284
00:12:56.920 --> 00:12:59.660
to figure out the best value for C.

285
00:12:59.660 --> 00:13:01.210
So the same question arises

286
00:13:01.210 --> 00:13:04.200
that came up when we were going over logistic regression

287
00:13:04.200 --> 00:13:06.150
which was how can we separate data

288
00:13:06.150 --> 00:13:08.730
that can't be separated with a line.

289
00:13:08.730 --> 00:13:11.470
Even here, if we used a soft margin SVM

290
00:13:11.470 --> 00:13:14.300
it wouldn't do a particularly great job

291
00:13:14.300 --> 00:13:16.280
at separating these into classes.

292
00:13:16.280 --> 00:13:18.760
So we can do a similar thing that we did for

293
00:13:18.760 --> 00:13:21.440
logistic regression and linear regression,

294
00:13:21.440 --> 00:13:23.500
such that we can project the data out

295
00:13:23.500 --> 00:13:25.410
into additional dimensions.

296
00:13:25.410 --> 00:13:28.410
Primarily we can multiply elements by themselves

297
00:13:28.410 --> 00:13:30.500
or have feature interaction terms.

298
00:13:30.500 --> 00:13:33.130
Here we're going to keep our original features.

299
00:13:33.130 --> 00:13:35.290
And then we're also going to add a third feature

300
00:13:35.290 --> 00:13:38.750
which is an interaction term between X1 and X2.

301
00:13:38.750 --> 00:13:41.410
So what is this actually doing to our data?

302
00:13:41.410 --> 00:13:43.800
Well, it's taking our two dimensional data

303
00:13:43.800 --> 00:13:47.200
and it's actually projecting it out into a third dimension.

304
00:13:47.200 --> 00:13:50.940
So here we had our X2 and X1, which we just saw here.

305
00:13:50.940 --> 00:13:54.290
And now that we've added this additional interaction term

306
00:13:54.290 --> 00:13:56.710
we have a third dimension that we can look at.

307
00:13:56.710 --> 00:14:00.290
And here, the third dimension is just X1 times X2

308
00:14:00.290 --> 00:14:04.793
and all of a sudden our data now becomes linearly separable.

309
00:14:05.660 --> 00:14:07.040
We won't use a line here

310
00:14:07.040 --> 00:14:10.570
because a line doesn't exist in these three dimensions.

311
00:14:10.570 --> 00:14:13.870
So instead we use a plane or a hyperplane

312
00:14:13.870 --> 00:14:18.270
in higher dimensions to literally separate this data out.

313
00:14:18.270 --> 00:14:20.020
Now, if we take our hyperplane

314
00:14:20.020 --> 00:14:23.170
and reduce it down to two dimensions like we had before

315
00:14:23.170 --> 00:14:25.470
we'll actually come out with this.

316
00:14:25.470 --> 00:14:27.360
So we can actually make our data

317
00:14:27.360 --> 00:14:30.670
be separated by line in higher dimensions.

318
00:14:30.670 --> 00:14:31.850
And then when we bring it down

319
00:14:31.850 --> 00:14:34.590
to the regular amount of dimensions that we had before

320
00:14:34.590 --> 00:14:37.240
it will appear to be not a line.

321
00:14:37.240 --> 00:14:41.460
Now this could work well, but what if we had 100 features?

322
00:14:41.460 --> 00:14:44.630
What we'd have to do is generate an interaction term

323
00:14:44.630 --> 00:14:47.353
across every single of the 100 features.

324
00:14:48.280 --> 00:14:50.010
The total number of features here

325
00:14:50.010 --> 00:14:51.770
would be absolutely ginormous

326
00:14:51.770 --> 00:14:55.583
which means our dimensions would be extremely high too.

327
00:14:56.720 --> 00:14:59.970
So we're basically taking something that is of

328
00:14:59.970 --> 00:15:02.150
maybe 100 dimensions.

329
00:15:02.150 --> 00:15:05.530
We're projecting it into potentially thousands of dimensions

330
00:15:05.530 --> 00:15:08.350
in order to capture all of these interaction terms.

331
00:15:08.350 --> 00:15:09.640
And then we're turning around

332
00:15:09.640 --> 00:15:11.870
and taking the dot product of it,

333
00:15:11.870 --> 00:15:15.460
which will result in a single scalar value.

334
00:15:15.460 --> 00:15:17.590
Now, the irony here is that we have

335
00:15:17.590 --> 00:15:19.410
some features and some dimension

336
00:15:19.410 --> 00:15:22.690
we're projecting it into a massive number of dimensions.

337
00:15:22.690 --> 00:15:25.470
And then we really just out of those massive dimensions,

338
00:15:25.470 --> 00:15:27.550
one, a single number.

339
00:15:27.550 --> 00:15:31.510
So we can do something about this called the Kernel Trick.

340
00:15:31.510 --> 00:15:34.820
What the kernel trick does is it allows us to avoid

341
00:15:34.820 --> 00:15:37.780
transforming all of our features into these

342
00:15:37.780 --> 00:15:39.247
larger dimensions.

343
00:15:39.247 --> 00:15:42.760
And it allows us to still extract that dot product

344
00:15:42.760 --> 00:15:46.090
without actually performing those feature transformations.

345
00:15:46.090 --> 00:15:49.470
The only caveat is that we can't use the kernel trick

346
00:15:49.470 --> 00:15:52.840
with the SVM in its primal form.

347
00:15:52.840 --> 00:15:57.673
So we have to find another form called the duel of the SVM.

348
00:15:58.890 --> 00:16:02.950
Now I'm intentionally going to breeze over some derivations.

349
00:16:02.950 --> 00:16:04.990
In my opinion, what's most important here

350
00:16:04.990 --> 00:16:07.620
is that we understand why the Colonel trick exists

351
00:16:07.620 --> 00:16:09.900
and what exactly it does for us.

352
00:16:09.900 --> 00:16:12.540
So we can use the Representer Theorem

353
00:16:12.540 --> 00:16:16.410
to represent our weights of our SVM by this equation.

354
00:16:16.410 --> 00:16:20.360
Here ei is one when a particular Xi

355
00:16:21.220 --> 00:16:25.620
and Yi pair is a support vector it's zero, otherwise.

356
00:16:25.620 --> 00:16:28.040
All right, so now you can take what's called the

357
00:16:28.040 --> 00:16:30.390
Legrand Jia Dual of the SVM

358
00:16:30.390 --> 00:16:35.390
and this is the new optimization form to represent our SVM.

359
00:16:35.410 --> 00:16:37.180
So in the primal form

360
00:16:37.180 --> 00:16:39.750
we had W transpose X.

361
00:16:39.750 --> 00:16:43.350
In the dual, we now have X transpose X.

362
00:16:43.350 --> 00:16:46.550
What this does is it benefits us in two ways.

363
00:16:46.550 --> 00:16:48.210
One way is that

364
00:16:48.210 --> 00:16:51.500
if the number of examples that you're trying to classify

365
00:16:51.500 --> 00:16:52.800
is far less

366
00:16:52.800 --> 00:16:56.930
than the number of dimensions that each example has.

367
00:16:56.930 --> 00:17:00.120
Then this computation will be a lot more efficient

368
00:17:00.120 --> 00:17:02.560
than computing W transpose X.

369
00:17:02.560 --> 00:17:05.800
Another benefit is that now that we have this form

370
00:17:05.800 --> 00:17:07.540
we can apply the Kernel trick.

371
00:17:07.540 --> 00:17:08.760
What this does for us

372
00:17:08.760 --> 00:17:11.640
is that we can have some dimension of X

373
00:17:11.640 --> 00:17:13.340
this function and the Kernel trick

374
00:17:13.340 --> 00:17:17.130
allows us to calculate the high dimensional dot product.

375
00:17:17.130 --> 00:17:20.780
And it will return to us a single scalar value.

376
00:17:20.780 --> 00:17:22.340
This is a massive savings

377
00:17:22.340 --> 00:17:24.730
when the number of examples that we have

378
00:17:24.730 --> 00:17:27.570
is far less than the dimensions that we want to

379
00:17:27.570 --> 00:17:29.420
project our data into.

380
00:17:29.420 --> 00:17:32.040
This function here could be the linear function

381
00:17:32.040 --> 00:17:33.320
a polynomial function

382
00:17:33.320 --> 00:17:34.770
even a gaussian.

383
00:17:34.770 --> 00:17:35.740
Speaking of gaussian,

384
00:17:35.740 --> 00:17:38.700
an extremely popular kernel function to use

385
00:17:38.700 --> 00:17:43.360
is called the RBF Kernel or the Radial Basis Function.

386
00:17:43.360 --> 00:17:46.190
What it does is it represents a separate dimension

387
00:17:46.190 --> 00:17:48.150
per data point that you have.

388
00:17:48.150 --> 00:17:50.150
So if you had 100 data points

389
00:17:50.150 --> 00:17:54.430
the RBF Kernel would represent your data in 100 dimensions.

390
00:17:54.430 --> 00:17:56.090
Why is this useful?

391
00:17:56.090 --> 00:17:59.270
Well, let's say that we had these series of points here

392
00:17:59.270 --> 00:18:01.730
and we wanted to find a line

393
00:18:01.730 --> 00:18:03.890
that could separate each of these points.

394
00:18:03.890 --> 00:18:06.480
Well, you would soon find this very difficult to do,

395
00:18:06.480 --> 00:18:09.960
this data is far from linearly separable.

396
00:18:09.960 --> 00:18:12.130
What the Radial Basis Function does

397
00:18:12.130 --> 00:18:14.410
is it assigns every one of your data points,

398
00:18:14.410 --> 00:18:16.090
a gaussian distribution,

399
00:18:16.090 --> 00:18:17.300
could have a different height

400
00:18:17.300 --> 00:18:18.750
or a different width.

401
00:18:18.750 --> 00:18:21.540
Then what it does is it traces a line

402
00:18:21.540 --> 00:18:25.010
of the sum of these gaussian distributions.

403
00:18:25.010 --> 00:18:26.370
Why does it do this?

404
00:18:26.370 --> 00:18:29.450
Well, it does it so it can project that point

405
00:18:29.450 --> 00:18:33.350
onto that line within its gaussian distribution.

406
00:18:33.350 --> 00:18:36.980
Now, what we have is linearly separable data.

407
00:18:36.980 --> 00:18:40.590
That means when we project our data back to one dimension

408
00:18:40.590 --> 00:18:43.100
from the five dimensions that we had,

409
00:18:43.100 --> 00:18:48.100
we'll get one line of separation in between each data point.

410
00:18:48.200 --> 00:18:49.340
For your reference,

411
00:18:49.340 --> 00:18:53.320
this is the Radial Basis Function Kernel equation.

412
00:18:53.320 --> 00:18:56.800
This Sigma here is a tuneable parameter.

413
00:18:56.800 --> 00:18:59.270
If the Sigma is too small

414
00:18:59.270 --> 00:19:01.090
then you're going to be over-fitting.

415
00:19:01.090 --> 00:19:05.100
If Sigma is too large, you could be under fitting.

416
00:19:05.100 --> 00:19:07.960
In fact, if this parameter gets too large

417
00:19:07.960 --> 00:19:10.850
it actually approaches a linear SVM.

418
00:19:10.850 --> 00:19:11.870
One thing to note here

419
00:19:11.870 --> 00:19:14.950
is that the kernel trick is not only for SVMs.

420
00:19:14.950 --> 00:19:17.390
For instance, we can take the Legrand Jia duel

421
00:19:17.390 --> 00:19:21.600
of linear regression, and we can get our X terms together.

422
00:19:21.600 --> 00:19:22.710
And that means that we could use

423
00:19:22.710 --> 00:19:24.750
the kernel trick here as well.

424
00:19:24.750 --> 00:19:26.340
If you have many more data points

425
00:19:26.340 --> 00:19:27.860
than features per data point

426
00:19:27.860 --> 00:19:30.980
or less features than you want to project into,

427
00:19:30.980 --> 00:19:33.220
this could be prohibitively expensive

428
00:19:33.220 --> 00:19:36.090
due to the computation required to get the dot product

429
00:19:36.090 --> 00:19:38.620
of all of your data points.

430
00:19:38.620 --> 00:19:42.780
So what about predicting multiple classes within SVM?

431
00:19:42.780 --> 00:19:46.850
One way to do this is to create an SVM per class.

432
00:19:46.850 --> 00:19:49.080
The idea is to be able to classify

433
00:19:49.080 --> 00:19:52.250
between a particular class and every other class.

434
00:19:52.250 --> 00:19:53.420
So in this example,

435
00:19:53.420 --> 00:19:55.670
both of these would be treated as the same class,

436
00:19:55.670 --> 00:19:58.170
because they're simply not this class.

437
00:19:58.170 --> 00:20:01.230
This paradigm is called a One verse Rest.

438
00:20:01.230 --> 00:20:04.520
So SVM one would train on this class.

439
00:20:04.520 --> 00:20:06.830
SVM two would train on this class

440
00:20:06.830 --> 00:20:08.900
and the green class would be over here.

441
00:20:08.900 --> 00:20:11.930
Finally, this third SVM, which train on this class

442
00:20:11.930 --> 00:20:14.930
and then the red and the green class would be over here.

443
00:20:14.930 --> 00:20:16.600
So to choose a prediction

444
00:20:16.600 --> 00:20:19.120
what we do is we plug in this example

445
00:20:19.120 --> 00:20:20.960
for every single SVM

446
00:20:20.960 --> 00:20:24.940
and we measure the margin it produces between the classes.

447
00:20:24.940 --> 00:20:28.400
We predict the class that produces the largest margin

448
00:20:28.400 --> 00:20:30.890
between the example and the other class.

449
00:20:30.890 --> 00:20:33.400
So in this case, we would select this example

450
00:20:33.400 --> 00:20:37.530
to be in the green class, since it formed a larger margin

451
00:20:37.530 --> 00:20:39.730
between itself and the other classes.

452
00:20:39.730 --> 00:20:42.160
Another way to handle multiple classes

453
00:20:42.160 --> 00:20:44.920
is to create a one versus one paradigm.

454
00:20:44.920 --> 00:20:48.710
In this way what we're doing is creating a pair-wise SVM.

455
00:20:48.710 --> 00:20:50.810
So for every single pair of classes

456
00:20:50.810 --> 00:20:53.420
we're creating a single SVM.

457
00:20:53.420 --> 00:20:56.830
Be careful because as the number of classes grow

458
00:20:56.830 --> 00:20:58.730
the number of SVMs required

459
00:20:58.730 --> 00:21:02.580
to create a one verse one paradigm also grows.

460
00:21:02.580 --> 00:21:05.710
To get a prediction from the one verse one SVMs,

461
00:21:05.710 --> 00:21:08.270
we'd simply feed the unseen example

462
00:21:08.270 --> 00:21:10.400
through every single SVM

463
00:21:10.400 --> 00:21:13.940
and select whichever class was most often predicted

464
00:21:13.940 --> 00:21:15.380
for that example.

465
00:21:15.380 --> 00:21:16.300
One thing to note

466
00:21:16.300 --> 00:21:18.770
is that even though the one first one paradigm

467
00:21:18.770 --> 00:21:21.380
requires a lot of SVMs,

468
00:21:21.380 --> 00:21:26.200
the data required per each SVM is only two classes.

469
00:21:26.200 --> 00:21:29.790
So this can actually be faster in some cases

470
00:21:29.790 --> 00:21:31.670
depending on your data.

471
00:21:31.670 --> 00:21:34.540
And finally a more elegant way to

472
00:21:34.540 --> 00:21:37.370
perform multi-class in terms of SVMs

473
00:21:37.370 --> 00:21:40.430
is to use something called a Structured SVM.

474
00:21:40.430 --> 00:21:43.740
Here instead of the margin being negative one and one,

475
00:21:43.740 --> 00:21:45.570
the margin is actually the distance

476
00:21:45.570 --> 00:21:47.490
between the two closest classes.

477
00:21:47.490 --> 00:21:51.160
So here and here would be the margin.

478
00:21:51.160 --> 00:21:55.263
Lastly, though not common SVMs can be used for regression.

479
00:21:56.120 --> 00:21:59.700
The idea is very similar to the SVM classifier.

480
00:21:59.700 --> 00:22:01.950
The only difference is that the goal this time

481
00:22:01.950 --> 00:22:04.513
is to keep all of the points within the margin.

482
00:22:05.380 --> 00:22:06.560
The slack variables

483
00:22:06.560 --> 00:22:08.510
or the ei terms here

484
00:22:08.510 --> 00:22:10.910
come from points that lie outside of the margin.

485
00:22:11.880 --> 00:22:14.020
The key takeaway is that SVMs

486
00:22:14.020 --> 00:22:17.960
can linearly separate separable data out of the box.

487
00:22:17.960 --> 00:22:21.350
This here is called a Hard-margin SVM.

488
00:22:21.350 --> 00:22:22.840
If we add slack variables

489
00:22:22.840 --> 00:22:26.470
which allows some examples to fall, either within the margin

490
00:22:26.470 --> 00:22:29.170
or even outside of the margins,

491
00:22:29.170 --> 00:22:32.220
this is now called a Soft-Margin SVM.

492
00:22:32.220 --> 00:22:35.040
We can use something called sub gradient descent

493
00:22:35.040 --> 00:22:38.020
to optimize a soft Marchant SVM.

494
00:22:38.020 --> 00:22:40.170
And we can't use regular gradient descent

495
00:22:40.170 --> 00:22:41.353
because an element within the function

496
00:22:41.353 --> 00:22:44.740
that we're trying to optimize is not differentiable.

497
00:22:44.740 --> 00:22:46.480
As with logistic regression

498
00:22:46.480 --> 00:22:49.990
we can add feature interaction terms to separate data

499
00:22:49.990 --> 00:22:52.950
even if it's not linearly separable.

500
00:22:52.950 --> 00:22:55.520
This is often preferred when you have either

501
00:22:55.520 --> 00:22:58.700
a low number of dimensions that you wanna project into

502
00:22:58.700 --> 00:23:01.360
or if your data is extremely large.

503
00:23:01.360 --> 00:23:03.630
However, if your data isn't so large

504
00:23:03.630 --> 00:23:06.170
and you want to project your features into

505
00:23:06.170 --> 00:23:07.970
a very high dimensional space,

506
00:23:07.970 --> 00:23:10.300
you can use something called the Kernel trick,

507
00:23:10.300 --> 00:23:11.860
which allows us to avoid

508
00:23:11.860 --> 00:23:15.130
computing this actual feature transformation.

509
00:23:15.130 --> 00:23:18.160
Keep in mind that SVMs are distance based.

510
00:23:18.160 --> 00:23:21.040
So we have to consider scaling our features as well.

511
00:23:21.040 --> 00:23:22.780
You might be wondering why would we use this

512
00:23:22.780 --> 00:23:24.500
over logistic regression?

513
00:23:24.500 --> 00:23:27.780
Well, one, if you have a low number of examples

514
00:23:27.780 --> 00:23:29.470
just start with a linear SVM

515
00:23:29.470 --> 00:23:32.060
because they only focus on the support vectors.

516
00:23:32.060 --> 00:23:35.320
Now, if you have a ton of data points and not many features

517
00:23:35.320 --> 00:23:36.530
then you might be better off

518
00:23:36.530 --> 00:23:38.460
starting with logistic regression.

519
00:23:38.460 --> 00:23:41.050
This is a generalization and should be taken lightly

520
00:23:41.050 --> 00:23:42.850
at the end of the day, cross validate

521
00:23:42.850 --> 00:23:45.910
and really make sure which model is best you.

522
00:23:45.910 --> 00:23:48.500
In the specific case of the power outages

523
00:23:48.500 --> 00:23:51.550
I went ahead and stuck with the logistic regression model

524
00:23:51.550 --> 00:23:54.790
so that we could keep the interpretability that comes along

525
00:23:54.790 --> 00:23:56.900
with the logistic regression coefficients.

526
00:23:56.900 --> 00:23:58.160
That's all for this video.

527
00:23:58.160 --> 00:23:59.480
Thanks so much for joining.

528
00:23:59.480 --> 00:24:00.920
Join us in the next video

529
00:24:00.920 --> 00:24:03.193
as we continue our machine learning journey.

