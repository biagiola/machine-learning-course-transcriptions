WEBVTT <v Lecturer>Welcome back everybody.</v> This is ML Experts machine learning crash course. In this episode, we're gonna be talking about learning to rank. In the recommendation video, we talked about how to recommend an item such as content or a product to a user. However, it only considered one item at a time per prediction. This prediction which was a score or probability, helped us gauge whether or not we should send a push notification alerting the user of some particular content. Let's say instead, we wanted to create a feed or a series of posts for a user to scroll through on their home screen. This is no longer a question of should we recommend a single item or not, this is now a question of what order or ranking do we place a series of items in to provide the best feed for a user. This is quite a different problem from classification or regression. So let's start over and frame our problem appropriately. So our first goal is to extract relevant posts from all of the posts available to the user. This is typically called candidate generation or obtaining the Top K. So here we have all of our posts available and we selected three out of them to get the top three posts. Once we have these top three posts, then we will rank them in some particular order for the feed. So how do we actually perform this candidate generation? Well, it goes back to matrix factorization. Let's review that now. We have some user item matrix. Here the items or just posts. We'll expand this U into this, which has some dimension, here it's two. We talked about in the recommendation video how this can be a hyper parameter to tune. Let's also expand this P. Now the idea is that if we multiply these two together, it will give us an approximation of this matrix. Remember that these two were found via alternating these squares. All right, so let's look at what these are. They are actually embeddings. We've talked about embedding layers before but in this case it's not an embedding layer, it's just an embedding. So what do these embeddings actually mean? These embeddings are understood to represent some characteristics about some user or some post. In the example of a post, this value could be the amount that the post aligns with some political content. This value could be some alignment with a particular company and the same could go for the users. So what we can do is plot these two dimensional embeddings into a graph. So here we have our post three, post one, post two. We also have our user one, user two, user three and they all exist in the same space. Again, to get this point, we just plotted the first and second value of the user embedding for user one. We did the same thing for the posts. Okay, so how do we get our Top K from this? Well, let's say that we want to generate Top K for user one. Well, what we can do is get the closest adjacent posts to this user. By closest, we can use Euclidean distance, we can use cosine similarity and we can even use the dot product. So here our Top K posts or our candidates will be post one and post three. Let's say that we didn't use the matrix factorization method and instead we used the deep recommendation system. Well, here we had embedding layers still. So we can take the outputs of these embedding layers and we can plot those as well and do the same exact thing in terms of candidate generation. Okay, so now we have generated the Top K via embeddings in a neural network or embeddings in the matrix factorization. The next step is to rank those Top K. So here we have extracted the Top K post from all of the available posts and now we need to rank these top candidates. Now, intuitively you may be asking, well what we could do is just order or rank these documents by their distance away from a particular user. So here since post one is closest to user one, we would put post one first in the ranking, post three would go second and post two would go third. So there's a couple of problems with this. One is that there could be tons of users in post in this space when in reality, if we extract the Top K, we could generate a more sophisticated model because now we're only dealing with a small fraction of the users and the posts. Two, what if we had different systems generating our candidates? We could subset our users into groups. For instance, one group of users could be employed at Amazon, another group of users could be employed by Google and these would exist in separate embedding spaces. However, if we want to do applied rankings of posts across these different embedding spaces, then we would have to have some mechanism to rank beyond just the distances, because those wouldn't translate across different embeddings. We need a better solution here. So let's just label these documents. And our goal is to figure out why this particular ranking is better than this ranking. One way we can tell is by clicks. So let's say the user clicked on this post. They passed this post their feed and they clicked on this one. Given this particular ranking and these clicks, I would say that a better ranking would have been this. It would have been to invert A and C such that B and A were on top. I only say this because B got clicked, A got clicked, and C didn't, so we should put C below. I would call this a better ranking than the other one. So let's explore this more. We've generated our Top K through embeddings or some matrix factorization embedding. Then with those Top K, we can perform a ranking. So let's say that we have A, B and C here. Given this ideal ranking, which we decided on because they clicked this one and clicked that one but not that one, we could train a model to output some ranking and we could penalize that model based on how many inversions it took to restore the ideal ordering. This is called learning to rank. Learning to rank takes some probability that some post i should appear before some post j. It's modeled as a sigmoid here of Si and Sj. Si is going to be the output for some particular user and some document i, here document and post are synonymous. And Sj is the output of some function given a particular user and document j. These functions here are actually just neural networks. By the way these users, which we're using as user features, they don't have to be users, they can also be queries. In fact originally, learning to rank, was used as a search optimization tool. In our case though, we're going to treat queries as users. This probability here will also reference as y hat ij. So this is the predicted probability that i should outrank j. So now let's look at the loss function. The loss is the same exact loss that we had before which is the negative log loss, and we're also going to be using gradient descent to minimize this loss. Let's look at an example. Let's say that we have these documents A, B, C. The first step is to generate all unique payers of these documents because remember our loss function takes into account the probability that document i outranks document j. So here we have to represent all ij pairs. So now that we have these pairwise posts, let's go ahead and look at what the user clicked on when we presented A, C, B. The user clicked on a A, past over C and then clicked on B. So our label or what we want the result of our learning to rank model to output is A, B, C. That's simply because they clicked on A and B, but they didn't click on C. So now what we do is we assign y hat ij that we saw earlier. So here, this should be one because what that means is that A should rank over B and we see that in our label. This could be one because B should rank over C, which is what we see. And then C to A pairwise should be zero because C does not rank over A. As a side note, if we had more documents over here that were below the click documents, we would assign them to be .5. This is because we don't know if they would have clicked it or not, they may have not even seen it. All right, so we have our labels and we have our pairwise inputs. So let's look at our neural network. Here we have our pairwise documents. Here we have our user input node, which is just summarized because you can actually take on any dimension. D here is for document, which will accept our document in and that can also take on any sort of dimension that will feed into two fully connected layers here, we'll end with a linear activation and that will produce our output. So first we apply A to this document while keeping the user constant. That will give S1. We then put B through this document while holding the user constant. That will give us S2. Then we take those S1 and S2s, 'cause here our function was that neural network and we've put in our user and our document i document j here, that was A and B. And we got our results, .32 and .85. We will substitute those into this equation and then this value will go into our loss function. We will take the gradient of that and update that step. This is extremely similar to what we saw in our neural network video. The only difference here is that we're placing adjacent documents into the neural network one at a time and we're applying those differences and outputs to the overall loss function instead of using two documents at once as an input. This strategy, which is called RankNet, was designed by Microsoft. So now let's say that we've trained our neural network with these weight updates here and we get in new documents that we haven't seen. So then we first generate the pairwise documents. We then repeat the same process, except this time with the S1s and the S2s, we just simply plug them into this and we get out a probability. If that probability is greater than 50%, then we should rank item one above item two. Then we just go to the next pair of documents, so we'll start with E. We get S2 again. We go to F and we get S3 and we simply plug this into the equation again to get new probabilities. If we continue this for all of the pairwise documents, we will come out with a consistent order of which we should place the documents in. And by consistent I mean that y hat ij or the probability that i outranks j, will propagate consistency across the entire rank. So here in training, if the probability that document one outranks document two is .5, and remember we use .5 for when the item wasn't clicked below items that were clicked. So potentially the user didn't even see them. And then if the probability that document two outranks document three is also .5, then the way that these probabilities are defined, it will consistently state that the probability of one outranking three will also be .5. So here, complete uncertainty propagates. This consistency also holds for complete certainty. So how can we improve this model? Well one, RankNet pairwise may not be the best penalty. All this means is that we may want to consider more than just two documents at a time when trying to rank all of the documents. Two, it's pretty inefficient. For all pairs of documents, let's say that we want it to rank 10 or 100 documents, we have to put all of those pairs into the neural network and performs stochastic gradient descent on every one. So in terms of learning to rank, after RankNet, there came something called LambdaNet. Two improvements came with that. One was that it was more efficient. They factorized the gradient such that they could find the gradient update for a document in comparison to all the others without having to evaluate itself versus every other pair. Next, LambdaNet enabled us to use better metrics such as nDCG for evaluating a particular ranking. Here, normalized discounted cumulative gain considers more than just a pair of documents at a time in a particular ranking. Let's go ahead and see how that works. So here we have our documents and here are the probabilities or relevances of each document. Remember one means they clicked it, zero means they didn't and .5 means the document was below the clicked documents so we don't know if they saw it or not. Here, if we just use the number of inversions, we would count one inversion here. This would give us the optimal ranking. So the model would be penalized by one. However here, let's say that this document A and B were actually just one down and C was on top. The problem is that the same penalty would be incurred for an inversion down here, versus if it was up higher. So nDCG or normalized discounted cumulative gain, considers documents up to some position P. So for instance, let's say that we had these documents here again and we wanted to measure the DCG at a particular P. Here we'll just make P three, since we only have three documents. So this is the equation for DCG. Relevance at i is just this value and i is just whatever document that we're on. If we calculate the DCG here, we get 1.04. However, if we calculate the DCG of this ranking here, we actually get .76. What that means is that even though inversions would penalize this the same, this DCG is clearly lower than this DCG, simply because these elements appeared later down the list. So the last thing that we'd like to do is to be able to compare DCGs across different users. And by that all we have to do is take the DCG for a particular user and divide by the ideal DCG or the ideal ranking. So here, if this was our input ranking, we've already calculated this DCG is 1.04. The ideal ranking would look like this though and that DCG would be this value here. This is the nDGC or normalized discounted cumulative gain for our ranking. What this does is it allows us to use the same loss for all users. The only problem with incorporating nDCG into our loss function is that it's not differentiable. So what LambdaNet did was just cheat a little bit. Instead of defining the gradient as the gradient of the loss function itself, they just assigned the gradient to a value called lambda which is defined by this here. At the time, they didn't really have any mathematical backing for this but it was later proven that this was completely fine and it actually does optimize for nDCG. So now that we're using this lambda instead of a gradient, how do we update our weights? Well, it's extremely similar to our old update functions. The only difference is that we're subtracting these terms because we wanna find the difference between this particular document and that particular document. You'll notice here that there is no partial derivative with respect to some cost function. Now it's just the partial derivative of Si, with respect to Wk and Sj with respect to Wk. So after LambdaNet, there was another iteration called LambdaMART. The only difference is that it uses a gradient boosted tree in place of the neural network. Now LambdaMART does appear to be pairwise because it compares pairwise documents, but nDGC considers elements beyond the pairs, so it's technically not a pairwise model. So why is it called gradient boosted trees? Now it's called gradient boosted trees because the regression trees that we looked at earlier, the carts, each of those leaves were evaluated by the mean squared error. If you need to refresh, go ahead and look at the decision tree video. Now, the gradient of the mean squared error is just the difference between y hat and y. And those are exactly the errors that we talked about being propagated and learnt on into the next tree. So here, a boosted tree by using mean squared error and just propagating those differences between the prediction and what the true value was into the next tree, you're technically performing gradient boosting just from the fact that the gradient of the mean squared error is simply that difference that we were using to pass to the next tree. Now, in addition to the carts that we learned, typically you'll have some regularization term here which will be multiplied by the error as it's passed on. This is to smooth the learning process in the same way that a learning rate is applied to a neural network. So now that we have these concepts down, let's go ahead and look at MART. So MART will take in a single example. And here, we just have the user features and the features of the post. In our case, this can be embeddings or it can be specific side features that we talked about in the recommendation video. This example will find itself at a leaf node after it traverses down. This leaf node will take on a value. So here we're gonna move this up to make a little bit of room. The error is actually going to be the prediction at the leaf node, which is yi and we're gonna divide that by w or the weight. Here, yi is actually just the lambda. Wi here is actually just the gradient of the lambda with respect to the output here at this leaf node. And yi divided by Wi is actually the error that gets passed on to the next tree to be learned. By the way these is are here because this is performed for all of the elements in the leaf node. And just for reference, the lambda divided by the derivative of the lambda, we're actually taking what's called a Newton step to figure out our error to be trained on in the next tree. So using the concept of MART with our lambda, instead of the actual gradient, we end up with a LambdaMART. So now let's move on. If you recall, we had a particular ranking derived from a set of clicks or no clicks. Well, we don't actually have to just use clicks. What we can do is say a person liked a particular post, performed no action on a particular post or commented on the post. Here, we define our entire schedule of how we map a particular action to some integer. This is called implicit relevance feedback. So this post would receive a relevance of two, this post would receive a relevance of zero and this post would receive a relevance of three. This is different from our relevances earlier that were only one, .5 and zero. This is similar to what we used in the recommendation video in terms of the implicit response. So with these feedbacks of clicks or the implicit relevance feedback used here, we can use nDCG as an evaluation metric for our ranking. This will be averaged across all users if we want to determine our model performance. We can also use mean average precision as an evaluation metric. This is binary, so we would effectively mark a document as relevant or not relevant, we would no longer use .5. For implicit relevance feedback, we could transform that to binary by arranging some cutoff, say three, in which if they liked it or anything more, then it would be labeled as relevant, else it would receive the label of zero. How we calculate mean average precision is we take the first element that we see and we divide by the number of relevant items that we've seen. So here that's one relevant item out of one total items. Then we skip any non-relevant items and expand our window to the next relevant item. So here at the next relevant item, which is C, we have now gotten two out of three relevant items and we just add these cumulative results. So here we have one because our first window had one. Our second window was this big and we had two out of three and we average those out and we get 83% mean average precision. Now, this is usually calculated for some position like nDCG at P. So you would calculate the mean average precision at three or five and this is also averaged across all users if you want to evaluate your model. Finally, you can use something called mean reciprocal rank which is for binary use cases again, and that's calculated by just one over the first relevant item. So here, this particular ranking would have one over one because the first relevant item was in the first position. Here, it would be one half, since the first relevant item was in the second position. We would average that across all of the users as well. Now, we're using users here, but again, this could be queries or search terms to be used in an information retrieval model. Now moving on, sometimes it is preferable to encode your n-ary labels, which we saw in implicit relevance feedback as something binary so that we can use those binary evaluation metrics just for another perspective. As well, all of these evaluation metrics that we talked about, can also be used in the lambda. So here we have nDCG, but we can also use MRR and we can use map. So let's finally apply this to our personalized feed application. Our first step is to generate Top K and we're going to do that from six human labeled post categories each with separate user item embeddings. The six categories are going to be coworker interactions, worker to manager interactions, manager to worker interactions and finally we're gonna have a general category, which is made up of product releases, IPO's or acquisitions and layoffs. All right, so our first step is to generate the Top K from this user item embeddings. Second, the Top K candidates from each of those generators will be ranked via LambdaMART. Finally, we have to de-bias our clicks. Our clicks are going to be susceptible to presentation and trust bias. Basically the results appearing lower in the ranking can affect one, if the post is even seen, that's presentation bias, and two even if it is seen, the fact that it occurs lower in the list could make the user not trust that result and not click on it. This could be more relevant in search engines. However, it could carry some weight in terms of our feed that we generate for the users. So how do we account for this? Well, let's look at our lambda again and now we're still using nDCG. All we have to do is divide by the biases themselves. Here, b of i or the bias of i is the probability that some item in rank i is clicked over some other relevant item that's ranked lower than it. Here, b of j is the same thing as b of i but in terms of irrelevant items. So let's look at how our model performed for application. We had a general increased app session time by over 25%. This simply means that users were spending more time in our app. As well, we decreased the feed scrolling time by 10%. This means that users were more likely to find their relevant content first, such that they didn't have to scroll as much. And we know that this content was relevant because they actually increased their session time in the app. These two signs indicate that this was a successful model. Well, that wraps it up for this video. Thanks for joining and join us next video as we continue our machine learning journey.