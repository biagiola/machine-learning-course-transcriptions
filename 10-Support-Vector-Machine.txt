WEBVTT <v Instructor>Welcome back everybody.</v> This is ML Experts Machine Learning Crash Course. Previously, we went over logistic regression. Logistic regression determined the probability of a class. For instance, no power outage or power outage, given some features. Now we used a tuneable decision boundary here 50% to determine the class in thus the decision boundary itself. What if we didn't care about calculating the exact probability of one class or another? What if we just cared that the examples were correctly classified? This is the idea behind Support Vector Machines. SVMs or Support Vector Machines, remove the concept of a decision threshold by instead selecting a decision boundary which maximizes the distance between itself and the two most difficult examples to classify. These two most difficult examples to classify are called the Support Vectors. In comparison, logistic regression found a decision boundary which minimized the negative log loss of the training examples. SVMs on the other hand only focus on the support vectors or the most difficult to classify points across the data. Now for SVMs, the decision boundary also called the hyperplane is defined by the equation zero is equal to WX minus B. This should be extremely familiar as it's the same line equation we've been using for your linear regression and also logistic regression. The values of W and B will be optimized when the distance between the decision boundary and the support vectors is maximized. You can think about support vector machines is actually making a three lines, one line for the hyperplane defined by this equation. Another line defined by this equation in which all of the positive examples, here a positive example is any example which was labeled to have a power outage. All of the positive examples lie to the right of this line and all of the negative examples or the examples where a power outage did not occur lie to the left of that line. Notice here that a negative example is defined by negative one instead of zero as it was in logistic regression. Now here, we actually have two dimensions of data to predict our label which means that this equation is actually a dot product. A dot product is going to be the first weight times the first dimension, plus the second weight times a second dimension. This is exactly how we've been representing our line equations for both linear regression and logistic regression. We're now just introducing a more compact form called the dot product. Now the distance between the negative example line and the positive example line is called the margin, and the margin is equal to two over the norm of W. Here the norm is just equal to the first weight squared plus all the other weight squared and then the square root of that, two over that number will give you the margin. We said earlier that the goal of an SVM is to maximize the distance between the decision boundary or the hyperplane and its support vectors. So here, this is one over the norm of W, this side over here is one over norm W and the total distance between the two is going to be two times that and that's where we get our margin. Now, if our goal is to maximize the distance between our decision boundary and our support vectors then technically we want to minimize the denominator here that will ensure that we have the largest margin possible. What we can't do is maximize our margin so much so that we actually end up going past the support vectors themselves. Remember, the idea was that this line will ensure that all the examples to the left of it are negative examples and this line to the right of it all of the examples will be positive labels. So if we make the margin too big then positive examples actually end up within the margin. And so do negative examples as well. This violates our rules. Mathematically, what this means is that we want to minimize our norm therefore maximizing the margin because that's two over the norm of W while also considering that our first-line equation which categorizes positive examples always produces something greater than or equal to one given that it is in fact, a positive example. We also want to enforce the fact that the negative example line will always be less than or equal to one, given that the example itself is negative. This is saying the same thing we just said mathematically instead of graphically. So here, what we want is the positive examples to be on the right side of the margin. We want the negative examples to be on the left side of the margin and how we represent that mathematically is with these two inequalities. Technically for mathematical conciseness we can combine them into a single inequality like this. All we did was now multiplied by the true label of the example and that will allow us to combine the two inequalities into one inequality. Now, this is the equation to solve for that line that best separates the support vectors here. Our goal is to maximize the margin or minimize the norm of W since that's in the denominator of the margin. The other goal is to make sure that all of our labels are on the correct side of the margin. Notice that this optimization is different from what we had to optimize for logistic regression. This problem has constraints as well as something to optimize. Logistic regression was minimizing the Cross-entropy loss here, we're minimizing the norm of the W but in the case of SVMs we also have a constraint to worry about. Gradient descent in its typical form can't perform optimization while also considering constraints. So we have to use another optimization technique which can handle constrained optimization problems. Since W is a linear term, we could use a technique called linear programming. However linear programming won't guarantee us a unique solution and it can be unstable in some cases. So typically people square this term which makes the term quadratic. This allows us to use quadratic programming which will guarantee us a unique solution. If we used quadratic programming to fit our model weights we'd have, what's called a Hard-Margin SVM. This could work with our data that we show earlier. So what if our data looks like this? Here we have a case of a cluster over here and another cluster over here of different labeled data. And in the middle we have what's called an outlier. Well with our hard margin SVM we'd get this line as the optimal solution according to our quadratic program optimization. To me it doesn't really look correct. This is because these two data points would be considered the support vectors which we talked about earlier which are the two most difficult points to classify. What we'd really like is to be able to give some slack to this Hard-Margin SVM, such that it could assign support vectors like this and ignore this outlier. To ignore this outlier, we're going to have to use something called slack. Slack allows us to relax our constraints, such that every single label of this type doesn't have to be to the left of our support vector on the left. This gives us something called a Soft Margin SVM. So now we'll have to update our optimization and our constraints to support the idea of slack. To update this constraint, We'll have to add an ei term here. What this is saying is that no longer does every single point have to exist on the correct side of the margin. Instead, we can allow some points to go within the margin. This ei is the distance between our new support vector which is here and the distance between what the old support vector would have been in the case of the hard margin SVM as well as updating the constraint, we also have to update what we want to minimize. What this term means is that we're going to sum all of the variables or the distance from the margin that these examples are. We're going to sum that up and we want to minimize it. So, yes, while we've now allowed certain points to exist within the margin. We also want to minimize the amount that we allow points in the margin. This parameter here C is for regularization. This C term indicates how much we want to penalize a particular example lying with inside the margin. Let's sum up what this ei will equal based on where an example lies either inside or outside of the margin. E of I or some ei of an example is going to be equal to zero when that example lies on the correct side of the margin. If an example lies within its margin but not over the hyperplane itself then it will incur an error between zero and one. However, if an example lies on the other side of the hyperplane and not within its own margin between it and the hyperplane then the penalty incurred will be higher than one. And it will make this term higher when we want to minimize it. So here this ei is between zero and one because it hasn't quite crossed to the hyperplane yet. If this example did cross the hyperplane the ei would be larger than one. If this point lied on the correct side of the margin then the ei would be zero. All right so now we have detailed out our balancing act. This constraint is allowing us to ei and have points with inside the margin. And this equation is trying to minimize how many errors we have incurred from points lying within or over the margin. Notice that we have matching ei here and idea is that we could solve for ei in this equation and plug it in for ei here such that we no longer have any constraints associated with our optimization problem. We can do that by solving for ei. And with the understanding that e of i is always going to be greater than or equal to zero we can actually write our inequality like this. Now we can substitute this n for this ei. We now have an optimization problem that has no constraints. An obvious question is, can we use gradient descent now? If you recall for gradient descent we had to take the derivative of the loss function. Here we would take the derivative of this function that we're trying to minimize in order to figure out how we should update the weights in order to optimize. The problem is that this function which is called the hinge loss is not differentiable. If something's not differentiable we can't take the derivative of it. Therefore, we can't find the gradient of the function. Let's plot this hinge loss. Now that we've plotted we can see that there's a sharp point here at one. And this sharp point is not differentiable. Now, fortunately for us, there is a technique called Sub-gradient Descent, which in particular we can use Pegasus for soft margin SVMs, which can solve or optimize this equation with gradient descent. One benefit with this is that we get a guaranteed minimum just like we got with logistic regression which guaranteed that the loss function was convex. Now we would update the weights for this equation in the same exact way we would for gradient descent. In logistic regression, we would find the gradient or the sub gradient here through the Pegasus algorithm. And then we would update the weights W in the opposite directions of the gradient. So in total, we've learned that soft margin SVMs allow us to fit data, that is either one, not completely separable in which a hard margin SVM wouldn't be able to find a solution. And two it allows us to fit data that would have been fragile for a hard margin SVM such as data points with outliers on them. Finally, we can use the C term here to tune how much we want to penalize points lying either inside of the margins or complete mis-classifications. I recommend you use cross validation to figure out the best value for C. So the same question arises that came up when we were going over logistic regression which was how can we separate data that can't be separated with a line. Even here, if we used a soft margin SVM it wouldn't do a particularly great job at separating these into classes. So we can do a similar thing that we did for logistic regression and linear regression, such that we can project the data out into additional dimensions. Primarily we can multiply elements by themselves or have feature interaction terms. Here we're going to keep our original features. And then we're also going to add a third feature which is an interaction term between X1 and X2. So what is this actually doing to our data? Well, it's taking our two dimensional data and it's actually projecting it out into a third dimension. So here we had our X2 and X1, which we just saw here. And now that we've added this additional interaction term we have a third dimension that we can look at. And here, the third dimension is just X1 times X2 and all of a sudden our data now becomes linearly separable. We won't use a line here because a line doesn't exist in these three dimensions. So instead we use a plane or a hyperplane in higher dimensions to literally separate this data out. Now, if we take our hyperplane and reduce it down to two dimensions like we had before we'll actually come out with this. So we can actually make our data be separated by line in higher dimensions. And then when we bring it down to the regular amount of dimensions that we had before it will appear to be not a line. Now this could work well, but what if we had 100 features? What we'd have to do is generate an interaction term across every single of the 100 features. The total number of features here would be absolutely ginormous which means our dimensions would be extremely high too. So we're basically taking something that is of maybe 100 dimensions. We're projecting it into potentially thousands of dimensions in order to capture all of these interaction terms. And then we're turning around and taking the dot product of it, which will result in a single scalar value. Now, the irony here is that we have some features and some dimension we're projecting it into a massive number of dimensions. And then we really just out of those massive dimensions, one, a single number. So we can do something about this called the Kernel Trick. What the kernel trick does is it allows us to avoid transforming all of our features into these larger dimensions. And it allows us to still extract that dot product without actually performing those feature transformations. The only caveat is that we can't use the kernel trick with the SVM in its primal form. So we have to find another form called the duel of the SVM. Now I'm intentionally going to breeze over some derivations. In my opinion, what's most important here is that we understand why the Colonel trick exists and what exactly it does for us. So we can use the Representer Theorem to represent our weights of our SVM by this equation. Here ei is one when a particular Xi and Yi pair is a support vector it's zero, otherwise. All right, so now you can take what's called the Legrand Jia Dual of the SVM and this is the new optimization form to represent our SVM. So in the primal form we had W transpose X. In the dual, we now have X transpose X. What this does is it benefits us in two ways. One way is that if the number of examples that you're trying to classify is far less than the number of dimensions that each example has. Then this computation will be a lot more efficient than computing W transpose X. Another benefit is that now that we have this form we can apply the Kernel trick. What this does for us is that we can have some dimension of X this function and the Kernel trick allows us to calculate the high dimensional dot product. And it will return to us a single scalar value. This is a massive savings when the number of examples that we have is far less than the dimensions that we want to project our data into. This function here could be the linear function a polynomial function even a gaussian. Speaking of gaussian, an extremely popular kernel function to use is called the RBF Kernel or the Radial Basis Function. What it does is it represents a separate dimension per data point that you have. So if you had 100 data points the RBF Kernel would represent your data in 100 dimensions. Why is this useful? Well, let's say that we had these series of points here and we wanted to find a line that could separate each of these points. Well, you would soon find this very difficult to do, this data is far from linearly separable. What the Radial Basis Function does is it assigns every one of your data points, a gaussian distribution, could have a different height or a different width. Then what it does is it traces a line of the sum of these gaussian distributions. Why does it do this? Well, it does it so it can project that point onto that line within its gaussian distribution. Now, what we have is linearly separable data. That means when we project our data back to one dimension from the five dimensions that we had, we'll get one line of separation in between each data point. For your reference, this is the Radial Basis Function Kernel equation. This Sigma here is a tuneable parameter. If the Sigma is too small then you're going to be over-fitting. If Sigma is too large, you could be under fitting. In fact, if this parameter gets too large it actually approaches a linear SVM. One thing to note here is that the kernel trick is not only for SVMs. For instance, we can take the Legrand Jia duel of linear regression, and we can get our X terms together. And that means that we could use the kernel trick here as well. If you have many more data points than features per data point or less features than you want to project into, this could be prohibitively expensive due to the computation required to get the dot product of all of your data points. So what about predicting multiple classes within SVM? One way to do this is to create an SVM per class. The idea is to be able to classify between a particular class and every other class. So in this example, both of these would be treated as the same class, because they're simply not this class. This paradigm is called a One verse Rest. So SVM one would train on this class. SVM two would train on this class and the green class would be over here. Finally, this third SVM, which train on this class and then the red and the green class would be over here. So to choose a prediction what we do is we plug in this example for every single SVM and we measure the margin it produces between the classes. We predict the class that produces the largest margin between the example and the other class. So in this case, we would select this example to be in the green class, since it formed a larger margin between itself and the other classes. Another way to handle multiple classes is to create a one versus one paradigm. In this way what we're doing is creating a pair-wise SVM. So for every single pair of classes we're creating a single SVM. Be careful because as the number of classes grow the number of SVMs required to create a one verse one paradigm also grows. To get a prediction from the one verse one SVMs, we'd simply feed the unseen example through every single SVM and select whichever class was most often predicted for that example. One thing to note is that even though the one first one paradigm requires a lot of SVMs, the data required per each SVM is only two classes. So this can actually be faster in some cases depending on your data. And finally a more elegant way to perform multi-class in terms of SVMs is to use something called a Structured SVM. Here instead of the margin being negative one and one, the margin is actually the distance between the two closest classes. So here and here would be the margin. Lastly, though not common SVMs can be used for regression. The idea is very similar to the SVM classifier. The only difference is that the goal this time is to keep all of the points within the margin. The slack variables or the ei terms here come from points that lie outside of the margin. The key takeaway is that SVMs can linearly separate separable data out of the box. This here is called a Hard-margin SVM. If we add slack variables which allows some examples to fall, either within the margin or even outside of the margins, this is now called a Soft-Margin SVM. We can use something called sub gradient descent to optimize a soft Marchant SVM. And we can't use regular gradient descent because an element within the function that we're trying to optimize is not differentiable. As with logistic regression we can add feature interaction terms to separate data even if it's not linearly separable. This is often preferred when you have either a low number of dimensions that you wanna project into or if your data is extremely large. However, if your data isn't so large and you want to project your features into a very high dimensional space, you can use something called the Kernel trick, which allows us to avoid computing this actual feature transformation. Keep in mind that SVMs are distance based. So we have to consider scaling our features as well. You might be wondering why would we use this over logistic regression? Well, one, if you have a low number of examples just start with a linear SVM because they only focus on the support vectors. Now, if you have a ton of data points and not many features then you might be better off starting with logistic regression. This is a generalization and should be taken lightly at the end of the day, cross validate and really make sure which model is best you. In the specific case of the power outages I went ahead and stuck with the logistic regression model so that we could keep the interpretability that comes along with the logistic regression coefficients. That's all for this video. Thanks so much for joining. Join us in the next video as we continue our machine learning journey.