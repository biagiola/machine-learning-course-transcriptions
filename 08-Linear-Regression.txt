WEBVTT <v Instructor>Welcome back.</v> This is ML Experts, machine learning crash course. Let's say that we have a plot of some points like so. If we wanted to draw a line that best summarizes the trend of this data, we could draw a line like this. This line summarizes the trend that as X1 goes up, X2 also goes up. Using this line, we can take some X1 value, which we don't know the corresponding X2 value of, and we can plug in the X1 value into this line to get an estimate of what the X2 value would be, okay. So this is the general idea behind linear regression, and basically linear regression answers the question of how do we find the line of best fit for the data? Let's say we've just been hired by California ISO, which oversees California's electric power management. They'd like to be able to predict the power demand of a particular region a day in advance so that they can ramp up supply to the region if they think demand is going to be higher. Or they can route power away from the region toward other needing regions, so that they don't end up with too much power in a particular region. Our hypothesis is that in the summer months, as the temperatures rise, so does the power demand, because people will turn on their AC. So for our features, we have the average daily regional temperature history provided by the national weather service. And for the labels, we have the daily historical power demand per region. So for a particular region, we can plot the power demand of that region, given some average daily temperature. So the idea is that we can use linear regression to find a line in which we can plug in an arbitrary temperature, to get out a predicted power demand. The line is often called a line of best fit. Now let's revisit the equation of a line. The X is our input. Here this would be average temperature. M would be the slope of a line, and B would be the Y intercept of that line. So Y is our dependent variable Because it depends on X, and X is our independent variable which will be on our X axis here. So slope is how steep the line itself is. The intercept will be the shift of the line vertically. The idea is that we would be able to plug in sum arbitrary X to get out sum output Y which would be the predicted power demand in that region for that temperature. Our challenge is to figure out what the best line equation is, which summarizes this dataset. Fortunately for us, there's a formula we can use. So if we have XY pairs of data, we can figure out the line of best fit by simply applying this formula here and got changed to A 'cause that's just commonly what you'll see. So here is any given element XI, here's any given element Y. This is just the average of X, this is just the average of Y, this is average of X. So we can plug in the averages and then we expand the summation of top. So XI, which is X1, the first one minus the average, multiplied by the first power demand, 42,000 minus the average power demand. This continues all the way through for every single pair. And then in the denominator, we can expand the summation, all right. And it's just the axis now, except we square the terms. And that leaves us with an A of 4,105. To solve for B, we just take the average Y and the average X, plug those in. We have A, because we just solved for that. So B will be negative 50,962, and this is our line of best fit. So for a few temperatures here in a given region, we've filled out more of the values here. So if we plug in our line of best fit that we solve for earlier, we'll see this line here up here. Now, what we can do is let's say that the national weather service is predicting that the temperature tomorrow on average will be 25.2 degrees Celsius. We can map that. We just putting that into the X value here in our line. We can map that to a Y value of 52,484 megawatts of power demand, okay. So that way we know a day in advance, based on the prediction of the temperature of the following day. We know generally what the power demand will be. So what does this coefficient actually mean? This coefficient means that for every one unit increase in X, we'll get 4,105 units increase in Y. So for instance, if we increase the temperature by one degree Celsius, we will get 4,105 megawatts of power demand increase. So one thing that we got to make sure of is that this coefficient just isn't disguised as noise. Let's say that if we just put a whole bunch of random points here, here, here, here, here, here, here. Technically this would still learn a line. So we have to make sure that this coefficient just isn't noise. One thing we can do is to check the P value of the coefficient. A P value is a probability that this relationship or a stronger relationship could have been observed among random data points also called the null hypothesis. The P value, even though completely arbitrary, is usually required to be less than or equal to .05. This means that there needs to be a 5% chance or less that this relationship or coefficient could have been explained by random noise. So fortunately for us, this condition is fulfilled in our case as our P value is .0009, which is less than .05. So technically we can deem this coefficient to be statistically significant since it passes our null hypothesis test. Some practitioners rely on more stringent requirements, say a P value of .001, but it's generally .05. Finally, since we've sampled the features and labels, such that we've only used the summer temperatures and then we've only used the past five years of data, the coefficient that we see is actually an estimate based on some actual coefficient that would exist if we had used all of the data in all of history and our measurements were absolutely perfect and we were absolutely generating this much power demand, things like that. So if everything was perfect, this coefficient would actually be a different value likely. To get an idea of how close this coefficient is to the actual value of the coefficient, we can use something called a confidence interval. So our confidence interval is between 3,100 and 5,000, okay, at 95%. This means there's a 95% chance that the true value of the coefficient lies in the interval between 3,100 and 5,000. So really, for every degree increase, there's a 95% chance that we'll see between a 3,100 and a 5,000 megawatt increase in power demand. Be careful that if your confidence interval contains zero, then your coefficient is not statistically significant. So this non-zero statistically-significant confidence interval coefficient implies a relationship between the temperature and the power demand. This relationship is referred to as a correlation. We can quantify correlation with the term R, which has a range of negative one to one. The negative R represents a negative correlation, and a positive R represents a positive correlation. A correlation of zero represents no correlation at all. For our data, we have a correlation of .99, which is fantastic because that means our temperature and our power demand correlate strongly. But it's important to recognize that R does not say anything about causation. You've probably heard that. And it just means exactly that R doesn't say anything about what causes what. For instance, this average temperature, we could've put it over here and put power demand over here. And then we would see, oh, as power demand rises, the average temperature goes up. So that's not what this is measuring. We would still see the same correlation coefficient there, .99. But we have to be careful in that. We're not specifying what's causing what. All right, so now that we have correlation figured out, the next step is to get R-squared value which we get by squaring R. This is a commonly used metric to determine how well the line fits the data. It's a percent, so it lies between zero and one. We can see this by taking the mean of our examples and measuring the residuals, which is the distance between the example and the mean of all the examples. And if we sum up that number and put it aside, let's say it's 100. And then we sum up the residuals compared to our line of best fit, and sum those up, let's say it's two. What R-squared represents is the explain difference we went from 100 to two. So we've explained 98% of the variance by just fitting the data to this line. What this means is that the other 2% of the variance of the data isn't explained by our temperature variable. One idea is to include more features than just temperature to see if you can get a higher R-squared value and thus explain more of the variance. Remember, even though that the goal is to get a high R-squared, it's not necessarily going to be ever 100% since there's typically noise in real-world environments. We're not gonna go too deep into it, but if you wanna know how P value and confidence intervals are derived, we're gonna visit it quickly. We can get a P value by finding what's called the standard error, the coefficient, okay. So this is just, it's a formula, all right. Then we'll take our coefficient, which is A, and divided by that standard error to get what's called a T statistic, all right. And then we plug that T statistic into our T distribution. What we'll do is we'll get a area under the T distribution from our T statistic value, and the P value is obtained by the sum of the areas under that T statistic portion of the T distribution. For confidence intervals, we take that same standard error of the coefficient, and to get a 95% confidence interval, we take the coefficient, okay, minus 1.96 times the standard error coefficient, all right, that's our low end of the range. And our range ranges from that to the high end of our range, which is the coefficient, which was our A, plus 1.96 times the standard error coefficient. R-squared on the other hand can be calculated by taking the variance of those errors or residuals, dividing by the variance of Y. And you just one minus that and that'll give you your R-squared. As well, the adjusted R-squared incorporates the R-squared itself along with the number of independent variables we use in our model. All right, so let's say our model is doing exceptionally well and we want to expand that out to different regions other than just one. However, other regions are not populated equally. And we want to represent that in our model. We can add a feature to the examples called approximate population size, okay? And that's going to be small, medium or large. We can also include the region type, right? So we can have an industrial region, commercial region or residential region. The idea is that these features are important in terms of power demand. This feature is ordinal and this feature is categorical. We can incorporate these features into our linear regression model like so. All that we did was replace alpha with betas. And so for instance, temperature is now a beta times X. Be comfortable seeing any type of letters here. Just know that you're just multiplying numbers. And here we've included our ordinal feature to designate region size. So that would be small, medium and large for zero, one and two. What this means is that X will be zero for small, one for medium and two for large. Here in order to represent categorical variables in linear regression, we're going to use one-hot encoding. So what this means is that X3 is basically all the same variable, but they're one-hot encoded such that only one of these terms will be present at any given time. This term will be present when we're dealing with an industrial example, this term will be present when we're dealing with a residential example, and this term will be present when we're dealing with a commercial example. It's typical in linear regression. We'll take the one-hot encoding and represent at least one state at zero, zero. So this is the same thing. It's just saying that this term will be here for industrial, this term will be here for residential, and neither of these terms will be here for commercial. So we just took our one-hot encoding and made sure to represent zero zero with it, so now we need one less variable. Now that we've introduced more features or independent variables to predict our label or dependent variable, we really only have to worry about one additional thing. Since the features are called independent variables, what happens if they aren't so independent? For instance, what if we decided to introduce a feature of humidity? And what if it also just happens that temperature and humidity correlate? This correlation between independent variables or features is called collinearity or multicollinearity if multiple features do it. Collinearity won't affect our performance of the model itself. So our R-squared will remain unchanged. Our model can still make effective predictions, however, the way we interpret the coefficients will have to change. For instance, if we still wanted to interpret this temperature coefficient, our assumption was that in order to interpret the temperature coefficient, and its contribution toward power demand increase, all else in the equation would have to remain the same. But since these now correlate, we can't say for sure that all of these will stay the same when we just evaluate a single degree temperature change. So at least how can we detect that variables are collinear? Well, we can look at the features VIF or variance inflation factor. VIF is derived from finding the correlation itself between certain features. A VIF of one implies no collinearity. VIF of one to five is a moderate collinearity but you still probably don't have to do any form of mitigation. And anything above five, it's going to be a severe collinearity and you'll probably want to use a mitigation strategy. One of those strategies is centering your features. It just means subtracting the average feature value from the feature itself, and that will center your features and can lower the variance inflation factor. So one interesting thing now that we can incorporate since we have multiple features, are feature interactions. If we think that the power demand of a small residential region is going to behave differently than a small industrial region, then we can model this distinction by using an interaction term. This simply means multiplying the two features together. We had to add two here because these are one-hot encoded, but really only one of these will be present at any given time after introducing the interaction term if the R-squared goes up and the P value of the interaction term is significant, then you can be reasonably confident that these terms are in fact interacting. A rather unintuitive question that arises is can we multiply a feature with itself? The answer is yes, but why would we want to? Imagine we added the winter months along with the summer months to our model. As the temperature gets colder, the power demand goes up as well because people turn on their heat. If we fit this data with our existing model, then we'd get this line. It does minimize the mean squared error, but only as much as a line can. So if we add a term where we square the temperature or multiply it by itself, then we're now able to fit a curve. This is because the equation for a parabola contains that squared term. You can actually even do cubix, where you multiply the same term by itself three times. What does that do? Well, that gives us an extra bend over here. We can even do to the fourth power which would give us another bend over here. And now our model looks like this. One thing to note is that you can put so many terms in that you over-fit your data, and it may not generalize well to the test set. But you can put a one over X or log of X to fit any range of functions. Now, the only rule is that you have to follow the pattern of the coefficient times sum function of X, coefficient times sum function of X. If you do something like this, you then have nonlinear regression, okay? Nonlinear regression is powerful. We're not going to touch on it right now, more than we already have. So now we can finally train our linear regression model to fit a curve for more than one region in which we have multiple types to features. We can use it by simply plugging in some X value to get out sum Y value. One thing to look out for when fitting data is called Simpson's paradox. For example, when we expanded our model to different regions, we included the label for the region size. So let's say our model started out in a large region. And as we expanded to a medium or small, we included that label along with the data of indicating that it was in fact from small or medium region. Now, since we did that, we actually got to learn lines like this, because we knew that we had to separate them out and learn separate lines from them based on the region. However, imagine if we didn't include the features of the region size, adding data would have looked like this, right? So we would have been existing in a large region. We would have added our medium data and our small data, and the model wouldn't have no one to differentiate them. So it would have learned a single line for this data that would have looked like this. This is extremely dangerous. What this is saying is as temperature goes up, power demand goes down, which we know is just not true. This is Simpson's paradox, it's something to look out for. And really a good way to avoid it is to add as many dimensions to our model which segment the data we're trying to predict, right? So in our case, we added these dimensions to our model, which indicated segments of our data which allowed Simpson's paradox to be avoided. So finally to wrap this up, a good linear record software package that I like is called Stats Model. It lets you conduct multiple regression, which is multiple coefficients and multiple independent variables. And then it will also give you R-squared, your P values, and there is even a path to get you your variance inflation factors as well. FYI, the close form, linear regression formula that we used to calculate those lines, it's N-cubed. So it's not actually used in practice. Libraries use another method based on a singular value decomposition, which we'll talk about later. But for now, that's it for this video. Thanks for joining, join us on our next video as well as we continue our machine learning journey.