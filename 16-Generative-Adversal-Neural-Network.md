WEBVTT <v Instructor>Welcome back everybody.</v> This is ML experts, machine learning crash course. In this episode, we're going to be going over something called generative adversarial networks. So let's continue along our adventure of using satellite imagery to monitor the distribution centers for this particular gaming company. We talked about earlier how we could use traffic camera footage for the distribution centers in the United States. However, outside of the United States we couldn't obtain that traffic camera footage. So instead we were left to using satellite imagery. Fortunately, satellite imagery has enough resolution for us to be able to recognize tractor trailers but anything smaller becomes difficult to see what it actually is. As a government mandate, the minimum that a satellite image can capture is 30 centimeters per pixel. That means if we had a two by two sub region of an image, each pixel would capture 30 centimeters. So this section here would capture 3,600 square centimeters or roughly three square feet. Now the thought is that if we can make these pictures more clear and represent perhaps, 10 square centimeters per pixel or something like that, we can identify more than just these large tractor trailers and we can expand our trading strategy to monitor other supply chains beyond just tractor trailers. Now, in reality most satellites actually won't hit that minimum mandate of 30 centimeters per pixel simply because satellites that do capture at that resolution are very expensive. So often it's even worse than 30 centimeters per pixel. So if you actually need something with higher resolution than satellite images, you can use something called aerial images. Now for aerial images, there's no legal minimum for what resolution you can capture at. Here is an example of an aerial image. You can make out more of the truck. You can make out different types of vehicles and overall the image quality is improved. So what if we could create a model which takes in satellite images and produces an equivalent aerial image. So now that we have a goal let's go ahead and settle our features. Here we'll have about 200,000 satellite images. We will capture different coastlines, ports, cities, farms, mountains, oceans, and suburbs. The labels will be equivalent aerial images corresponding to each satellite image. These aerial images will also be four times the resolution as the satellite image. So when we say four times the resolution, what we mean is that if we have an input satellite image which has a two by two subsection, we want to take that two by two subsection and turn it into a four by four subsection. So in total, there will be four times as many pixels in the target images. So how do we collect our features and labels? Well, satellite images will be used for the features so we can just contract out satellite companies to capture these images. Now for the labels, we will have to capture aerial imagery of the same exact area, preferably at the exact same time that these satellite images are taken. So what model can we actually use for this? Well, there's a type of neural network called a generative adversarial neural network or GAN. These GANs are really just two neural networks in themselves. One neural network is called the generator. Now in our case it's images, but it doesn't have to be. The generator will take in a low resolution image here that would be the satellite images. The generator would then create its best guess for what the high resolution image or the aerial image would be of the equivalent low resolution image. The second part to the GAN is called the discriminator. The discriminator will take in real aerial images and the aerial images or the aerial image estimates produced by the generator, and it will decide if the input image is real or it will guess that it's fake if it thinks that the generator produced the high resolution estimate of the image. Now let's look at how these work against each other. The generator will take in these low resolution or satellite images, it will attempt to generate equivalent aerial image estimates. The discriminator will take in high res images or the aerial images, and try to predict them as real. The generator will then send its fake images or its generated images to the discriminator in an attempt to fool the discriminator into thinking that it is a real image. So here these two neural networks are working against each other. The generator is working to confuse the discriminator and the discriminator is working its best to differentiate between the generated images and the real images. Now if the generator can produce these high resolution estimates, and the discriminator thinks that they are real, and then when we pass in a real example here of the aerial images and the discriminator thinks that it's fake, we have effectively confused this discriminator and the generator is done training. When the generator's done training, we can put in various satellite images and it will generate an estimate for what it thinks the aerial image would look like. Remember, this aerial image will be higher resolution, and in our case it will have four times the resolution as these input images. So let's go over in more depth, how this model is trained. So here we have our generator and discriminator, we'll take a low resolution or satellite image from the pool of satellite images that we have, we'll then pass that through the generator, which since we've randomly initialized it and we haven't trained that at all yet, it will likely produce a random high resolution image. And we will then pass that to the discriminator. The discriminator, since it's also just been randomly initialized, will either output a real or a fake indicator based on what it believes this image to be. We then use the loss generated from this prediction to train the weights of the discriminator. We then get a separate high resolution image from the pool of aerial images. We put that through the discriminator and since again it has just begun being trained, it could make any sort of prediction here. So here it's calling this real image fake. And then we train on that particular image and update the weights of the discriminator according to the loss generated here. So notice in our first iteration here we've only trained the weights of the discriminator on the fake images generated and the real images that we have. Now in the next iteration, after the discriminator has been trained a little bit on some real and fake images, we grab a low resolution or satellite image from the pool of satellite images, and we run that through the generator again. It will produce a pretty random image here. We'll send that fake image to the discriminator. And now since the discriminator has been trained a little bit, it will likely output that it's a fake image. And then what we do is we take the loss generated from this discriminator here, and we actually propagate that loss through to the generator. So here we're fixing or keeping constant the weights of the discriminator. And we're only updating the generator in accordance to the loss generated from its attempt to fool the discriminator. So let's go over what these losses are. The discriminator loss is just going to be the log loss. This first term here is the probability that the discriminator guesses that a real input, your X, is a real image. This over here is the probability that the discriminator correctly guesses that a generated input, here with this function G means it came from the generator and Z means that it came from our low resolution pool. When that is put through the discriminator, we also want to correctly identify that as not being a real image. So these two terms together correctly identifying a real image and correctly identifying a fake image, the discriminator wants to maximize this loss. Now the generator's loss on the other hand is going to be this. What it wants to do is to minimize the probability that the discriminator correctly guesses that an image is fake. Notice that these two terms here are the same. Now, since these are equivalent here we can actually combine these two loss functions into one. We'd get that the generator is trying to minimize this function. The discriminator is trying to maximize this function. This is called an adversarial min-max and this is where the generative adversarial network gets its name. How do we know when this loss function has converged? Well, we talked about earlier that we know the generator has done its job when it has confused the generator. To confuse the generator means that the discriminator's accuracy will drop to about 50%. This means that the discriminator is now no better than random chance. However, now the feedback in terms of the gradients from the loss of the discriminator fed back to the generator will also be no better than random. So we don't want to keep training after we hit this point. Ideally we stop the training just as the discriminator holds 50% accuracy. So now that we know about GANs, let's build one for our application. Our generator is going to be a convolutional neural network. If you need a refresher on what these are go ahead and watch the convolutional neural network video. The input will be a satellite image or low res. The output will be the estimated aerial image or the approximated high res. And finally this convolutional neural network will have to up sample to create a four times high resolution estimate. Now let's take a look at our convolutional neural network. You may notice right away that we have a lot of convolutional layers adjacent to one another. This is different from what we learned in the CNN video. That's because this generator has to take this input image. And instead of reducing the dimensions down to some binary classification, for instance, we actually want to upstate sample this 200 by 200 image to a 400 by 400 image. So let's focus on these layers. We're using a three by three kernel each time, we're using a stride of one and we're actually using same padding instead of valid padding, which means that we will get the same number of dimensions out every single time that we apply these kernels. So our 200 by 200 image is preserved across these six because layers. Now, the interesting part about this network is this right here. This is called a pixel shuffle. A pixel shuffle layer takes in some number of feature maps, here 256 at some dimension here, 200 by 200, and increases the dimension. while leaving a less number of feature maps. How does it do this? So if we have some image here and we want two times for the resolution, what we can do is generate four feature maps in the previous layers of the pixel shuffle layer. And then we can stack these feature maps on top of one another, and we can assign each of these pixels to 1/3 dimensional row there. This third dimensional row can be assigned all of these pixels. And we can carry this process on until our new four by four image has been completely filled out by the values in these feature maps. So what's going on here is that these 256 feature maps of dimension 200 by 200 are being transformed into a 64 feature maps or a 1/4 of 256. But those reduction in feature maps is being translated into larger images. So that is how we can increase the resolution of our images in our convolutional neural network. So in addition to this pixel shuffle layer here, to increase our dimensions we're also going to add residual connections which we also talked about in the CNN video. This will help us avoid the vanishing gradient because our neural network is pretty deep here. Now let's move on to the discriminator. This is no different than the previous CNNs that we've looked at in the CNN video. We start out with some image, we apply some convolutional layers, some max pooling, some more convolutional layers. We end with a flat layer which feeds into a sigmoid for binary classification. Here this is y I will be one when the input has been predicted to be a fake image and zero otherwise. The only difference is now instead of ReLUs we're actually going to be using leaky ReLUs and here the slope of this line past the left of this Y axis here is going to be equal to two. So that's our leaky ReLU parameter. The mini batch size is just 16. You'll notice that this is quite a bit smaller than our previous mini batches, especially when we're training convolutional neural networks. Remember that before we said that the discriminator goes through a few iterations of training before we allow the generator to start learning from the discriminator's decisions. Keeping these mini batches smaller means that the discriminator will only get 16 images to train on before the generator gets to jump in and begin training as well. This is so the discriminator doesn't out learn the generator, such that the discriminator doesn't supply any valuable feedback to the generator because it's already learned so much and so well that these gradients will be quite small and won't actually help the generator learn. So the generator and the discriminator actually have to learn together. Besides keeping a small amount of examples per mini batch, you can also adjust the learning rates of the discriminator or lower them to make sure that the generator can still keep up and learn together with it. Another thing that you have to watch out for when training is something called a mode collapse. Mode collapse is when the generator figures out an image which can fool the discriminator. And it just continues to output that same exact image, since it's found out how to fool it. This could be avoided by using something called an unrolled GAN, which allows the generator to see what the discriminator will look like in a few more steps ahead of it such that the generator is encouraged not to learn some local exploitation of the discriminator. It now has to account for what the discriminator will also look for in the future. Other than that, we would train the CNNs in the same way that we've seen in the past. All right, so how do we evaluate our model performance, so that if we get a low resolution satellite image that our model wasn't trained on and it generates some high resolution image, how do we gauge its performance? Well, in our test set if we have the true label or the true high res image, we can just take the mean squared air per pixel. Another thing we can look at is called the peak signal to noise ratio. This is a gauge of how noisy the produced images. The smaller the mean squared air the better the peak signal to noise ratio will be. Finally, we can just use human raters to determine if they can tell the difference between our generated images versus the real aerial images. Fortunately, our GAN performed well and allowed us to pick out different size vehicles, such as small boats and SUV's. This allowed us to unlock different trading strategies, which involved parking lot capacity estimates to gauge retail activity. We could gauge automobile sales by looking at cars being towed by tractor trailers and various other applications. So we would gauge this as a success. The next step would be to potentially license out the software. All right, well, that's it for this video. Thanks for joining and join us in our next video as we continue our machine learning journey.