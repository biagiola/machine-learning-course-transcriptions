WEBVTT <v Instructor>Welcome back, everybody.</v> This is MLExpert's machine learning crash course. In this episode, we're going to be talking about convolutional neural networks. Let's say that we wanted to classify an image, based on whether it had mountains or not mountains, in the image itself. One idea could be to apply our fully connected neural network model to these images. So if we zoom in on this little box here, basically, what we get, is we get pixels. Now, each pixel will actually get a red, green, and blue value. Now, each of these red, green, blue values will get a value between zero and 255. So let's say our image is actually 200 by 200 total pixels. That little box we looked at was just three by three. What that means is that our 200 by 200 image will have 40,000 pixels, 200 times 200, and each pixel will have a red, green, and blue value. This makes the input to our neural network have 120,000 nodes. So here's what the input layer would look like for our image, into a fully connected neural network. Here would be the red value of the first pixel on the top left of the image. Here would be the final green value for the bottom-right page of our image. Let's go ahead and use the same strategy we talked about in the fully connected neural network video. That strategy was that if we have 120,000 inputs and we have one output, here, because we're just going to be doing binary classification, based on whether the input image does or does not have mountains in it, that means our middle layer will be the average of these two. So this one will have about 60,000 neurons in the hidden layer. So let's go ahead and connect these neurons up. And we have our output here and our input there, and these are fully connected layers. So the problem with this is that there will be over 7 billion weights to learn. Additionally, we'll have some odd 61,000 biases to learn as well, but we still have 7 billion weights. This is largely too many parameters to learn for a simple classification of whether an image contains mountains or not. Additionally, what if we took our input image like this and we just flipped it around? Well, what this means is that if we take our same little box that we examined, our little three by three, here, that box would actually be red in this region of the input of our input layer. Now, our original image did have mountains in this space, right here, but all of a sudden, now that we flipped the image, the mountains aren't there. So there's no mountains there. And it could potentially output a zero, here, just because we simply flipped the image. This isn't a desirable property of a network that will tell us if mountains exist in an image or not. So the problem with this is that the network is sensitive to the object's location within the image. So we need a better solution. And we can borrow something from image processing, called filtering. Filtering involves using kernels, which are just in array of values to alter an image. For instance, here, we can have our original image. We can apply a blur kernel to this image and we will get a blurry image out. Now, this kernel called the outline kernel, when given an input image, will produce it an outline of the edges within the image. So what does it mean to apply these kernels to an image? Let's go back to a little section of our image, here, with these zero to 255 red, green, blue values. And what we're going to do is grayscale them. So what that means is we will average all of the RGBs per pixel into a single value between zero and 255. Here, that's what we've done. So we've taken our image which was RGB to now, just grayscale. So here's our grayscale image and here's our outline kernel that we talked about which defines edges from an input image. So let's go over how to apply this kernel to this image. What we do is just overlay the kernel in the image. Then what we do is we multiply every entry in the kernel with every entry in the image of which the kernel is overlaid. So here, even though these values have nothing to multiply themselves with, we're just going to assume zero padding. So each of these elements in the kernel would just be multiplied by zero. So let's get this value, here. So all we have to do is multiply eight times 132. We have to add that to negative one times four. We have to add that to negative one times 143. And then we add that to negative one time six. All of these terms zeroed out because of the zero padding, so that's why you don't see them here. The result of this is the value that goes in our new image at this pixel location. So let's put that in, now. Next, what we do is we slide our kernel, just one element to the right. And we repeat the same process in terms of multiplying each element together in the kernel with the image, and then we sum that result to get this. So here, we would multiply eight times four. We'd multiply negative one times 132. We'd go through every element, anything on negative one times 47, in the image. We'd sum all of those up and we'd come out with negative 312. Remember since we're using zero padding, here, these elements negative one times zero, which is why they're not appearing over here. We continue this process by moving one more to the right, and we have performed the same calculation, here, to get this pixel value. We would then do that again and move to the right again to get this pixel value. And then when we get here, we just go one row down. And this process continues all the way until we get to the bottom-right of this image. In this bottom-right pixel, here, after the calculation with the kernel, would be assigned to this value, here. So here are the results from the kernel applied to the image that we have. And you can see that these values are no longer between zero and 255. So what we can do is we can just scale the image to make sure that it's between zero and 255. This is the equation we'll use. It involves taking the maximum value and the minimum value of this new image, and the maximum and the minimum value of what we want the image to exist in. Then we will get our new image. And again, this new image, since we applied the outline kernel to it, we'll find the outlines in the input image. Since this filter can extract edges, it would be interesting if we could use this in some way, in our neural networks, so that we could detect the edges of the mountains. Now, these kernels, they look pretty arbitrary. Here, we just have an eight and all the rest are negative one. What if instead of using this kernel or another kernel on the image to get its edges, what if we could use a neural network to decide what these values should be? Let's write how we apply this filter down, mathematically. So it's the same thing that we did earlier. So it would be negative one times the first pixel value in whichever image we're considering. That would sum all the way through until the ninth pixel of the image. Now, what if instead of using these fixed values, here, what if we could assign weights to each of these? Well, this looks extremely similar to what our neural network already does. Here, we have W transpose X. The benefit here is that we can now learn these W's according to some loss function. This loss function could be the negative log loss of identifying whether there's mountains or not, in an image. So let's go over how to actually apply this custom kernel, here, with weights, in an actual neural network. So here, we have a very basic, small image. And what we're going to do is we're going to build a neural network that represents the same technique we use to apply a kernel to an image. So first, we overlay the kernel on top of the image, starting with the top left pixel, here. So how would we represent this with our neural network? X1 is going to be multiplied by W5. And that will be added to a particular summation. W6 or weight six will be multiplied X2, here, and that will be added to the summation. W8 will be multiplied by X5, which is somewhere in here. And then, finally, the ninth weight for the last week, here, in our kernel, will be multiplied by X6, which is also in here. These all will be summed together. This is exactly what we were doing before. We're now just representing it in terms of our neural network. So now, let's slide this kernel over, just like we did before. And we see here that we have W4 times X1. We have W5 times X2. We have W6 times X3. W7 times X5, which is somewhere in here. And then we just continue on through the rest of the kernel, here and here, for W8 and W9. And all of those values, again, are just going to be summed. Remember since we have zero padding, here, these weights will be multiplied by zero. So they don't appear as connections over here. Now, let's continue by just sliding over one more. And we can see that we're simply following the same process. So here, W4 is a weight which is multiplied by X2, and that is summed with X3 times W5 and X4 times W6, and so on throughout the rest of the kernel, all the way to W9 and X8, which is somewhere in there. Now, we would just continue to the next row. We'd continue until we hit the very bottom-right pixel, here, of our image. That would complete our application of this kernel, here, to our image. The only difference this time is that we're using weights instead of some predefined kernel that we borrowed from image processing. Now, we're actually treating these values in the kernel as weights themselves in our neural network. This layer here is called a convolutional layer. And I should mention that each of these neurons still have biases. And with the convolutional layer, comes a new concept called the receptive field. All the receptive field means that per every neuron, we have only a subset of the nodes in which the neuron has knowledge of. Now, this is different from our fully connected neural network, here, which we learned in the previous video of the deep learning section, where the receptive field of every single neuron or node, here, is all of the inputs themselves. By the way, all of these weights take on different values. Now, this is different from our convolutional layer because we actually only train nine weight, here. So what does this do for us? So one, it reduces our parameters or weights from 256, which is what we would have had if we used a fully connected layer, here. So we do get a reduction in parameters. Two, what we also get is weight sharing across the layer itself. What this means is that if our kernel of nine weights learns to identify mountains in one portion of an image, those same weights can be used to identify mountains in different portions of the image as well. Kernels will typically be between three and seven by seven in dimension. Now, you'll notice that these are odd dimensions. That's because using that odd dimension allows us to center the pixel within the kernel. If you use an even dimension, you'll have to offset it in some ways. So you typically won't see that. Now, let's go over padding. We mentioned zero padding earlier, but you don't have to use it. Let's see what that looks like in terms of using it versus not. So here we have a three by three kernel and we have our four by four image. If we apply this three by three kernel by this four by four image, we will get the same exact size image out, four by four, if we choose to use padding. If we don't want to use padding, then we'll actually overlay the kernel like this and we'll only get four values. One from here, from centering on X7, X10, and X11. Now, this will come out to be a two by two image. The third concept that we should cover is stride. So we were using a stride of one when we were showcasing these kernels applied to some image. So here, a stride one means that you're going to work one to the right, one to the right, and then one down when you get to the next row of the image. However, stride two would mean that we would go from X1 to X three, and skip X2. And then when it came time to go to the next row, we would actually skip X5 and go directly to X9. So here, what this does is it reduces our dimension of our image by half. Kind of like when we elected for no padding, but this time, the dimensionality reduction comes from stride, instead of no padding. So after stride, another concept is the number of kernels to learn for a particular layer. What this means is that per convolutional layer, we actually won't just learn one set of weights for a particular kernel, we'll learn more than one kernel at a time. So here, we would take these kernels. We could stack them up. And then we would just apply these three kernels at the same time, in the same layer, to the same image. So let's go over that, now. Here, we're using no padding. We're simply taking these three kernels and sliding them the same way across the image that we did earlier. The only difference is now we're actually going to generate three of these new images, instead of just one. By the way, instead of calling these, new images, you'll actually hear these be called feature maps. Now, these feature maps would actually be stacked in the same way that we stacked our kernels. Now, generally the number of kernels can vary from four per layer to over 500. So, one question that may come up is how do we know that these kernels, since we're applying them at the same exact time to the same image, aren't learning the exact same things? Basically, how do we know that these W's across the different kernels, don't all equal each other? Well, it comes down to initialization. If we initialize our weights with the different initialization strategies that we talked about in the original neural network video, we can avoid each of these kernels learning the same thing. So one kernel could learn an edge, another kernel could learn maybe a zigzag, another kernel could maybe learn a particular texture. Okay, so now that we know we can learn more than one kernel per layer, we have to now talk about the number of channels. So, do you remember how we talked about the grayscale image where we just had one value per pixel in which it ranged between zero and 255? What that means is we're using one channel in our input. Typically, images will be represented by red, green, blue, which means that we'll have an X1 for red, we'll have an X1 for green, and X1 for blue, each between zero and 255. So how do we take these three channels and apply a kernel? Well, we actually do it in the same exact way. So we take our kernel. And instead of applying our kernel to just one image, we're actually applying our kernel to three images; one for red, one for green, one for blue. And since we have one kernel, we're going to get one feature map out. So how we get from one kernel across three images to one feature map, is we actually just sum the result. Since we're using no padding, we'll end up with a four by four. If this value, here, came out to be four, if this pixel value came out to be six, and if this pixel value came out to be eight, we simply would sum up the results. So here, it would be 18, here, would be negative two. If we moved over here, we would get 10. And over here, we would get negative eight. What this means is that no matter the number of input channels. Here, three, if we have one kernel, we will get out one feature map. Now, typically, what you'll see is these channels actually stacked on top of each other to form a three-dimensional array. All right, so this is effectively everything that we need to know for a convolutional layer itself. So let's see how this would actually look when we apply it to an image. You'll see that we have an input here. Our input is just an image. It has three channels; one for red, one for green, one for blue. And its dimensions are 185 pixels by 185 pixels. Now, if we wanted to apply a convolutional layer to our input image, here, we would specify that as a five-by-five kernel, valid padding. Valid padding, here, just means no padding. Finally, we've applied a stride of two in our convolutional layer. So after applying these convolutions, here, we end up with six channels at 91 times 91. Where did the six come from? Well, this is just what we picked as the number of kernels to learn, for this input image. Remember that no matter the number of input channels, if you apply a single kernel to it, you will get a single kernel out. Here, we just wanted to learn six kernels. Which means that our new number of channels will be six. How about this 91 by 91? Well, there's a formula that allows us to figure out these new dimensions. So here, we take our N input, which is the dimensions of our input. We're assuming that they're square. We then add that to two times the padding, which since we used valid padding, padding would be zero. We subtract the kernel size, here, which is five. And then finally we divide by the stride. That result, we then just add one to. So let's substitute those numbers. So here we have 185, zero for the padding, five for the dimensions of the kernel, and then we have a stride of two. Since we're assuming square images and feature maps, here, that results in a 91 by 91 dimensionality. So this is actually just one convolutional layer. But in reality, more than one conv layer or convolutional layer, is used. The idea is that the earlier layers can learn lines or corners, and then the deeper layers themselves can learn curves and shapes from the lines and corners. So let's see how that would look. We have our input image, here, and we have our same convolutional layer and our same feature map results as we had in the previous convolutional layer. This time though, we've now added another convolutional layer in which we wanted to learn nine kernels. Here, we learned a five by five kernel with no padding, which is valid padding, and a stride of two. We'll call out a couple of things here. One, the number of kernels per layer is growing. The idea is that the number of ways to learn lines or zig-zags, is pretty limited. But the number of ways to combine those lines and zig-zags has far more combinations and possibilities. We'll actually choose to learn more kernels as the convolutional neural network gets deeper and deeper. Another thing I want to mention is that we started out with 185 by 185. And through these different convolutional layers, since we're using a stride of two, we've actually decreased our image dimensions. Here, the idea is that we can learn a summary of our input image by applying these convolutional layers to it. So here, we're summarizing 185 by 185, down to 44 by 44, with the understanding that we're preserving the important parts of this image in these feature maps. Now, most of the time you won't actually see two convolutional layers adjacent to one another. What you'll actually see is another way to cut down these dimensions, beside using the stride and the convolution. What you'll actually see is something called a pooling layer. A pooling layer, we're going to go back to grayscale images, now, just for simplicity. So, typically, we've applied a kernel with weights to our image. But what a pooling layer does, for instance, a max pooling layer, all it does is when you overlay it on an image... So let's say this is our image values. We simply take the maximum value of whatever exists in this kernel. So here, for a kernel centered at this value six, the maximum value is 143. So we would just put 143, here. Then in the same way that we did earlier, we would slide our kernel one to the right, centering it here, now, and we would take the maximum value of whatever's in the kernel space, now. And that would be 72. We continue this process on for the rest of the image. So what does this do for us? So one, it gets our dimensions down. Remember our goal is to summarize the input image, the best way that we can. With this, comes two things. One, we need to reduce the dimensions of the image, that is the summary, and then the weights we're learning will hopefully extract the important elements within the image, according to whatever loss we're trying to optimize. So what this does is it helps us reduce those dimensions. Another thing that it does is that since we're looking for the max values in this image, according to the kernel, it increases the chances that we will become shift in variant. All that means is that if a certain pixel value over here, then shifted over to here, well, the kernel would actually capture that, instill extract that maximum value. So what that means is that we can shift different objects in the image and still be able to identify those objects within the image. Additionally, these pooling layers act as a way to denoise the image. All right, so there's another pooling layer which is less common and which is called average pooling. The same principles apply, except instead of taking the maximum of all the pixels, you actually average it. In general, max pooling is preferred. So what we'll now do is return to our convolutional layer. And instead of inserting a convolutional layer, here, we're now going to insert a max pooling layer. So the max pooling layer will have a kernel size of three by three, and we're going to use a stride of four. Now, two things I wanna point out: when you have a max pooling layer, the number of channels will remain the same as the previous layer. Also you'll notice that these dimensions are a lot less than this. We used a stride of four. This means we were skipping four of the values, every time that we shifted the max pooling kernel. This means our resulting image is going to now be 23 by 23. Remember the goal is to summarize this image. So now, we summarized it here with feature maps, and then we applied max pooling to this feature maps to get even a smaller representation of this original image. So now, what we'll do is insert another convolutional layer after this max pooling layer. Here, we have a kernel size of seven by seven. We used valid padding or no padding, and we also used a stride of four. We also chose to extract 10 kernels from this previous layer. All this means we can use the same formula we talked about before, is we'll actually end up with a five by five image. So what do we actually do with these resulting feature maps? Well, we can actually add what's called a flatten layer. And what that does is it just unravels this entire thing into one flat layer. So here, we have five by five, which is 25 pixels. And we have 10 feature maps learned from the 10 kernels that we applied to this max pooling layer, here. And those values will go here in this flattened layer. So this flattened layer will have 250 inputs. By the way, these layers are fully connected. And those will be finally sent out to a single neuron, here, which will end up in our binary classification. This value will be zero if the input image did not contain mountains. And it will be one if it did. So let's go into our practical application, here. Let's say it's early 2017, and we worked for a very small hedge fund that's interested in making better earnings predictions. Earnings which are typically released to the public, once every quarter, are reports detailing financial information of a company, like revenue and expenses. Investors, now, with the knowledge of the earnings report, will trade the particular company's stock. In that, it could either go up or down, based on the earnings report. The idea is that if we can predict the earnings report before they're released, then we can place a trade before earnings and make money if the trade goes in our favor. Fortunately, around this time, a gaming company was set to release a somewhat novel gaming device within the coming months. Our hypothesis was that if we could determine how many of these devices were being distributed out to customers, we could get a better idea of what their earnings report would look like. At the time, there were three distribution centers in North America, two were in the United States, and one was in Canada. Our thought was that if we can monitor how many tractor, trailers, or trucks, leave the distribution center for a few months before the release, and then measure the increase in the number of trucks trafficking the distribution center after release, we could get an idea of how many consoles they're selling and hopefully form a better earnings prediction. So we need a way to monitor these distribution centers. Let's start with traffic cameras. In the U.S. we can use the Freedom of Information Act and work with local and state governments to get traffic camera footage in the vicinity of these distribution centers. Now, unfortunately, this tactic won't work for the distribution center outside of the United States. But fortunately for us, cloud coverage was not a huge problem. So that allowed us to work with satellite imaging companies. So for this video, let's go ahead and stick to the distribution centers in the United States, here. And let's assume that we did obtain traffic camera footage, just outside of the distribution center. So over here is the distribution center. Here is just a four way intersection. Over here is to the highway, and here cuts across through town. Now, what we could do is just hire someone to count the number of trucks coming and going, and report back a number. However, if we wanted to apply this concept to other distribution centers or different tactics, then that wouldn't scale. So what we need to do now is build a model which can tell us whether a truck is in the frame or not. We don't care about regular cars like sedans or SUVs. And we also don't care if the truck is coming or going from this particular location, because we assume that for every truck that enters, a truck will leave. So what are these traffic camera feeds? Well, they're actually just 30 images per second. We need to model if the truck is in the frame or not. And we also need logic surrounding the model to actually count the number of trucks. So what we can do is that as a truck enters the frame of the image, we can start a counter for how many times the truck is identified in the image. And then as soon as the truck is no longer identified in the image, we can divide by the total number of images in which the truck was found. All this means is that we will end up with the number one for every truck that enters and exits the frame. As well, we'll divide the final count by two because we don't care if trucks are coming or going. Let's say that we need 100,000 images to train our network. This will equate to about an hour of video. The image sizes are 335 in their RGB. We're gonna have half of these 100,000 images, one hour of video. Half no trucks in them. And then what we're gonna do is take these 100,000 images with half no truck, and we're gonna ship them off to Mechanical Turk. Mechanical Turk is a service owned by Amazon which allows people to outsource simple tasks to a massive amount of workers. So the first stage that we'll send off to mechanical Turk will be identifying segments of the video which contain a truck and which don't contain a truck. The second stage will be taking those segments and labeling each image in those segments, either one or zero for containing or not containing a truck. The idea is that we want various angles of the truck, pulling into or out of these distribution centers. Again, we'll end up with 100,000 images labeled of whether a truck or a truck does not appear in the image. And we'll have various angles of that truck. All right, so what does our model look like? Well, we're gonna use a convolutional neural network, here. Our input is going to be RGB. So three channels by 335. We're going to have our first convolutional layer which will have eight kernels. It will use a three by three dimension for each of those eight kernels. The stride will be two and it will use valid padding, which means no padding. Then those eight feature maps produced by that convolutional layer will be fed into a max pool layer of a kernel of three by three and a stride of four. This similar pattern will continue on with increasing the number of kernels per convolutional layer. As we said before, to learn more and more complex features within the image. We'll finally end with a flattening, which is a fully connected layer with a log loss. As for hyper parameters, for regularization, we're going to use L1 and L2 regularization. We're going to use ReLUs for our convolutional layers, as well as our fully connected layers. We're going to use Adam as our optimizer and we're going to use Kaiming initialization because of our asymmetric activation function. If you need to review these, go ahead and watch the original neural network video in the deep learning section. For training, we're going to have an 80/10/10 split. What that means is 80% of our images will be used for training, 10% will be used for validation, and then 10% will be used as the test set. We're going to use 10 epochs, here, which means we're going to iterate through our entire dataset or this 80%, here, 10 times. And our mini batch is going to be the size of 128. So for that gradient step update, we're going to process 128 images. So after doing all of these things, we had a problem. It was overfitting, which means that our training loss looked really great. It looked like it had converged, but when we went over to the validation set, it didn't perform well. So what that means is we're going to add dropout to our neural network. Here, we're going to add dropout after our pooling layers. Additionally, we're going to add dropout with our fully connected layer upon flattening. So after we did this, the problem was that the model was too slow to converge. So training this neural network took too long for the loss function to decrease. So what we can add instead is something called batch normalization. All this means here is that we normalize the values, that means subtracting the mean and dividing by the standard deviation, for all the values, before they get passed to the activation function within the convolutional layers. We can also do this to the values before they enter the activation functions of the fully connected layers in the flattening layer. This will help smooth and speed up the loss function convergence. We can also decrease the amount of dropout because batch normalization usually contributes to regularization. All right, so after all of those things, we actually ended up with about 100% test accuracy, and identifying which frames did and did not contain a truck. So that's extremely good for us. The output of our model, though, in counting these trucks, actually goes into another model to predict the earnings report. And then that actually is a feature into another model which allows us to predict which trading strategy to use in order to reflect what we think will happen with our predicted earnings report. Fortunately, our feature generated from this model was found to be very predictive and it helped us place a very nice trade that paid off well. Now, our convolutional neural network is actually pretty simple. Let's take a look at some of the state of the art convolutional neural networks. And by that, I mean some of the best available convolutional neural networks. AlexNet actually runs two convolutional neural networks in parallel, and introduces cross connections between them. This has performed pretty well. GoogLeNet, on the other hand, introduces something called an inception layer, which is really just a cool name for using different kernel sizes per convolution. These different sized kernels produce different sized feature maps. And these feature maps are actually just upended on top of each other, and the smaller feature maps are actually just zero padded to match the larger feature maps in which were generated from the larger kernels. Finally, we'll look at ResNet, which is a convolutional neural network which introduces residual connections. This can be effective at eliminating these vanishing gradients as the network grows deeper and deeper. And they're implemented by simply adding past feature map values to future feature maps. This allows networks using these skip connections or ResNet to go fairly deep. ResNet actually has 150 layers. And these residual connections have been used in some networks to go up to 1000 layers. The assumption is that the deeper the network, the more ability the network has to represent complex data. ResNet did beat GoogLeNet and AlexNet. So the results speak for themselves. So to wrap up, Keras is a good library that I recommend. They offer convolutional layers, it's called Conv2D, where you can specify the number of kernels you want, the kernel dimensions or sizes, what activation functions you want at the particular layer. They offer dense layers and fully connected layers if you just wanna make regular neural networks. They have flattening layers. And they offer GPU support which is a graphical processing unit. If you're not familiar with this yet, don't worry, we'll actually discuss that more in the large scale machine learning section. Now, if you don't want to start from scratch and instead you want to use other models, especially those state of the art models that we talked about earlier, you can use something called Keras Applications which gives you access to pre-trained models. So then you can go in there and you can access ResNet or you can access InceptionNet. All right, well, that wraps it up for this video. Thanks for joining. Join us in our next video as we continue our deep learning journey.