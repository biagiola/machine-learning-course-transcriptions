WEBVTT <v Instructor>Welcome back everybody to ML Experts</v> machine learning crash course. In this video, we're going to go over something called singular value decomposition. What if we had a bunch of data and we didn't really know much about it? There could be underlying patterns in the data, and some of the patterns could potentially tell us a lot more than the other patterns. What we'd like to be able to do is take data, look for patterns in the data, separate out these patterns from the data itself, hopefully order them by importance, and then we'd be able to walk away with an understanding about how the data behaves. We can use singular value decomposition for this, or SVD. Let's take a look at these points. So we have three points here for two dimensions, so this is X one X two for each point. Let's go ahead and plot them. All right, so now that we've of them, what would you say is a general idea you'd like to take away or summarize about this data? Well, generally I think it's important for us to note that we have two points close together, and one point further away. Now, SVD states that our input array, A, which was our data point, can be represented by three different matrices. Let's assume that we can solve for each of these matrices. For now here, we'll take care of this transpose. So how do we use this to help us summarize our data? So let's go ahead and define something called A one. A one will take the first column of U, it will take the first value of sigma here, and then it will take the first row of the transpose. If we take this result, so we'll multiply this element by these two, and then we'll multiply these, what we get is three new points. Now, if we plot these points, let's see what happens. So here we have plotted our three points. So what did this do for us, and why is this different than the original three points? Now, if we look closely, we can see that these points actually lie on a single line. Now, let's take this line and just put it in one dimension which means all we have to do is rotate it. Now we said earlier that we wanna preserve the fact that these two points are close, and this point over here is further away from these two points. Now, what's interesting is if we look at these two points, and we look at our new one dimensional data, these two points actually got placed next to each other, relatively. As well, this point over here which was further away from these two points got placed over here. So all while reducing our dimensions from two dimensions to one dimension, we still preserve the relative location of the points. Now you may be tempted to say, "Well, I can reduce this to one dimension, all that I need to do is map each point onto the one access here." Well, let's give that a shot. If we do that, we see that we've eliminated the X two value here, we're just left with the X one values. Interestingly, though, we can see that these two points are closer together, then this point is the outlier. In reality, these two points are actually closer. So the benefit of SVD is that to form a projection instead of just going directly down to the axis itself, it allows the axis to rotate, then it will map those points onto the axis, and then we can rotate those points back to get our one dimensional representation. So really, that's exactly what these terms are doing. We can see here this term, U, is just simply rotating this line over here. Now sigma is mapping our points into one dimension, and then finally we're rotating that line again. So here, these terms actually do a rotation, a scaling, or a projection, and then finally a rotation again. So earlier, we just used the first column here, the first value here, and then the first row here this was called a rank one approximation, which is why we call the previous array A one. Now, if we multiply all of these matrices together in their entirety, we would end up with our original data. So let's get intuition about what these values mean. So if we take this value here which is the first element in this diagonal matrix, and we put that element in the numerator, and then we divide by the sum of all the elements in the diagonal, we get how much of the variance is explained by this particular representation. Again, this is the rank one approximation. Now, if we want to use the second value here only and we're going to ignore the rank one approximation, so here we'll just use the second column here, we'll use the second value in this diagonal matrix, and then we'll use the second row in the matrix V. Now what this gives us is another rotated line, but this rotated line is going to be perpendicular to our first rank one approximation line. So what this line represents is the next best representation for our line if we were going to reduce it to one dimension. So here, let's conduct the same process we did earlier with our rank one approximation. So we'll map these points onto this line here, and we'll now have our points on a single one dimensional line. Now, you can see here that this isn't as good of a representation of our data, this is because it is the second most important factor about our data. So here we can see that using just these as an approximation for our data only explain 34.3% of the variance of our original matrix. So one question that could come up is we use this column to get our rank one approximation along with this value and this row here. Now, if we only wanted to use the second column here we would use the second value here, and this second row here. So you might be wondering, well, where do we use this, and why is this diagonal matrix having these zeros over here? Well, we actually don't use this. As well, this being an M by N matrix means that there will be M minus N rows that we don't use. And the same goes for here, there will only be M minus N rows that we don't use. All right, so let's see what we can actually do with SVD. Let's say we were just hired by an online video platform, Vimeo. An assortment of copyright claims have come flooding in, basically these content creators upload their content to Vimeo. The idea is that Vimeo will show ads alongside their videos and the content creators will receive some form of revenue. Now, what could happen is a less than desirable character could download these videos, and they could upload basically the same exact videos that these content creators created and then begin to collect revenue from these content creators property. What should be fair is this person should get no revenue, and then all of these content creators, the original content creators, should receive the revenue. We need to fix this. So one solution is that when content creators upload their video, we embed a key in the video such that when someone downloads the video and tries to re-upload it, our system will scan the video for keys and if this user doesn't own this particular key, then we'll just reject the upload. What this will do is allow us to eliminate human intervention in the case of blatant copyright abuse. So how can we implement this? Now, we're going to play off of the idea of a watermark. A watermark is a somewhat transparent word or image placed on top of photos to clearly display copyright ownership. We don't wanna put obnoxious watermarks in our creator's videos though, so we'll have to do something more clever. What we can do is when our content creator uploads video, we can put in a humanly imperceptible watermark in each frame, such that if any content creator uploads the same video, we can scan for all of our known watermarks and reject the upload. So how we do this is we're first going to generate a key, think of it as a QR code or black and white pixels. We'll place this key randomly across the video in case someone crops it like this and we can just find the key in subsequent frames. Remember, we're doing this for every single video that gets uploaded, and every content creator or account will get their own unique key. Now, these QR codes or keys are just an arrangement of black and white pixels, so how we'd represent that in terms of a gray scale, like if we don't wanna represent red, green, blue if we just wanted to represent black and white, we can just assign these pixels a value of either zero or 255. 255 will be white, zero will be black, and that will generate our key for the user. So how do we hide this into the video such that people can't see a QR code in the video? Well, what we can do is take our QR code and apply singular value decomposition to it. This will give us our matrices, U sigma and V transpose. Okay, so once we apply SVD to this QR code, we'll take the rank one approximation of it. Let's say that results in this vector here. So let's set this rank one approximation of our QR code aside. Next, we'll take a look at a video that a user has uploaded. What we'll do is for every frame of the video, we will take a random section of the video, say a square or rectangle, and these will be the gray scale pixel values, again, technically you'd represent these videos with red, green, blue, but for our example, we're just going to represent them as gray scale which means every pixel will just get a value between zero and 255. We'll take that little square that we took from a single frame in the video, it will perform singular value decomposition on it. So now let's bring back the rank one approximation of the QR code that we generated for this user, and now we need to figure out how we want to hide this information in this SVD. Now, remember we said that the rank one approximation explains the most variance out of any other row or column that we could use. Now what we'll do is we'll plot the amount of information we get per row and column that we have. So for instance, the rank one approximation is here, and all of these approximations, they sum to 100% of the total information of the image. Because as we said earlier, if you use all of the rows and the columns in the SVD, you'll get back 100% of the original information. All right, so now, instead of just looking at every single individual row and column in that information content, now we'll just take the summation as we go along. So here's the rank one approximation, the rank two approximation uses the rank one plus the rank two, the rank three approximation technically uses the rank one approximation and the second row, second column, and the third row, third column in that SVD. Now what we see is we get this large information gain by using the first, we'll just call it R ranks, we get a very large spike in the total information of the image, but as we approach the 200, 300, 400 ranks we see that we don't get as much information from those as we got from the rank one, two, and three approximations. What we'd like to do is to be able to find a rank which covers about 95% of the information of the image. Now, 95 here is a rough estimate, it could be higher, but generally the goal is to figure out at which rank does adding more ranks become humanly imperceptible. For our example, it actually was 95% such that if we added more ranks, people couldn't notice, so here that was ranked 175. What we'll do is we'll take our first rank approximation of our QR code, and we'll embed it into the 175 rank of the SVD. So let's say that this 175th row looks like this, and remember, we want to embed our QR code in it, what we can do is take these values and simply append them to the end of the decimals at this place. Then what we'll do is replace this element with this new updated value, which contains our keys appended to the lowest decimal places. So how do we use this? Well, let's assume that we don't have any better system set up yet, so what we'll do is for every video that gets uploaded per frame, we'll go through every single possible box that we could have on that frame. So let's say that we've extracted a particular box from a single frame of a video that just got uploaded, what we'll do is we'll take the SVD of that, we'll look at the 175th row, or whatever row gives us 95% of the variance explained, then for every single key that we have, we will check to see if the key is embedded in that particular row. Here it is, one of the keys that we have matches this particular row, so we will reject the upload. Now in practice there are better ways that we can do this such that we don't have to scan every possible box and every single frame, we can also find a better way than checking every single key that we have by clever hashing techniques. But for now, let's just assume that this is our method. So how would this look? So we have our content creators, they upload their video content, we now have our watermarking service which embeds keys per user into the video, then what we have is if someone downloads a video from our site and then re-uploads it in an effort to collect some of their revenue, we will scan this video for their watermark, and we will reject their upload if we find a match. So we've talked about why SVD is useful and what each of these terms can represent, however, we haven't figured out where these terms come from. So we need to talk about something called eigendecomposition. Eigendecomposition states that any matrix that is square can be broken down into eigenvectors and eigenvalues. Now in this video, we won't be covering eigenvalues and eigenvectors, but there's a few problems with eigendecomposition. One, it only works on square matrices. Two, these eigenvalues, they don't necessarily lie between zero and one, as well, these ranks of these eigenvectors are not perpendicular. So we can't break down our data in the same way that we did with SVD. SVD solves these problems by allowing any sort of matrix, it doesn't have to be square. Additionally, this term is the eigenvalues of A times A transpose, this allows these values to lie between zero and one. Finally, this value is just the eigenvectors of A transpose A. To get the value of U, we can simply solve this equation. So now we know where SVD comes from as it relates to eigendecomposition. We also now know that SVD is a generalized version of eigendecomposition. It applies to any sized matrix. So now that we've talked about this, let's go ahead and talk about something called PCA, which is principal component analysis. Earlier, we talked about how A would be equal to VLV transpose. That was the eigendecomposition. Well, now what if we took our A, and we standardized it, so that means we subtracted the mean and divided by the standard deviation, and then we divided by N minus one. What that means is we have a correlation matrix. We talked about correlation earlier and the same concept applies here. The problem is this computation is typically unstable. So instead, what's typically done to get PCA is we use SVD, but instead we put the standardized matrix A in there which means we subtract the mean and divide by the standard deviation. And what that gives us is this here U sigma is now our principal component. So we can perform PCA, or principal component analysis, by performing SVD on the standardized matrix. So, one thing that we covered earlier in the course is that a lot of our algorithms were vulnerable to high dimensionality. We can use PCA or SVD for dimensionality reduction. So earlier, we drew this graph such that the percent of information was here and the total information representation was here, what we can do is we can approximate our data by a given rank. So let's say that we wanted to find some approximation of our data that can cover 95% of the variance of our data without using all 400 dimensions, well, now we can use only 175 dimensions to represent 95% of our data. This can be immensely helpful for a lot of algorithms. Keep in mind that PCA and SVD assume a linear correlation between your variables. However, there are nonlinear dimensionality reduction techniques. You can use something called kernel PCA. This actually uses the same kernel trick we talked about in the previous videos. What this would do is project this data into a higher dimension, and then reduce the dimensions to one dimension. For me, I think this is really impressive. So to wrap up, let's go over some libraries that you can use for PCA. Even though we say PCA, typically software will use SPD as the implementation. So I like to use scikit-learn, they offer PCA as well as kernel PCA. Well, that wraps up this video, thanks for joining. Join us next time as we continue our machine learning journey.