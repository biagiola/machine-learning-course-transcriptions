WEBVTT <v Instructor>Welcome back to MLExperts,</v> Machine Learning Crash Course. In this episode, we're gonna talk about decision trees. For instance, let's say we want to determine out of a group who will like a particular unreleased movie. We can take a portion of the group of people and show them a preview of the movie, and then ask if they would go see that movie if it were released. We can also ask them to fill out a survey. The survey could include questions about what movies they've previously enjoyed. Now, if we send the same survey to the rest of the group who didn't preview the movie, we could train a decision tree to predict those in the group who will go see the movie once it's released. If the survey only had two questions, one about liking "Finding Nemo" and the other about liking "The Dark Knight," the decision tree would look like this. If they answer yes, we put them on the left node. If they answer no, we put them on the right node. This particular person answered yes. They did like "Finding Nemo." Then we asked them, "Did you like "The Dark Knight"?" The same process follows. Put them on the left if they liked "Dark Knight," and we put them on the right if they don't. In this case, the person didn't like "The Dark Knight." So now that we've exhausted our questions, we simply change this leaf node to positive, because this person indicated that they would in fact go and see the unreleased movie if it were to be released. We then repeat this process for every other person. So what we have now is three leaf nodes. One person who landed over here who said they would, another person landed over here said they would also go see the unreleased movie, and this person who answered, "No, I don't like "Finding Nemo", "but yes, I like "Dark Knight." they answered they would not go and see the movie based on the preview they were shown. So, let's say that we wanted to predict within these people who didn't get to go see the preview of the unreleased movie, but they did fill out the same survey as the people who previewed the movie. Let's say we wanted to predict whether they would go and see it. So we hand them the same survey, and based on their responses, we traverse down this tree. And we can see that this first person, they ended on a positive node, which means we would predict that they would in fact go see the movie if it were to be released. We can just repeat this process for every person in the sample. Four people are predicted to would have gone to see it if it were released, seven people were predicted to not have gone seen it, and four people landed in this node over here, which means we don't know. We could have sampled more people to maybe fill in that node, but our sample size was too small. Okay, so there's several methods that we can use to build this tree for us, but we're gonna concentrate on something called CART, or Classification and Regression Trees. Let's dive into a real example. We were recently hired by Precision Drilling Corporation, which is among the top oil drilling companies in the U.S. Drilling for oil is an expensive project. As well, when you commit to drilling for oil at a particular site, you have no way of knowing if you'll actually hit the amount of oil output that you'd like to hit. Precision Drilling would like us to report which potential drill sites they should commit to in order to recover the most oil, and therefore afford them the highest profits. We have access to drill site measurements for the past successful and unsuccessful sites to use this data to train our model. The features we have are how porous the rock we're drilling into is, how much natural radiation is emitted from the rock, the density of the rock, and the time required for a sound wave to travel through the rock. Of these features, there are different units, and they exist on different scales. For instance, porosity will sit between zero and 0.5, and sonic can be in the thousandths of milliseconds. Fortunately for us, CARTs are invariant to scale, and are pretty robust to outliers. So we can leave out feature scaling. The features we have are reported for different depths. So for each depth, 1,000, 2,000, 3,000, 4,000 meters, we have all of these measurements. Taking these measurements is expensive. Not as expensive as drilling for oil, but definitely costly. As such, some of the measurements are actually skipped to save cost. Fortunately for us, CARTs can handle missing data in the training set, as well as for examples we want to make predictions for. The labels we're provided with per feature set are binary indicators which represent whether the project was profitable or not profitable after committing to that drilling site. So let's start with a few examples to see how we build our CART. Let's start by figuring out how to split up our examples by their labels. So we only have numerical features. So according to the CART algorithm, we need to decide which feature we should split on, and also if we do find a feature to split on, what value of that feature is best to make the split for. The goal is to find the best feature and value which separates our examples by labels the most. So for each feature, we need to figure out the best value to split on within that feature. Let's say that we were going to look at porosity at 1,000 meters, which is this first column here. The first thing we should do is order our examples by that column. So we did that here. We have 0.02, 0.07, 0.13 and 0.21. Then we just find the average between each adjacent data. So for instance, 0.045 is the average between these two, this is the average between these two, and this is the average between these two. Each of these averages is a split point that we'll evaluate. So we have the feature here, the porosity at 1,000 meters, and we also have each split point we want to evaluate for this porosity. We take all of our examples and we put them in a node, and if the value is less than a split point, you go left. If the value of the split point is greater in your example, then you go right. So for instance, 0.02 is less than 0.045, so it went left, all the other ones went right. And we evaluate every single adjacent split point like this. So how do we evaluate the effectiveness of each of these splits? Well, we can use something called Gini Impurity to help us out. The first step is we find the probability of getting class 1, square it, and add that to the probability of getting class 0, and square it, in a particular node. For reference X1 and X2 were successful drilling sites. X3 and X4 were not successful and are class 0. So here the Gini Impurity of this node is going to be 1 minus the probability of getting class 1 squared, which is 1 squared, plus the probability of getting class 0, which is 0 since there's no class 0's here. Three and four are not in this node, and you simply add those up. So 1 minus 1 is 0. Now for this node, the probability of getting an example with class 1 is 33%, because there's only one example here, X2, which is class 1. And the other examples, the other two examples, are class 0, so that probability is 0.66. Crunching those numbers, that gives us 0.45. Now the next step is to weight each node. So here we only contain one out of the four total examples among the split, so we times this result by 0.25. And here we contain 3/4 or 75% of the examples, so we multiply it by 0.75. That will give us 0.3375, which is the Gini Impurity produced by a split like that. Then we just continue and do the same thing for all the other splits. Notice that the lowest Gini Impurity produced is from this split right here. What we have is a very clean separation, a perfect separation, if you will, because the Impurity is 0, where X1 and X2 belong to the same class, and they're the only ones in that node. And X3 and X4 belong to the same class, and they're the only ones in that node. So in our example, for this case, the best split for porosity, and since you can't beat Gini Impurity of 0, our first split will be porosity at 1,000 meters with a split of 0.1. Now, let's say that we didn't achieve a Gini index of 0. What we would do is just go through and do the same thing with every other feature. But fortunately for us, like I said we can't beat a Gini index of zero. So we will pick porosity at 1000 meters with a split point of 0.1. So once we do have this first split point settled, we just repeat this process recursively for each node. So unfortunately we can't split these nodes anymore but if we could, we would just recursively repeat the same exact process for every subsequent node. And yes, you can use the same features again. That's completely fine. So this tree is an example of what could come from other data. So here we split on porosity first at 0.1. Down here, we split on Sonic. Then we split on Sonic again. And then basically we ended up with leaf nodes that contain all of the data. And each of these data will have their own labels. When do we stop splitting? Well, we can assign a max depth to a tree. We can assign a minimum number of examples per node. Let's say, we said 10. Though if a node has 10 examples, then we can't split. Or we could go until the nodes are pure which could introduce quite a bit of overfitting with our model. All right. So now that we have our decision tree built, how do we use it? So let's say that we have Xi in here. It's an unseen example that we haven't seen in our training data. We would simply navigate down the tree like we did on the toy example at the beginning of the video. Then once we got here, we would look at this node. Let's say that four of them were profitable drill sites, as a result. Two of them were negative drill sites. So this unlabeled example that we've seen, we would label it as a vote of all of the examples present on that node. So we would assign this unseen example to be a profitable drill site. That would be our prediction. Okay. So how do we handle missing data? Well, it's actually pretty easy. Remember when we were going through and we were establishing split points for features and then the split point values themselves. Well, all we do is we keep track of the N best split points. So let's say that the first split point we found porosity at 1000 meters at 0.1, let's say that feature did produce a Gini Impurity split of zero. Well, then what we do is we would go to the next best feature split. So let's say Sonic at 2000 meters at 0.4 value that produced the next best Gini Impurity for the split. And then we just continue down for effectively all the features. These are called surrogate splits. And what they do is let's say we get in an unseen example that is missing some data. This example, they happen to not take the measurement of porosity at 1000 meters, which was our most predictive and our best feature to split on. So what then we would do, is we'd go to the next best feature to split on. And let's say that somewhere in here. And we would split on that instead. And we would just continue all the way down the tree in the same way. So that's how we'd handle missing data. What about multi-class labels? Now let's say that instead of deeming a drill site profitable or not profitable, let's say now that they want to know what drilling technique they should use. So there's horizontal, there's vertical, and hydraulic drilling techniques. Well, now we just have to add another element to our Gini Impurity. Let's say that the label for example two was 1. The labels for example three and four are 0. And the labels for example one is 2. We'll call 0 to be vertical, 1 to be horizontal and 2 to be hydraulic. So the only difference is we just calculate Gini Impurity with this added term but it's exactly the same way that we did it with just two labels. So now how do we handle multi-class when it comes to predictions? Well, we still do the same thing, going all the way down. And let's say, we end up this node. Now this node will be containing examples through three different classes: hydraulic, vertical or horizontal drilling methods. And now just take the relative majority or the mode. And here that looks like this class. So now this example would be labeled as this class which in this case, green was hydraulic drilling. Okay. So we know how to handle missing data. We know how to handle multiple classes. What about regression? So what if instead of predicting profitability they actually want us to predict barrels of oil a day. So it's gonna range from a hundred to a thousand barrels per day. So in this case, when we're making these split points on the features, we're still gonna make the split points on the features in the same way, ordering them and taking the averages of the adjacent features. However, we're no longer gonna use the Gini Impurity. We're instead gonna use the Mean Squared Error. All this means is that we are summing the difference between all the values in the node and the average of that node. So here we only have one example, X1 is equal to 320. So that's 320 is the average. It's the only example. And we only have one example to sum through, so that's also 320. We square that term divided by one because there's one example and we get zero. Basically what this is saying is the difference between this example and the average of this node is zero which is the case because there's only one example. So what about this node? Well, the average of this node is going to be 421 and let's expand this summation out. So we have the leaf of X2 is going to be 140. So we'll plug that in. And this is actually, this is X4. So X4 is going to be 413. There's three nodes, so our n is three. So we actually get 54,182 as the Mean Squared Error. Now that's the Mean Squared Error of each node? However, we actually want the Mean Squared Error of the entire split. Now to do that, we'll actually just take all the terms that we've had in all of the numerators before. So here, this is X1 minus the average of X1 and this is X4 minus the average of this node and we're going to divide by the total number of examples. This will give us the Mean Squared Error of the entire split. Here, that's 40636.5. Now we want to compare the split Mean Squared Error with the other splits. So next we'll do this split here and we can find each nodes' Mean Squared Error. So here that'd be 8,100, and that would be 22052, just in the same way that we calculated each of these nodes' Mean Squared Error. Now to find the Mean Squared Error of the entire split, as we mentioned before, we have to take all of the squared errors in the numerator and divide by the total number of examples across the split. Here, that's four as well. So the Mean Squared Error of this split is 15,076. And we'll follow the same process for this split over here. That would give us 14,150. Now the split that we will select, out of all three of these, will be the split that gives us the smallest Mean Squared Error for the entire split. So here that would be this split over here. So, how do we form predictions with unseen examples? Well, we have Xi and have to traverse down to here. Instead of binary examples, because we're no longer have binary examples, all of these examples are actually just going to have values. We actually just take these values and average them together. The average of the values in that node is going to be assigned to Xi. Alright, so we've handled missing data. We've handled multi-class labels. We've even handled regression now. Fortunately for us, our model has been doing quite well in the U.S. and our company has been reached out to by Angola, and they would like to use our model. They have their own drilling site records that they've shared with us. The big difference is that Angola is in the Southern hemisphere and that could mean geological differences in the rocks. So we'll actually add a binary feature to our model signaling whether the example is in the Northern or the Southern hemisphere. So how do we split on the binary feature? Well, it's extremely simple. If you're on the Northern you'd go left, Southern, you'd go right. If you're doing classification, you use Gini Impurity. If you're looking at Regression, you'd still use Mean Squared Error. So we go continue using this model for some time. Finally, other members of OPEC hear about the success with the model and Angola now want to use it too. So no longer will this binary feature suffice. So we'll actually need to be representing each country, categorically. So we'll be working with the U.S., Saudi Arabia, Iraq, Iran, UAE, and Angola. So to evaluate the categorical split points we actually have to evaluate them as all the subsets of the categories. For instance, one split point could be all the U.S. examples go to the left and all the other countries go to the right. And then we could say all the U.S. and the Saudi Arabia examples go left and all the others go right. And we would continue on for every single subset. Now there's 2 to the n, minus 1 subsets. Just be careful that as your categories grow, the number of subsets that you need to evaluate, the split points, also grow. Again, same exact principle here. Classification, you'd use a Gini Impurity. If you're doing Regression, you would use Mean Squared Error. All right. So what's the downsides of this? It sounds pretty great. You know, you don't need to scale the data. You can handle missing data. Tons of different types of data, but what's the problem? Well, it actually over fits really easily. Remember we were talking about the depth. As the depth increases, the ability for the model to overfit the data increases as well. So, typically what we can do is limit the depth, typically between 3 and 5, and then use a really cool technique that I like, called Boosting. What is Boosting? Well, imagine we were doing a regression example. The ith example we've predicted the label to be 230. Well, the true label for that example was 270. So we had our tree, which we'll call tree 1, predicted 230, and the real answer was 270. So we actually generated an error here. Now, the true value is actually tree 1 plus the error. So what if instead we could take this error and train another tree on it? So it sounds like we're cheating and we kind of are, right? So tree 1 would have the label of 270 and predict 230. Tree 2 would have the label of the error, which is 40. And that would be the prediction. So we can continue this process indefinitely. So tree 1 would be the label of the example. Tree 2, we would train on the error of tree 1. Tree 3 we would train on the error tree 2. And we could just go on forever. So what's wrong with boosted trees? Well, they even overfit more than regular trees typically. So we need to perform a cross-validation to figure out how many trees and for how long we want to iterate this process of training on the previous tree's error. And we also need to figure out what depth do we want each tree to be? Remember, the idea was to take a CART, trim it down to maybe depth 3 or 5, to make what's called a weak learner, technically. And then use a whole bunch of these weak learners and an ensemble to create a better prediction. Another way we can improve the performance of boosted trees is by using Bagging. Bagging is a sampling technique and it stands for Bootstrap Aggregation. So all it is is, again, this is just the prediction model that we're showing here. Now we're showing the actual trees. All of these have a depth 2. And these are our examples. So basically, to train tree 1, we would take a subset of all the examples. Tree 2, we would take a different subset. We're basically sampling with replacement here. Okay. So, two trees can get the same example and the third tree, we're sampling with replacement a subset of the data. As well as sampling the examples, we can also bootstrap the features. We'll only look at this feature and this feature and we'll pretend that the other features don't exist. Likewise, sampling the features and the examples all the same. So this whole process is called Bagging, Bootstrap Aggregation. Out of pure math, it works out that roughly 36.7, which is 1 over e, of the examples actually won't be trained on. We automatically get a out-of-bag sample which we can use as our validation set. So Boosting is an ensemble of weak learners. Bagging reduces the variance in the predictions by making use of sampling techniques. For boosted trees, what we saw was tree 1 would make a prediction and we would use that error and that prediction and train the second tree on the error of prediction one. So we would have this sequence. Well, what Random Forests do is they just average the result of all of the trees. So they train a whole bunch of weak learners. They could use Bagging as well. And instead they just say, "Hey, what's your prediction?" "Hey, what's your prediction?" "Hey, what's your prediction?" And then just average those results together. You might also hear of a tree called C4.5. It was originally designed for classification only. Instead of binary splits, it can do n-ary splits and instead of Gini index, they use a type of information gain related to entropy. So far, we've gone over CARTs, Boosted trees, Bagging samples, and Bagging features. Some libraries that you can use to get started on your own. A great one I use is actually Boost, LightGBM I think is good too. Cat Boost is nice. Just letting you know, basically you'll rarely see CARTs used alone in practice. You'll generally see some form of a Boosting, Bagging or ensemble method. All right, that's it for this video. Join us next time as we continue our machine learning journey.